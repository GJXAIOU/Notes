# 六、深入服务端

## 一、协议设计

**Kafka 自定义了一组基于 TCP 的二进制协议，只要遵守这组协议的格式，就可以向 Kafka 发送/提交消息或其他操作**。

目前的 Kafka 2.0.0 中，共包含 43 种协议类型，每种协议类型都有对应的请求（Request）和响应（Response），它们都遵守特定的协议模式**。每种类型的 Request 都包含相同结构的协议请求头（RequestHeader）和不同结构的协议请求体（RequestBody）**，如下图所示。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230130225516962.png" alt="image-20230130225516962" style="zoom:50%;" />

协议请求头中包括如下四个域：

| 域（Field）    | 描述（Description）                                          |
| :------------- | :----------------------------------------------------------- |
| api_key        | API 标识，比如 PRODUCE、FETCH 等分别表示发送消息和拉取消息的请求 |
| api_version    | API 版本号                                                   |
| correlation_id | 由客户端指定的一个数字来唯一地标识这次请求的 id，服务端在处理完请求后也会把同样的 coorelation_id 写到 Response 中，这样客户端就能把某个请求和响应对应起来了 |
| client_id      | 客户端 id                                                    |

每种类型的 Response 也包含相同结构的协议响应头（ResponseHeader）和不同结构的响应体（ResponseBody），如下图所示。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230130225700580.png" alt="image-20230130225700580" style="zoom:50%;" />

协议响应头中只有一个 correlation_id，对应的释义同上。

Kafka 中所有协议类型的 Request 和 Response 的结构都是具备固定格式的，并且它们都构建于如下多种基本数据类型之上：

> int32 等都是在 C# 中的，Java 中是没有的，本质是在 C99 中定义；
>
> int16/32/63 对应 Java 中的 short/int/long 分别占 2/4/8 个字节；

| 类型（Type）    | 描述（Description）                                          |
| :-------------- | :----------------------------------------------------------- |
| boolean         | 布尔类型，使用0和1分别代表false和true                        |
| int8            | 带符号整型，占8位，值在$-2^7$至 $2^7 - 1$ 之间               |
| int16           | 带符号整型，占16位，值在$-2^{15}$ 至 $2^{15} - 1$ 之间       |
| int32           | 带符号整型，占32位，值在$-2^{31}$ 至 $2^{31} - 1$ 之间       |
| int64           | 带符号整型，占64位，值在$-2^{63}$ 至 $2^{63} - 1$ 之间       |
| unit32          | 无符号整型，占32位，值在 0 至 $2^{32} - 1$ 之间              |
| varint          | 变长整型，值在$-2^{31}$ 至 $2^{31} - 1$ 之间，使用 ZigZag 编码 |
| varlong         | 变长长整型，值在 $-2^{63}$ 至 $2^{63} - 1$ 之间，使用 ZigZag 编码 |
| string          | 字符串类型。开头是一个int16类型的长度字段（非负数），代表字符串的长度N，后面包含N个UTF-8编码的字符 |
| nullable_string | 可为空的字符串类型。如果此类型的值为空，则用-1表示，其余情况同string类型一样 |
| bytes           | 表示一个字节序列。开头是一个int32类型的长度字段，代表后面字节序列的长度N，后面再跟N个字节 |
| nullable_bytes  | 表示一个可为空的字节序列，为空时用-1表示，其余情况同bytes    |
| records         | 表示Kafka中的一个消息序列，也可以看作nullable_bytes          |
| array           | 表示一个给定类型T的数组，也就是说，数组中包含若干T类型的实例。T可以是基础类型或基础类型组成的一个结构。该域开头的是一个int32类型的长度字段，代表T实例的个数为N，后面再跟N个T的实例。可用-1表示一个空的数组 |

以最常见的消息发送和消息拉取的两种协议类型做细致的讲解。

- 消息发送的协议类型，即 ProduceRequest/ProduceResponse，对应的 api_key = 0，表示 PRODUCE。从 Kafka 建立之初，其所支持的协议类型就一直在增加，并且对特定的协议类型而言，内部的组织结构也并非一成不变。以 ProduceRequest/ProduceResponse 为例，截至目前就经历了7个版本（V0～V6）的变迁。下面就以最新版本（V6，即api_version = 6）的结构为例来做细致的讲解。ProduceRequest 的组织结构如下图所示。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230130235812565.png" alt="image-20230130235812565" style="zoom: 50%;" />

 



请求体中各个域的含义如下：

| 域（Field）      | 类 型           | 描述（Description）                                          |
| :--------------- | :-------------- | :----------------------------------------------------------- |
| transactional_id | nullable_string | 事务 id，从 Kafka 0.11.0 开始支持事务。如果不使用事务的功能，则值为 null |
| acks             | int16           | 对应客户端参数 acks                                          |
| timeout          | int32           | 请求超时时间，对应客户端参数 `request.timeout.ms`，默认值为 30000，即 30 秒 |
| topic_data       | array           | 代表 ProduceRequest 中所要发送的数据集合。以主题名称分类，主题中再以分区分类。注意这个域是数组类型 |
| topic            | string          | 主题名称                                                     |
| data             | array           | 与主题对应的数据，注意这个域也是数组类型                     |
| partition        | int32           | 分区编号                                                     |
| record_set       | records         | 与分区对应的数据                                             |

消息累加器 RecordAccumulator 中的消息是以 `<分区, Deque< ProducerBatch>>` 的形式进行缓存的，之后由 Sender 线程转变成 `<Node, List<ProducerBatch>>` 的形式，针对每个 Node，Sender 线程在发送消息前会将对应的 `List<ProducerBatch>` 形式的内容转变成 ProduceRequest 的具体结构。`List<ProducerBatch>` 中的内容首先会按照主题名称进行分类（对应 ProduceRequest 中的域 topic），然后按照分区编号进行分类（对应 ProduceRequest 中的域 partition），分类之后的 ProducerBatch 集合就对应 ProduceRequest 中的域 record_set。

另外每个分区中的消息是顺序追加的，那么在客户端中按照分区归纳好之后就可以省去在服务端中转换的操作了，这样将负载的压力分摊给了客户端，从而使服务端可以专注于它的分内之事，也提升了整体的性能。

如果参数 acks 设置非 0 值，那么生产者客户端在发送 ProduceRequest 请求之后就需要（异步）等待服务端的响应 ProduceResponse。对 ProduceResponse 而言，V6 版本中 ProduceResponse 的组织结构如下图所示。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131000356918.png" alt="image-20230131000356918" style="zoom:50%;" />

除了响应头中的 correlation_id，其余 ProduceResponse 各个域的含义如下表所示。

| 域（Field）         | 类 型  | 描述（Description）                                          |
| :------------------ | :----- | :----------------------------------------------------------- |
| throttle_time_ms    | int32  | 如果超过了配额（quota）限制则需要延迟该请求的处理时间。如果没有配置配额，那么该字段的值为0 |
| responses           | array  | 代表 ProudceResponse 中要返回的数据集合。同样按照主题分区的粒度进行划分，注意这个域是一个数组类型 |
| topic               | string | 主题名称                                                     |
| partition_responses | array  | 主题中所有分区的响应，注意这个域也是一个数组类型             |
| partition           | int32  | 分区编号                                                     |
| error_code          | int16  | 错误码，用来标识错误类型。目前版本的错误码有 74 种，具体可以参考[这里](https://kafka.apache.org/protocol.html#protocol_error_codes) |
| base_offset         | int64  | 消息集的起始偏移量                                           |
| log_append_time     | int64  | 消息写入broker端的时间                                       |
| log_start_offset    | int64  | 所在分区的起始偏移量                                         |

**消息追加是针对单个分区而言的，那么响应也是针对分区粒度来进行划分的，这样 ProduceRequest 和 ProduceResponse 做到了一一对应**。

- 拉取消息的协议类型，即 FetchRequest/FetchResponse，对应的 api_key = 1，表示 FETCH。截至目前，FetchRequest/FetchResponse 一共历经了 9 个版本（V0～V8）的变迁，下面就以最新版本的 FetchRequest 结构：

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131001435330.png" alt="image-20230131001435330" style="zoom: 50%;" />

FetchRequest 请求体中各个域的含义如下表所示。

| 域（Field）           | 类 型  | 描述（Description）                                          |
| :-------------------- | :----- | :----------------------------------------------------------- |
| replica_id            | int32  | 用来指定副本的 brokerId，这个域是用于 follower 副本向 leader 副本发起 FetchRequest 请求的，对于普通消费者客户端而言，这个域的值保持为-1<br />==》拉取消息不仅是客户端到服务器，也有各个副本之间的同步 |
| max_wait_time         | int32  | 和消费者客户端参数 `fetch.max.wait.ms` 对应，默认值为 500    |
| min_bytes             | int32  | 和消费者客户端参数 `fetch.min.bytes` 对应，默认值为 1        |
| max_bytes             | int32  | 和消费者客户端参数 `fetch.max.bytes` 对应，默认值为 52428800，即50MB |
| isolation_level       | int8   | 和消费者客户端参数 `isolation.level` 对应，默认值为 `read_uncommitted`，可选值为 `read_committed`，这两个值分别对应本域的 0 和 1 的值 |
| session_id            | int32  | fetch session 的 id，详细参考下面的释义                      |
| epoch                 | int32  | fetch session 的 epoch 纪元，它和 seesion_id 一样都是 fetch session 的元数据，详细参考下面的释义 |
| topics                | array  | 所要拉取的主题信息，注意这是一个数组类型                     |
| topic                 | string | 主题名称                                                     |
| partitions            | array  | 分区信息，注意这也是一个数组类型                             |
| partition             | int32  | 分区编号                                                     |
| fetch_offset          | int64  | 指定从分区的哪个位置开始读取消息。如果是follower副本发起的请求，那么这个域可以看作当前follower副本的LEO |
| log_start_offset      | int64  | 该域专门用于follower副本发起的FetchRequest请求，用来指明分区的起始偏移量。对于普通消费者客户端而言这个值保持为-1 |
| max_bytes             | int32  | 注意在最外层中也包含同样名称的域，但是两个所代表的含义不同，这里是针对单个分区而言的，和消费者客户端参数max.partition.fetch.bytes对应，默认值为1048576，即1MB |
| forgotten_topics_data | array  | 数组类型，指定从fetch session中指定要去除的拉取信息，详细参考下面的释义 |
| topic                 | string | 主题名称                                                     |
| partitions            | array  | 数组类型，表示分区编号的集合                                 |

**不管是 follower 副本还是普通的消费者客户端，如果要拉取某个分区中的消息，就需要指定详细的拉取信息**，也就是需要设定 partition、fetch_offset、log_start_offset 和 max_bytes 这 4 个域的具体值，那么对每个分区而言，就需要占用 4B+8B+8B+4B = 24B 的空间。

一般不管是 follower 副本还是普通的消费者，它们的订阅信息是长期固定的。即 FetchRequest 中的 topics 域的内容是长期固定的，只有在拉取开始时或发生某些异常时会有所变动。同时该请求通常非常频繁，如果要拉取的分区数有很多，比如有 1000 个分区，那么在网络上频繁交互 FetchRequest 时就会有固定的 1000×24B ≈ 24KB 的字节的内容在传动，如果可以将这 24KB 的状态保存起来，那么就可以节省这部分所占用的带宽。

Kafka 从 1.1.0 版本开始针对 FetchRequest 引入了 session_id、epoch 和 forgotten_ topics_data 等域，session_id 和 epoch 确定一条拉取链路的 fetch session，当 session 建立或变更时会发送全量式的 FetchRequest，所谓的全量式就是指请求体中包含所有需要拉取的分区信息；当 session 稳定时则会发送增量式的 FetchRequest 请求，里面的 topics 域为空，因为 topics 域的内容已经被缓存在了 session 链路的两侧。如果需要从当前 fetch session 中取消对某些分区的拉取订阅，则可以使用 forgotten_topics_data 字段来实现。

这个改进在大规模（有大量的分区副本需要及时同步）的 Kafka 集群中非常有用，它可以提升集群间的网络带宽的有效使用率。不过对客户端而言效果不是那么明显，一般情况下单个客户端不会订阅太多的分区，不过总体上这也是一个很好的优化改进。

与 FetchRequest 对应的 FetchResponse 的组织结构（V8版本）可以参考下图。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131085129558.png" alt="image-20230131085129558" style="zoom:50%;" />

FetchResponse 结构中主要分为4层，第1层： throttle_time_ms、error_code、session_id 和 responses，前面3个域都见过，其中 session_id 和 FetchRequest 中的 session_id 对应。**responses 是一个数组类型，表示响应的具体内容，即结构中的第 2 层**，具体地细化到每个分区的响应。第3层中包含分区的元数据信息（partition、error_code 等）及具体的消息内容（record_set），aborted_transactions 和事务相关。

通过协议的具体定义可以让我们从另一个角度来了解 Kafka 的本质。以 PRODUCE 和 FETCH 为例，从协议结构中就可以看出消息的写入和拉取消费都是细化到每一个分区层级的。并且，通过了解各个协议版本变迁的细节也能够从侧面了解 Kafka 变迁的历史，在变迁的过程中遇到过哪方面的瓶颈，又采取哪种优化手段，比如 FetchRequest 中的 session_id 的引入。

完整的协议类型列表可以参考[官方文档](https://link.juejin.cn/?target=http%3A%2F%2Fkafka.apache.org%2Fprotocol.html%23protocol_api_keys)。

## 时间轮

**Kafka 中存在大量的延时操作，比如延时生产、延时拉取和延时删除等。Kafka 并没有使用 JDK 自带的 Timer 或 DelayQueue 来实现延时的功能，而是基于时间轮的概念自定义实现了一个用于延时功能的定时器（SystemTimer）。JDK 中 Timer 和 DelayQueue 的插入和删除操作的平均时间复杂度为 O(nlogn) 并不能满足 Kafka 的高性能要求，而基于时间轮可以将插入和删除操作的时间复杂度都降为 O(1)**。时间轮的应用并非 Kafka 独有，其应用场景还有很多，在 Netty、Akka、Quartz、ZooKeeper 等组件中都存在时间轮的踪影。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131085556423.png" alt="image-20230131085556423" style="zoom:50%;" />

**Kafka 中的时间轮（TimingWheel）是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务列表（TimerTaskList）。TimerTaskList 是一个环形的双向链表，链表中的每一项表示的都是定时任务项（TimerTaskEntry），其中封装了真正的定时任务（TimerTask）**。

时间轮由多个时间格组成，每个时间格代表当前时间轮的基本时间跨度（tickMs）。时间轮的时间格个数是固定的，可用 wheelSize 来表示，那么整个时间轮的总体时间跨度（interval）可以通过公式 tickMs×wheelSize 计算得出。

**时间轮还有一个表盘指针（currentTime），用来表示时间轮当前所处的时间，currentTime 是 tickMs 的整数倍。currentTime 可以将整个时间轮划分为到期部分和未到期部分，currentTime 当前指向的时间格也属于到期部分，表示刚好到期，需要处理此时间格所对应的 TimerTaskList 中的所有任务。**

若时间轮的 tickMs 为 1ms 且 wheelSize 等于20，那么可以计算得出总体时间跨度 interval 为 20ms。

初始情况下表盘指针 currentTime 指向时间格 0，此时有一个定时为 2ms 的任务插进来会存放到时间格为 2 的 TimerTaskList 中。随着时间的不断推移，指针 currentTime 不断向前推进，过了 2ms 之后，当到达时间格 2 时，就需要将时间格 2 对应的 TimeTaskList 中的任务进行相应的到期操作。此时若又有一个定时为 8ms 的任务插进来，则会存放到时间格 10 中，currentTime 再过 8ms 后会指向时间格 10。

如果同时有一个定时为 19ms 的任务插进来怎么办？新来的 TimerTaskEntry 会复用原来的 TimerTaskList，所以它会插入原本已经到期的时间格 1。总之，整个时间轮的总体跨度是不变的，随着指针 currentTime 的不断推进，当前时间轮所能处理的时间段也在不断后移，总体时间范围在 currentTime 和 currentTime+interval 之间。

如果此时有一个定时为 350ms 的任务该如何处理？直接扩充 wheelSize 的大小？Kafka 中不乏几万甚至几十万毫秒的定时任务，这个 wheelSize 的扩充没有底线，就算将所有的定时任务的到期时间都设定一个上限，比如 100 万毫秒，那么这个 wheelSize 为 100 万毫秒的时间轮不仅占用很大的内存空间，而且也会拉低效率。Kafka 为此引入了**层级时间轮**的概念，**当任务的到期时间超过了当前时间轮所表示的时间范围时，就会尝试添加到上层时间轮中**。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131090154481.png" alt="image-20230131090154481" style="zoom: 67%;" />

如上图所示，复用之前的案例，第一层的时间轮 tickMs=1ms、wheelSize=20、interval=20ms。**第二层的时间轮的 tickMs 为第一层时间轮的 interval，即20ms。每一层时间轮的 wheelSize 是固定的，都是 20，那么第二层的时间轮的总体时间跨度 interval 为 400ms**。以此类推，这个 400ms 也是第三层的 tickMs 的大小，第三层的时间轮的总体时间跨度为 8000ms。

对于之前所说的 350ms 的定时任务，显然第一层时间轮不能满足条件，所以就升级到第二层时间轮中，最终被插入第二层时间轮中时间格 17 所对应的 TimerTaskList。如果此时又有一个定时为 450ms 的任务，那么显然第二层时间轮也无法满足条件，所以又升级到第三层时间轮中，最终被插入第三层时间轮中时间格 1的 TimerTaskList。注意到在到期时间为 [400ms,800ms) 区间内的多个任务（比如 446ms、455ms 和 473ms 的定时任务）都会被放入第三层时间轮的时间格 1，时间格 1 对应的 TimerTaskList 的超时时间为 400ms。

随着时间的流逝，当此 TimerTaskList 到期之时，原本定时为 450ms 的任务还剩下 50ms 的时间，还不能执行这个任务的到期操作。**这里就有一个时间轮降级的操作**，会将这个剩余时间为 50ms 的定时任务重新提交到层级时间轮中，此时第一层时间轮的总体时间跨度不够，而第二层足够，所以该任务被放到第二层时间轮到期时间为 [40ms,60ms) 的时间格中。再经历 40ms 之后，此时这个任务又被“察觉”，不过还剩余 10ms，还是不能立即执行到期操作。所以还要再有一次时间轮的降级，此任务被添加到第一层时间轮到期时间为 [10ms,11ms) 的时间格中，之后再经历 10ms 后，此任务真正到期，最终执行相应的到期操作。

常见的钟表就是一种具有三层结构的时间轮，第一层时间轮 tickMs=1s、wheelSize=60、interval=1min，此为秒钟；第二层 tickMs=1min、wheelSize=60、interval=1hour，此为分钟；第三层 tickMs=1hour、wheelSize=12、interval=12hours，此为时钟。

在 Kafka 中，第一层时间轮的参数同上面的案例一样：tickMs=1ms、wheelSize=20、interval=20ms，各个层级的 wheelSize 也固定为20，所以各个层级的 tickMs 和 interval 也可以相应地推算出来。Kafka 在具体实现时间轮 TimingWheel 时还有一些小细节：

- TimingWheel 在创建的时候以当前系统时间为第一层时间轮的起始时间（startMs），这里的当前系统时间并没有简单地调用 System.currentTimeMillis()，而是调用了 Time.SYSTEM.hiResClockMs，这是因为 currentTimeMillis() 方法的时间精度依赖于操作系统的具体实现，有些操作系统下并不能达到毫秒级的精度，而 **Time.SYSTEM.hiResClockMs 实质上采用了 System.nanoTime()/1_000_000 来将精度调整到毫秒级**。
- TimingWheel 中的每个双向环形链表 TimerTaskList 都会有一个哨兵节点（sentinel），引入哨兵节点可以简化边界条件。哨兵节点也称为哑元节点（dummy node），它是一个附加的链表节点，该节点作为第一个节点，它的值域中并不存储任何东西，只是为了操作的方便而引入的。如果一个链表有哨兵节点，那么线性表的第一个元素应该是链表的第二个节点。
- 除了第一层时间轮，其余高层时间轮的起始时间（startMs）都设置为创建此层时间轮时前面第一轮的 currentTime。每一层的 currentTime 都必须是 tickMs 的整数倍，如果不满足则会将 currentTime 修剪为 tickMs 的整数倍，以此与时间轮中的时间格的到期时间范围对应起来。修剪方法为：**currentTime = startMs - (startMs % tickMs)**。currentTime 会随着时间推移而推进，但不会改变为 tickMs 的整数倍的既定事实。若某一时刻的时间为 timeMs，那么此时时间轮的 currentTime = timeMs - (timeMs % tickMs)，时间每推进一次，每个层级的时间轮的 currentTime 都会依据此公式执行推进。
- Kafka 中的定时器只需持有 TimingWheel 的第一层时间轮的引用，并不会直接持有其他高层的时间轮，但每一层时间轮都会有一个引用（overflowWheel）指向更高一层的应用，以此层级调用可以实现定时器间接持有各个层级时间轮的引用。

关于时间轮的细节就描述到这里，各个组件中对时间轮的实现大同小异。读者读到这里是否会好奇文中一直描述的一个情景—“随着时间的流逝”或“随着时间的推移”，那么在 Kafka 中到底是怎么推进时间的呢？类似采用 JDK 中的 scheduleAtFixedRate 来每秒推进时间轮？显然这样并不合理，TimingWheel 也失去了大部分意义。

==下面关于时间轮的介绍没有理解==

**Kafka 中的定时器借了 JDK 中的 [DelayQueue](https://so.csdn.net/so/search?q=DelayQueue&spm=1001.2101.3001.7020) 来协助推进时间轮。具体做法是对于每个使用到的 TimerTaskList 都加入 DelayQueue，“每个用到的 TimerTaskList”特指非哨兵节点的定时任务项 TimerTaskEntry 对应的 TimerTaskList。DelayQueue 会根据 TimerTaskList 对应的超时时间 expiration 来排序，最短 expiration 的 TimerTaskList 会被排在 DelayQueue 的队头**。

Kafka 中会有一个线程来获取 DelayQueue 中到期的任务列表，有意思的是这个线程所对应的名称叫作“ExpiredOperationReaper”，可以直译为“过期操作收割机”，和第4节的“SkimpyOffsetMap”的取名有异曲同工之妙。当“收割机”线程获取 DelayQueue 中超时的任务列表 TimerTaskList 之后，既可以根据 TimerTaskList 的 expiration 来推进时间轮的时间，也可以就获取的 TimerTaskList 执行相应的操作，对里面的 TimerTaskEntry 该执行过期操作的就执行过期操作，该降级时间轮的就降级时间轮。

读到这里或许会感到困惑，开头明确指明的 DelayQueue 不适合 Kafka 这种高性能要求的定时任务，为何这里还要引入 DelayQueue 呢？注意对定时任务项 TimerTaskEntry 的插入和删除操作而言，TimingWheel[时间复杂度](https://so.csdn.net/so/search?q=时间复杂度&spm=1001.2101.3001.7020)为 O(1)，性能高出 DelayQueue 很多，如果直接将 TimerTaskEntry 插入 DelayQueue，那么性能显然难以支撑。就算我们根据一定的规则将若干 TimerTaskEntry 划分到 TimerTaskList 这个组中，然后将 TimerTaskList 插入 DelayQueue，如果在 TimerTaskList 中又要多添加一个 TimerTaskEntry 时该如何处理呢？对 DelayQueue 而言，这类操作显然变得力不从心。

分析到这里可以发现，Kafka 中的 TimingWheel 专门用来执行插入和删除 TimerTaskEntry 的操作，而 DelayQueue 专门负责时间推进的任务。试想一下，DelayQueue 中的第一个超时任务列表的 expiration 为 200ms，第二个超时任务为 840ms，这里获取 DelayQueue 的队头只需要 O(1) 的时间复杂度（获取之后 DelayQueue 内部才会再次切换出新的队头）。如果采用每秒定时推进，那么获取第一个超时的任务列表时执行的200次推进中有199次属于“空推进”，而获取第二个超时任务时又需要执行639次“空推进”，这样会无故空耗机器的性能资源，这里采用 DelayQueue 来辅助以少量空间换时间，从而做到了“精准推进”。Kafka 中的定时器真可谓“知人善用”，用 TimingWheel 做最擅长的任务添加和删除操作，而用 DelayQueue 做最擅长的时间推进工作，两者相辅相成。