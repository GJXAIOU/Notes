# 六、深入服务端

## 一、协议设计

**Kafka 自定义了一组基于 TCP 的二进制协议，只要遵守这组协议的格式，就可以向 Kafka 发送/提交消息或其他操作**。

目前的 Kafka 2.0.0 中，共包含 43 种协议类型，每种协议类型都有对应的请求（Request）和响应（Response），它们都遵守特定的协议模式**。每种类型的 Request 都包含相同结构的协议请求头（RequestHeader）和不同结构的协议请求体（RequestBody）**，如下图所示。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230130225516962.png" alt="image-20230130225516962" style="zoom:50%;" />

协议请求头中包括如下四个域：

| 域（Field）    | 描述（Description）                                          |
| :------------- | :----------------------------------------------------------- |
| api_key        | API 标识，比如 PRODUCE、FETCH 等分别表示发送消息和拉取消息的请求 |
| api_version    | API 版本号                                                   |
| correlation_id | 由客户端指定的一个数字来唯一地标识这次请求的 id，服务端在处理完请求后也会把同样的 coorelation_id 写到 Response 中，这样客户端就能把某个请求和响应对应起来了 |
| client_id      | 客户端 id                                                    |

每种类型的 Response 也包含相同结构的协议响应头（ResponseHeader）和不同结构的响应体（ResponseBody），如下图所示。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230130225700580.png" alt="image-20230130225700580" style="zoom:50%;" />

协议响应头中只有一个 correlation_id，对应的释义同上。

Kafka 中所有协议类型的 Request 和 Response 的结构都是具备固定格式的，并且它们都构建于如下多种基本数据类型之上：

> int32 等都是在 C# 中的，Java 中是没有的，本质是在 C99 中定义；
>
> int16/32/63 对应 Java 中的 short/int/long 分别占 2/4/8 个字节；

| 类型（Type）    | 描述（Description）                                          |
| :-------------- | :----------------------------------------------------------- |
| boolean         | 布尔类型，使用0和1分别代表false和true                        |
| int8            | 带符号整型，占8位，值在$-2^7$至 $2^7 - 1$ 之间               |
| int16           | 带符号整型，占16位，值在$-2^{15}$ 至 $2^{15} - 1$ 之间       |
| int32           | 带符号整型，占32位，值在$-2^{31}$ 至 $2^{31} - 1$ 之间       |
| int64           | 带符号整型，占64位，值在$-2^{63}$ 至 $2^{63} - 1$ 之间       |
| unit32          | 无符号整型，占32位，值在 0 至 $2^{32} - 1$ 之间              |
| varint          | 变长整型，值在$-2^{31}$ 至 $2^{31} - 1$ 之间，使用 ZigZag 编码 |
| varlong         | 变长长整型，值在 $-2^{63}$ 至 $2^{63} - 1$ 之间，使用 ZigZag 编码 |
| string          | 字符串类型。开头是一个int16类型的长度字段（非负数），代表字符串的长度N，后面包含N个UTF-8编码的字符 |
| nullable_string | 可为空的字符串类型。如果此类型的值为空，则用-1表示，其余情况同string类型一样 |
| bytes           | 表示一个字节序列。开头是一个int32类型的长度字段，代表后面字节序列的长度N，后面再跟N个字节 |
| nullable_bytes  | 表示一个可为空的字节序列，为空时用-1表示，其余情况同bytes    |
| records         | 表示Kafka中的一个消息序列，也可以看作nullable_bytes          |
| array           | 表示一个给定类型T的数组，也就是说，数组中包含若干T类型的实例。T可以是基础类型或基础类型组成的一个结构。该域开头的是一个int32类型的长度字段，代表T实例的个数为N，后面再跟N个T的实例。可用-1表示一个空的数组 |

以最常见的消息发送和消息拉取的两种协议类型做细致的讲解。

- 消息发送的协议类型，即 ProduceRequest/ProduceResponse，对应的 api_key = 0，表示 PRODUCE。从 Kafka 建立之初，其所支持的协议类型就一直在增加，并且对特定的协议类型而言，内部的组织结构也并非一成不变。以 ProduceRequest/ProduceResponse 为例，截至目前就经历了7个版本（V0～V6）的变迁。下面就以最新版本（V6，即api_version = 6）的结构为例来做细致的讲解。ProduceRequest 的组织结构如下图所示。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230130235812565.png" alt="image-20230130235812565" style="zoom: 50%;" />

 



请求体中各个域的含义如下：

| 域（Field）      | 类 型           | 描述（Description）                                          |
| :--------------- | :-------------- | :----------------------------------------------------------- |
| transactional_id | nullable_string | 事务 id，从 Kafka 0.11.0 开始支持事务。如果不使用事务的功能，则值为 null |
| acks             | int16           | 对应客户端参数 acks                                          |
| timeout          | int32           | 请求超时时间，对应客户端参数 `request.timeout.ms`，默认值为 30000，即 30 秒 |
| topic_data       | array           | 代表 ProduceRequest 中所要发送的数据集合。以主题名称分类，主题中再以分区分类。注意这个域是数组类型 |
| topic            | string          | 主题名称                                                     |
| data             | array           | 与主题对应的数据，注意这个域也是数组类型                     |
| partition        | int32           | 分区编号                                                     |
| record_set       | records         | 与分区对应的数据                                             |

消息累加器 RecordAccumulator 中的消息是以 `<分区, Deque< ProducerBatch>>` 的形式进行缓存的，之后由 Sender 线程转变成 `<Node, List<ProducerBatch>>` 的形式，针对每个 Node，Sender 线程在发送消息前会将对应的 `List<ProducerBatch>` 形式的内容转变成 ProduceRequest 的具体结构。`List<ProducerBatch>` 中的内容首先会按照主题名称进行分类（对应 ProduceRequest 中的域 topic），然后按照分区编号进行分类（对应 ProduceRequest 中的域 partition），分类之后的 ProducerBatch 集合就对应 ProduceRequest 中的域 record_set。

另外每个分区中的消息是顺序追加的，那么在客户端中按照分区归纳好之后就可以省去在服务端中转换的操作了，这样将负载的压力分摊给了客户端，从而使服务端可以专注于它的分内之事，也提升了整体的性能。

如果参数 acks 设置非 0 值，那么生产者客户端在发送 ProduceRequest 请求之后就需要（异步）等待服务端的响应 ProduceResponse。对 ProduceResponse 而言，V6 版本中 ProduceResponse 的组织结构如下图所示。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131000356918.png" alt="image-20230131000356918" style="zoom:50%;" />

除了响应头中的 correlation_id，其余 ProduceResponse 各个域的含义如下表所示。

| 域（Field）         | 类 型  | 描述（Description）                                          |
| :------------------ | :----- | :----------------------------------------------------------- |
| throttle_time_ms    | int32  | 如果超过了配额（quota）限制则需要延迟该请求的处理时间。如果没有配置配额，那么该字段的值为0 |
| responses           | array  | 代表 ProudceResponse 中要返回的数据集合。同样按照主题分区的粒度进行划分，注意这个域是一个数组类型 |
| topic               | string | 主题名称                                                     |
| partition_responses | array  | 主题中所有分区的响应，注意这个域也是一个数组类型             |
| partition           | int32  | 分区编号                                                     |
| error_code          | int16  | 错误码，用来标识错误类型。目前版本的错误码有 74 种，具体可以参考[这里](https://kafka.apache.org/protocol.html#protocol_error_codes) |
| base_offset         | int64  | 消息集的起始偏移量                                           |
| log_append_time     | int64  | 消息写入broker端的时间                                       |
| log_start_offset    | int64  | 所在分区的起始偏移量                                         |

**消息追加是针对单个分区而言的，那么响应也是针对分区粒度来进行划分的，这样 ProduceRequest 和 ProduceResponse 做到了一一对应**。

- 拉取消息的协议类型，即 FetchRequest/FetchResponse，对应的 api_key = 1，表示 FETCH。截至目前，FetchRequest/FetchResponse 一共历经了 9 个版本（V0～V8）的变迁，下面就以最新版本的 FetchRequest 结构：

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131001435330.png" alt="image-20230131001435330" style="zoom: 50%;" />

FetchRequest 请求体中各个域的含义如下表所示。

| 域（Field）           | 类 型  | 描述（Description）                                          |
| :-------------------- | :----- | :----------------------------------------------------------- |
| replica_id            | int32  | 用来指定副本的 brokerId，这个域是用于 follower 副本向 leader 副本发起 FetchRequest 请求的，对于普通消费者客户端而言，这个域的值保持为-1<br />==》拉取消息不仅是客户端到服务器，也有各个副本之间的同步 |
| max_wait_time         | int32  | 和消费者客户端参数 `fetch.max.wait.ms` 对应，默认值为 500    |
| min_bytes             | int32  | 和消费者客户端参数 `fetch.min.bytes` 对应，默认值为 1        |
| max_bytes             | int32  | 和消费者客户端参数 `fetch.max.bytes` 对应，默认值为 52428800，即50MB |
| isolation_level       | int8   | 和消费者客户端参数 `isolation.level` 对应，默认值为 `read_uncommitted`，可选值为 `read_committed`，这两个值分别对应本域的 0 和 1 的值 |
| session_id            | int32  | fetch session 的 id，详细参考下面的释义                      |
| epoch                 | int32  | fetch session 的 epoch 纪元，它和 seesion_id 一样都是 fetch session 的元数据，详细参考下面的释义 |
| topics                | array  | 所要拉取的主题信息，注意这是一个数组类型                     |
| topic                 | string | 主题名称                                                     |
| partitions            | array  | 分区信息，注意这也是一个数组类型                             |
| partition             | int32  | 分区编号                                                     |
| fetch_offset          | int64  | 指定从分区的哪个位置开始读取消息。如果是follower副本发起的请求，那么这个域可以看作当前follower副本的LEO |
| log_start_offset      | int64  | 该域专门用于follower副本发起的FetchRequest请求，用来指明分区的起始偏移量。对于普通消费者客户端而言这个值保持为-1 |
| max_bytes             | int32  | 注意在最外层中也包含同样名称的域，但是两个所代表的含义不同，这里是针对单个分区而言的，和消费者客户端参数max.partition.fetch.bytes对应，默认值为1048576，即1MB |
| forgotten_topics_data | array  | 数组类型，指定从fetch session中指定要去除的拉取信息，详细参考下面的释义 |
| topic                 | string | 主题名称                                                     |
| partitions            | array  | 数组类型，表示分区编号的集合                                 |

**不管是 follower 副本还是普通的消费者客户端，如果要拉取某个分区中的消息，就需要指定详细的拉取信息**，也就是需要设定 partition、fetch_offset、log_start_offset 和 max_bytes 这 4 个域的具体值，那么对每个分区而言，就需要占用 4B+8B+8B+4B = 24B 的空间。

一般不管是 follower 副本还是普通的消费者，它们的订阅信息是长期固定的。即 FetchRequest 中的 topics 域的内容是长期固定的，只有在拉取开始时或发生某些异常时会有所变动。同时该请求通常非常频繁，如果要拉取的分区数有很多，比如有 1000 个分区，那么在网络上频繁交互 FetchRequest 时就会有固定的 1000×24B ≈ 24KB 的字节的内容在传动，如果可以将这 24KB 的状态保存起来，那么就可以节省这部分所占用的带宽。

Kafka 从 1.1.0 版本开始针对 FetchRequest 引入了 session_id、epoch 和 forgotten_ topics_data 等域，session_id 和 epoch 确定一条拉取链路的 fetch session，当 session 建立或变更时会发送全量式的 FetchRequest，所谓的全量式就是指请求体中包含所有需要拉取的分区信息；当 session 稳定时则会发送增量式的 FetchRequest 请求，里面的 topics 域为空，因为 topics 域的内容已经被缓存在了 session 链路的两侧。如果需要从当前 fetch session 中取消对某些分区的拉取订阅，则可以使用 forgotten_topics_data 字段来实现。

这个改进在大规模（有大量的分区副本需要及时同步）的 Kafka 集群中非常有用，它可以提升集群间的网络带宽的有效使用率。不过对客户端而言效果不是那么明显，一般情况下单个客户端不会订阅太多的分区，不过总体上这也是一个很好的优化改进。

与 FetchRequest 对应的 FetchResponse 的组织结构（V8版本）可以参考下图。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131085129558.png" alt="image-20230131085129558" style="zoom:50%;" />

FetchResponse 结构中主要分为4层，第1层： throttle_time_ms、error_code、session_id 和 responses，前面3个域都见过，其中 session_id 和 FetchRequest 中的 session_id 对应。**responses 是一个数组类型，表示响应的具体内容，即结构中的第 2 层**，具体地细化到每个分区的响应。第3层中包含分区的元数据信息（partition、error_code 等）及具体的消息内容（record_set），aborted_transactions 和事务相关。

通过协议的具体定义可以让我们从另一个角度来了解 Kafka 的本质。以 PRODUCE 和 FETCH 为例，从协议结构中就可以看出消息的写入和拉取消费都是细化到每一个分区层级的。并且，通过了解各个协议版本变迁的细节也能够从侧面了解 Kafka 变迁的历史，在变迁的过程中遇到过哪方面的瓶颈，又采取哪种优化手段，比如 FetchRequest 中的 session_id 的引入。

完整的协议类型列表可以参考[官方文档](https://link.juejin.cn/?target=http%3A%2F%2Fkafka.apache.org%2Fprotocol.html%23protocol_api_keys)。

## 时间轮

> 相关博客：https://mp.weixin.qq.com/s/WC_XIFllMKPFjm2Vrg8JJw

**Kafka 中存在大量的延时操作，比如延时生产、延时拉取和延时删除等。Kafka 并没有使用 JDK 自带的 Timer 或 DelayQueue 来实现延时的功能，而是基于时间轮的概念自定义实现了一个用于延时功能的定时器（SystemTimer）。JDK 中 Timer 和 DelayQueue 的插入和删除操作的平均时间复杂度为 O(nlogn) 并不能满足 Kafka 的高性能要求，而基于时间轮可以将插入和删除操作的时间复杂度都降为 O(1)**。时间轮的应用并非 Kafka 独有，其应用场景还有很多，在 Netty、Akka、Quartz、ZooKeeper 等组件中都存在时间轮的踪影。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131085556423.png" alt="image-20230131085556423" style="zoom:50%;" />

**Kafka 中的时间轮（TimingWheel）是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务列表（TimerTaskList）。TimerTaskList 是一个环形的双向链表，链表中的每一项表示的都是定时任务项（TimerTaskEntry），其中封装了真正的定时任务（TimerTask）**。

时间轮由多个时间格组成，每个时间格代表当前时间轮的基本时间跨度（tickMs）。时间轮的时间格个数是固定的，可用 wheelSize 来表示，那么整个时间轮的总体时间跨度（interval）可以通过公式 tickMs×wheelSize 计算得出。

**时间轮还有一个表盘指针（currentTime），用来表示时间轮当前所处的时间，currentTime 是 tickMs 的整数倍。currentTime 可以将整个时间轮划分为到期部分和未到期部分，currentTime 当前指向的时间格也属于到期部分，表示刚好到期，需要处理此时间格所对应的 TimerTaskList 中的所有任务。**

若时间轮的 tickMs 为 1ms 且 wheelSize 等于20，那么可以计算得出总体时间跨度 interval 为 20ms。

初始情况下表盘指针 currentTime 指向时间格 0，此时有一个定时为 2ms 的任务插进来会存放到时间格为 2 的 TimerTaskList 中。随着时间的不断推移，指针 currentTime 不断向前推进，过了 2ms 之后，当到达时间格 2 时，就需要将时间格 2 对应的 TimeTaskList 中的任务进行相应的到期操作。此时若又有一个定时为 8ms 的任务插进来，则会存放到时间格 10 中，currentTime 再过 8ms 后会指向时间格 10。

如果同时有一个定时为 19ms 的任务插进来怎么办？新来的 TimerTaskEntry 会复用原来的 TimerTaskList，所以它会插入原本已经到期的时间格 1。总之，整个时间轮的总体跨度是不变的，随着指针 currentTime 的不断推进，当前时间轮所能处理的时间段也在不断后移，总体时间范围在 currentTime 和 currentTime+interval 之间。

如果此时有一个定时为 350ms 的任务该如何处理？直接扩充 wheelSize 的大小？Kafka 中不乏几万甚至几十万毫秒的定时任务，这个 wheelSize 的扩充没有底线，就算将所有的定时任务的到期时间都设定一个上限，比如 100 万毫秒，那么这个 wheelSize 为 100 万毫秒的时间轮不仅占用很大的内存空间，而且也会拉低效率。Kafka 为此引入了**层级时间轮**的概念，**当任务的到期时间超过了当前时间轮所表示的时间范围时，就会尝试添加到上层时间轮中**。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/image-20230131090154481.png" alt="image-20230131090154481" style="zoom: 67%;" />

如上图所示，复用之前的案例，第一层的时间轮 tickMs=1ms、wheelSize=20、interval=20ms。**第二层的时间轮的 tickMs 为第一层时间轮的 interval，即20ms。每一层时间轮的 wheelSize 是固定的，都是 20，那么第二层的时间轮的总体时间跨度 interval 为 400ms**。以此类推，这个 400ms 也是第三层的 tickMs 的大小，第三层的时间轮的总体时间跨度为 8000ms。

对于之前所说的 350ms 的定时任务，显然第一层时间轮不能满足条件，所以就升级到第二层时间轮中，最终被插入第二层时间轮中时间格 17 所对应的 TimerTaskList。如果此时又有一个定时为 450ms 的任务，那么显然第二层时间轮也无法满足条件，所以又升级到第三层时间轮中，最终被插入第三层时间轮中时间格 1的 TimerTaskList。注意到在到期时间为 [400ms,800ms) 区间内的多个任务（比如 446ms、455ms 和 473ms 的定时任务）都会被放入第三层时间轮的时间格 1，时间格 1 对应的 TimerTaskList 的超时时间为 400ms。

随着时间的流逝，当此 TimerTaskList 到期之时，原本定时为 450ms 的任务还剩下 50ms 的时间，还不能执行这个任务的到期操作。**这里就有一个时间轮降级的操作**，会将这个剩余时间为 50ms 的定时任务重新提交到层级时间轮中，此时第一层时间轮的总体时间跨度不够，而第二层足够，所以该任务被放到第二层时间轮到期时间为 [40ms,60ms) 的时间格中。再经历 40ms 之后，此时这个任务又被“察觉”，不过还剩余 10ms，还是不能立即执行到期操作。所以还要再有一次时间轮的降级，此任务被添加到第一层时间轮到期时间为 [10ms,11ms) 的时间格中，之后再经历 10ms 后，此任务真正到期，最终执行相应的到期操作。

常见的钟表就是一种具有三层结构的时间轮，第一层时间轮 tickMs=1s、wheelSize=60、interval=1min，此为秒钟；第二层 tickMs=1min、wheelSize=60、interval=1hour，此为分钟；第三层 tickMs=1hour、wheelSize=12、interval=12hours，此为时钟。

在 Kafka 中，第一层时间轮的参数同上面的案例一样：tickMs=1ms、wheelSize=20、interval=20ms，各个层级的 wheelSize 也固定为20，所以各个层级的 tickMs 和 interval 也可以相应地推算出来。Kafka 在具体实现时间轮 TimingWheel 时还有一些小细节：

- TimingWheel 在创建的时候以当前系统时间为第一层时间轮的起始时间（startMs），这里的当前系统时间并没有简单地调用 System.currentTimeMillis()，而是调用了 Time.SYSTEM.hiResClockMs，这是因为 currentTimeMillis() 方法的时间精度依赖于操作系统的具体实现，有些操作系统下并不能达到毫秒级的精度，而 **Time.SYSTEM.hiResClockMs 实质上采用了 System.nanoTime()/1_000_000 来将精度调整到毫秒级**。
- TimingWheel 中的每个双向环形链表 TimerTaskList 都会有一个哨兵节点（sentinel），引入哨兵节点可以简化边界条件。哨兵节点也称为哑元节点（dummy node），它是一个附加的链表节点，该节点作为第一个节点，它的值域中并不存储任何东西，只是为了操作的方便而引入的。如果一个链表有哨兵节点，那么线性表的第一个元素应该是链表的第二个节点。
- 除了第一层时间轮，其余高层时间轮的起始时间（startMs）都设置为创建此层时间轮时前面第一轮的 currentTime。每一层的 currentTime 都必须是 tickMs 的整数倍，如果不满足则会将 currentTime 修剪为 tickMs 的整数倍，以此与时间轮中的时间格的到期时间范围对应起来。修剪方法为：**currentTime = startMs - (startMs % tickMs)**。currentTime 会随着时间推移而推进，但不会改变为 tickMs 的整数倍的既定事实。若某一时刻的时间为 timeMs，那么此时时间轮的 currentTime = timeMs - (timeMs % tickMs)，时间每推进一次，每个层级的时间轮的 currentTime 都会依据此公式执行推进。
- Kafka 中的定时器只需持有 TimingWheel 的第一层时间轮的引用，并不会直接持有其他高层的时间轮，但每一层时间轮都会有一个引用（overflowWheel）指向更高一层的应用，以此层级调用可以实现定时器间接持有各个层级时间轮的引用。

关于时间轮的细节就描述到这里，各个组件中对时间轮的实现大同小异。读者读到这里是否会好奇文中一直描述的一个情景—“随着时间的流逝”或“随着时间的推移”，那么在 Kafka 中到底是怎么推进时间的呢？类似采用 JDK 中的 scheduleAtFixedRate 来每秒推进时间轮？显然这样并不合理，TimingWheel 也失去了大部分意义。

==下面关于时间轮的介绍没有理解==

**Kafka 中的定时器借了 JDK 中的 [DelayQueue](https://so.csdn.net/so/search?q=DelayQueue&spm=1001.2101.3001.7020) 来协助推进时间轮。具体做法是对于每个使用到的 TimerTaskList 都加入 DelayQueue，“每个用到的 TimerTaskList”特指非哨兵节点的定时任务项 TimerTaskEntry 对应的 TimerTaskList。DelayQueue 会根据 TimerTaskList 对应的超时时间 expiration 来排序，最短 expiration 的 TimerTaskList 会被排在 DelayQueue 的队头**。

Kafka 中会有一个线程来获取 DelayQueue 中到期的任务列表，有意思的是这个线程所对应的名称叫作“ExpiredOperationReaper”，可以直译为“过期操作收割机”，和第4节的“SkimpyOffsetMap”的取名有异曲同工之妙。当“收割机”线程获取 DelayQueue 中超时的任务列表 TimerTaskList 之后，既可以根据 TimerTaskList 的 expiration 来推进时间轮的时间，也可以就获取的 TimerTaskList 执行相应的操作，对里面的 TimerTaskEntry 该执行过期操作的就执行过期操作，该降级时间轮的就降级时间轮。

读到这里或许会感到困惑，开头明确指明的 DelayQueue 不适合 Kafka 这种高性能要求的定时任务，为何这里还要引入 DelayQueue 呢？注意对定时任务项 TimerTaskEntry 的插入和删除操作而言，TimingWheel[时间复杂度](https://so.csdn.net/so/search?q=时间复杂度&spm=1001.2101.3001.7020)为 O(1)，性能高出 DelayQueue 很多，如果直接将 TimerTaskEntry 插入 DelayQueue，那么性能显然难以支撑。就算我们根据一定的规则将若干 TimerTaskEntry 划分到 TimerTaskList 这个组中，然后将 TimerTaskList 插入 DelayQueue，如果在 TimerTaskList 中又要多添加一个 TimerTaskEntry 时该如何处理呢？对 DelayQueue 而言，这类操作显然变得力不从心。

分析到这里可以发现，Kafka 中的 TimingWheel 专门用来执行插入和删除 TimerTaskEntry 的操作，而 DelayQueue 专门负责时间推进的任务。试想一下，DelayQueue 中的第一个超时任务列表的 expiration 为 200ms，第二个超时任务为 840ms，这里获取 DelayQueue 的队头只需要 O(1) 的时间复杂度（获取之后 DelayQueue 内部才会再次切换出新的队头）。如果采用每秒定时推进，那么获取第一个超时的任务列表时执行的200次推进中有199次属于“空推进”，而获取第二个超时任务时又需要执行639次“空推进”，这样会无故空耗机器的性能资源，这里采用 DelayQueue 来辅助以少量空间换时间，从而做到了“精准推进”。Kafka 中的定时器真可谓“知人善用”，用 TimingWheel 做最擅长的任务添加和删除操作，而用 DelayQueue 做最擅长的时间推进工作，两者相辅相成。

## 延时操作

如果在使用生产者客户端发送消息的时候将 acks 参数设置为-1，那么就意味着需要等待 ISR 集合中的所有副本都确认收到消息之后才能正确地收到响应的结果，或者捕获超时异常。

如下面3张图所示，假设某个分区有3个副本：leader、follower1 和 follower2，它们都在分区的 ISR 集合中。为了简化说明，这里我们不考虑 ISR 集合伸缩的情况。[Kafka](https://so.csdn.net/so/search?q=Kafka&spm=1001.2101.3001.7020) 在收到客户端的生产请求（ProduceRequest）后，将消息3和消息4写入 leader 副本的本地日志文件。由于客户端设置了 acks 为-1，那么需要等到 follower1 和 follower2 两个副本都收到消息3和消息4后才能告知客户端正确地接收了所发送的消息。如果在一定的时间内，follower1 副本或 follower2 副本没能够完全拉取到消息3和消息4，那么就需要返回超时异常给客户端。生产请求的超时时间由参数 request.timeout.ms 配置，默认值为30000，即30s。

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png.png" alt="6-9" style="zoom:50%;" />

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png-1679497555297-1.png" alt="6-10" style="zoom:50%;" />

<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png-1679497555298-2.png" alt="6-11" style="zoom:50%;" />



​    那么这里等待消息3和消息4写入 follower1 副本和 follower2 副本，并返回相应的响应结果给客户端的动作是由谁来执行的呢？在将消息写入 leader 副本的本地日志文件之后，Kafka 会创建一个延时的生产操作（DelayedProduce），用来处理消息正常写入所有副本或超时的情况，以返回相应的响应结果给客户端。

​    在 Kafka 中有多种延时操作，比如前面提及的延时生产，还有延时拉取（DelayedFetch）、延时数据删除（DelayedDeleteRecords）等。延时操作需要延时返回响应的结果，首先它必须有一个超时时间（delayMs），如果在这个超时时间内没有完成既定的任务，那么就需要强制完成以返回响应结果给客户端。其次，延时操作不同于定时操作，定时操作是指在特定时间之后执行的操作，而延时操作可以在所设定的超时时间之前完成，所以延时操作能够支持外部事件的触发。

​    就延时生产操作而言，它的外部事件是所要写入消息的某个分区的 HW（高水位）发生增长。也就是说，随着 follower 副本不断地与 leader 副本进行消息同步，进而促使HW进一步增长，HW 每增长一次都会检测是否能够完成此次延时生产操作，如果可以就执行以此返回响应结果给客户端；如果在超时时间内始终无法完成，则强制执行。

​    延时操作创建之后会被加入延时操作管理器（DelayedOperationPurgatory）来做专门的处理。延时操作有可能会超时，每个延时操作管理器都会配备一个定时器（SystemTimer）来做超时管理，定时器的底层就是采用时间轮（TimingWheel）实现的。在第7节中提及时间轮的轮转是靠“收割机”线程 ExpiredOperationReaper 来驱动的，这里的“收割机”线程就是由延时操作管理器启动的。也就是说，定时器、“收割机”线程和延时操作管理器都是一一对应的。延时操作需要支持外部事件的触发，所以还要配备一个监听池来负责监听每个分区的外部事件—查看是否有分区的HW发生了增长。另外需要补充的是，ExpiredOperationReaper 不仅可以推进时间轮，还会定期清理监听池中已完成的延时操作。

> 题外话：在 Kafka 中将延时操作管理器称为 DelayedOperationPurgatory，这个名称比之前提及的 ExpiredOperationReaper 和 SkimpyOffsetMap 的取名更有意思。Purgatory 直译为“炼狱”，但丁的《神曲》中有炼狱的相关描述。炼狱共有9层，在生前犯有罪过但可以得到宽恕的灵魂，按照人类的七宗罪（傲慢、忌妒、愤怒、怠惰、贪财、贪食、贪色）分别在这里修炼洗涤，而后一层层升向光明和天堂。Kafka中采用这一称谓，将延时操作看作需要被洗涤的灵魂，在炼狱中慢慢修炼，等待解脱升入天堂（即完成延时操作）。



<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png-1679497555298-3.png" alt="6-12" style="zoom:50%;" />

 



   上图描绘了客户端在请求写入消息到收到响应结果的过程中与延时生产操作相关的细节，在了解相关的概念之后应该比较容易理解：如果客户端设置的 acks 参数不为-1，或者没有成功的消息写入，那么就直接返回结果给客户端，否则就需要创建延时生产操作并存入延时操作管理器，最终要么由外部事件触发，要么由超时触发而执行。



<img src="%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png-1679497555298-4.png" alt="6-13" style="zoom:50%;" />

 



​    有延时生产就有延时拉取。以上图为例，两个 follower 副本都已经拉取到了 leader 副本的最新位置，此时又向 leader 副本发送拉取请求，而 leader 副本并没有新的消息写入，那么此时 leader 副本该如何处理呢？可以直接返回空的拉取结果给 follower 副本，不过在 leader 副本一直没有新消息写入的情况下，follower 副本会一直发送拉取请求，并且总收到空的拉取结果，这样徒耗资源，显然不太合理。

​    Kafka 选择了延时操作来处理这种情况。Kafka 在处理拉取请求时，会先读取一次日志文件，如果收集不到足够多（fetchMinBytes，由参数 fetch.min.bytes 配置，默认值为1）的消息，那么就会创建一个延时拉取操作（DelayedFetch）以等待拉取到足够数量的消息。当延时拉取操作执行时，会再读取一次日志文件，然后将拉取结果返回给 follower 副本。延时拉取操作也会有一个专门的延时操作管理器负责管理，大体的脉络与延时生产操作相同，不再赘述。如果拉取进度一直没有追赶上leader副本，那么在拉取 leader 副本的消息时一般拉取的消息大小都会不小于 fetchMinBytes，这样 Kafka 也就不会创建相应的延时拉取操作，而是立即返回拉取结果。

​    延时拉取操作同样是由超时触发或外部事件触发而被执行的。超时触发很好理解，就是等到超时时间之后触发第二次读取日志文件的操作。外部事件触发就稍复杂了一些，因为拉取请求不单单由 follower 副本发起，也可以由消费者客户端发起，两种情况所对应的外部事件也是不同的。如果是 follower 副本的延时拉取，它的外部事件就是消息追加到了 leader 副本的本地日志文件中；如果是消费者客户端的延时拉取，它的外部事件可以简单地理解为HW的增长。

​    目前版本的 Kafka 还引入了事务的概念，对于消费者或 follower 副本而言，其默认的事务隔离级别为“read_uncommitted”。不过消费者可以通过客户端参数 isolation.level 将事务隔离级别设置为“read_committed”（注意：follower 副本不可以将事务隔离级别修改为这个值），这样消费者拉取不到生产者已经写入却尚未提交的消息。对应的消费者的延时拉取，它的外部事件实际上会切换为由 LSO（LastStableOffset）的增长来触发。LSO 是 HW 之前除去未提交的事务消息的最大偏移量，LSO≤HW，有关事务的内容可以分别参考后面的章节。

​    本节主要讲述与日志（消息）存储有关的延时生产和延时拉取的操作，至于其他类型的延时操作就不一一介绍了，不过在讲解到相关内容时会做相应的阐述。

# 控制器

​     在 [Kafka](https://so.csdn.net/so/search?q=Kafka&spm=1001.2101.3001.7020) 集群中会有一个或多个 broker，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。当某个分区的 leader 副本出现故障时，由控制器负责为该分区选举新的 leader 副本。当检测到某个分区的 ISR 集合发生变化时，由控制器负责通知所有broker更新其元数据信息。当使用 kafka-topics.sh 脚本为某个 topic 增加分区数量时，同样还是由控制器负责分区的重新分配。

## 控制器的选举及异常恢复

​    Kafka 中的控制器选举工作依赖于 ZooKeeper，成功竞选为控制器的 broker 会在 ZooKeeper 中创建 /controller 这个临时（EPHEMERAL）节点，此临时节点的内容参考如下：

```cobol
{"version":1,"brokerid":0,"timestamp":"1529210278988"}
```

​    其中 version 在目前版本中固定为1，brokerid 表示成为控制器的 broker 的 id 编号，timestamp 表示竞选成为控制器时的时间戳。

​    在任意时刻，集群中有且仅有一个控制器。每个 broker 启动的时候会去尝试读取 /controller 节点的 brokerid 的值，如果读取到 brokerid 的值不为-1，则表示已经有其他 broker 节点成功竞选为控制器，所以当前 broker 就会放弃竞选；如果 ZooKeeper 中不存在 /controller 节点，或者这个节点中的数据异常，那么就会尝试去创建 /controller 节点。当前 broker 去创建节点的时候，也有可能其他 broker 同时去尝试创建这个节点，只有创建成功的那个 broker 才会成为控制器，而创建失败的 broker 竞选失败。每个 broker 都会在内存中保存当前控制器的 brokerid 值，这个值可以标识为 activeControllerId。

​       ZooKeeper 中还有一个与控制器有关的 /controller_epoch 节点，这个节点是持久（PERSISTENT）节点，节点中存放的是一个整型的 controller_epoch 值。controller_epoch 用于记录控制器发生变更的次数，即记录当前的控制器是第几代控制器，我们也可以称之为“控制器的纪元”。

​     controller_epoch 的初始值为1，即集群中第一个控制器的纪元为1，当控制器发生变更时，每选出一个新的控制器就将该字段值加1。每个和控制器交互的请求都会携带 controller_epoch 这个字段，如果请求的 controller_epoch 值小于内存中的 controller_epoch 值，则认为这个请求是向已经过期的控制器所发送的请求，那么这个请求会被认定为无效的请求。如果请求的 controller_epoch 值大于内存中的 controller_epoch 值，那么说明已经有新的控制器当选了。由此可见，Kafka 通过 controller_epoch 来保证控制器的唯一性，进而保证相关操作的一致性。

具备控制器身份的 broker 需要比其他普通的 broker 多一份职责，具体细节如下：

- 监听分区相关的变化。为 ZooKeeper 中的 /admin/reassign_partitions 节点注册 PartitionReassignmentHandler，用来处理分区重分配的动作。为 ZooKeeper 中的 /isr_change_notification 节点注册 IsrChangeNotificetionHandler，用来处理 ISR 集合变更的动作。为 ZooKeeper 中的 /admin/preferred-replica-election 节点添加 PreferredReplicaElectionHandler，用来处理优先副本的选举动作。
- 监听主题相关的变化。为 ZooKeeper 中的 /brokers/topics 节点添加 TopicChangeHandler，用来处理主题增减的变化；为 ZooKeeper中 的 /admin/delete_topics 节点添加 TopicDeletionHandler，用来处理删除主题的动作。
- 监听 broker 相关的变化。为 ZooKeeper 中的 /brokers/ids 节点添加 BrokerChangeHandler，用来处理 broker 增减的变化。
- 从 ZooKeeper 中读取获取当前所有与主题、分区及broker有关的信息并进行相应的管理。对所有主题对应的 ZooKeeper中的 /brokers/topics/<topic> 节点添加 PartitionModificationsHandler，用来监听主题中的分区分配变化。
- 启动并管理分区状态机和副本状态机。
- 更新集群的元数据信息。
- 如果参数 auto.leader.rebalance.enable 设置为 true，则还会开启一个名为“auto-leader-rebalance-task”的定时任务来负责维护分区的优先副本的均衡。

​        控制器在选举成功之后会读取 ZooKeeper 中各个节点的数据来初始化上下文信息（ControllerContext），并且需要管理这些上下文信息。比如为某个主题增加了若干分区，控制器在负责创建这些分区的同时要更新上下文信息，并且需要将这些变更信息同步到其他普通的 broker 节点中。

​       不管是监听器触发的事件，还是定时任务触发的事件，或者是其他事件（比如 ControlledShutdown）都会读取或更新控制器中的上下文信息，那么这样就会涉及多线程间的同步。如果单纯使用锁机制来实现，那么整体的性能会大打折扣。针对这一现象，Kafka 的控制器使用单线程基于事件队列的模型，将每个事件都做一层封装，然后按照事件发生的先后顺序暂存到 LinkedBlockingQueue 中，最后使用一个专用的线程（ControllerEventThread）按照 FIFO（First Input First Output，先入先出）的原则顺序处理各个事件，这样不需要锁机制就可以在多线程间维护线程安全，具体可以参考下图。



![6-14](%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png-1679498703751-15.png)

 



​     在 Kafka 的早期版本中，并没有采用 Kafka Controller 这样一个概念来对分区和副本的状态进行管理，而是依赖于 ZooKeeper，每个 broker 都会在 ZooKeeper 上为分区和副本注册大量的监听器（Watcher）。当分区或副本状态变化时，会唤醒很多不必要的监听器，这种严重依赖 ZooKeeper 的设计会有脑裂、羊群效应，以及造成 ZooKeeper 过载的隐患（旧版的消费者客户端存在同样的问题）。

​    在目前的新版本的设计中，只有 Kafka Controller 在 ZooKeeper 上注册相应的监听器，其他的 broker 极少需要再监听 ZooKeeper 中的数据变化，这样省去了很多不必要的麻烦。不过每个 broker 还是会对 /controller 节点添加监听器，以此来监听此节点的数据变化（ControllerChangeHandler）。

​     当 /controller 节点的数据发生变化时，每个 broker 都会更新自身内存中保存的 activeControllerId。如果 broker 在数据变更前是控制器，在数据变更后自身的 brokerid 值与新的 activeControllerId 值不一致，那么就需要“退位”，关闭相应的资源，比如关闭状态机、注销相应的监听器等。有可能控制器由于异常而下线，造成 /controller 这个临时节点被自动删除；也有可能是其他原因将此节点删除了。

​     当 /controller 节点被删除时，每个 broker 都会进行选举，如果 broker 在节点被删除前是控制器，那么在选举前还需要有一个“退位”的动作。如果有特殊需要，则可以手动删除 /controller 节点来触发新一轮的选举。当然关闭控制器所对应的 broker，以及手动向 /controller 节点写入新的 brokerid 的所对应的数据，同样可以触发新一轮的选举。

## 优雅关闭

​     如何优雅地关闭 Kafka？笔者在做测试的时候经常性使用 jps（或者 ps ax）配合 kill -9 的方式来快速关闭 Kafka broker 的服务进程，显然 kill -9 这种“强杀”的方式并不够优雅，它并不会等待 Kafka 进程合理关闭一些资源及保存一些运行数据之后再实施关闭动作。在有些场景中，用户希望主动关闭正常运行的服务，比如更换硬件、操作系统升级、修改 Kafka 配置等。如果依然使用上述方式关闭就略显粗暴。

​    那么合理的操作应该是什么呢？Kafka 自身提供了一个脚本工具，就是存放在其 bin 目录下的 kafka-server-stop.sh，这个脚本的内容非常简单，具体内容如下：

```perl
PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $1}')



 



if [ -z "$PIDS" ]; then



  echo "No kafka server to stop"



  exit 1



else 



  kill -s TERM $PIDS



fi
```

​     可以看出 kafka-server-stop.sh 首先通过 ps ax 的方式找出正在运行 Kafka 的进程号 PIDS，然后使用 kill -s TERM $PIDS 的方式来关闭。但是这个脚本在很多时候并不奏效，这一点与 ps 命令有关系。在 Linux 操作系统中，ps 命令限制输出的字符数不得超过页大小 PAGE_SIZE，一般 CPU 的内存管理单元（Memory Management Unit，简称 MMU）的 PAGE_SIZE 为4096。也就是说，ps 命令的输出的字符串长度限制在4096内，这会有什么问题呢？我们使用 ps ax 命令来输出与 Kafka 进程相关的信息，如下图所示。



![6-15](%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png-1679498703752-16.png)

 



​    细心的读者可以留意到白色部分中的信息并没有打印全，因为已经达到了4096的字符数的限制。而且打印的信息里面也没有 kafka-server-stop.sh 中 ps ax | grep -i 'kafka.Kafka' 所需要的“kafka.Kafka”这个关键字段，因为这个关键字段在4096个字符的范围之外。与Kafka进程有关的输出信息太长，所以 kafka-server-stop.sh 脚本在很多情况下并不会奏效。

> 注意要点：Kafka 服务启动的入口就是 kafka.Kafka，采用 Scala 语言编写 object。

​    那么怎么解决这种问题呢？我们先来看一下ps命令的相关源码（Linux 2.6.x 源码的 /fs/proc/base.c 文件中的部分内容）：

```cobol
static int proc_pid_cmdline(struct task_struct *task, char * buffer)



{



   int res = 0;



   unsigned int len;



   struct mm_struct *mm = get_task_mm(task);



   if (!mm)



      goto out;



   if (!mm->arg_end)



      goto out_mm;   /* Shh! No looking before we're done */







   len = mm->arg_end - mm->arg_start;



 



   if (len > PAGE_SIZE)



      len = PAGE_SIZE;



 



   res = access_process_vm(task, mm->arg_start, buffer, len, 0);



（....省略若干....）
```

​     我们可以看到 ps 的输出长度 len 被硬编码成小于等于 PAG_SIZE 的大小，那么我们调大这个 PAGE_SIZE 的大小不就可以了吗？这样是肯定行不通的，因为对于一个 CPU 来说，它的 MMU 的页大小 PAGE_SIZE 的值是固定的，无法通过参数调节。要想改变 PAGE_SIZE 的大小，就必须更换成相应的 CPU，显然这也太过于“兴师动众”了。还有一种办法是，将上面代码中的 PAGE_SIZE 换成一个更大的其他值，然后重新编译，这个办法对于大多数人来说不太适用，需要掌握一定深度的 Linux 的相关知识。

​    那么有没有其他的办法呢？这里我们可以直接修改 kafka-server-stop.sh 脚本的内容，将其中的第一行命令修改如下：

```perl
PIDS=$(ps ax | grep -i 'kafka' | grep java | grep -v grep | awk '{print $1}')
```

​    即把“.Kafka”去掉，这样在绝大多数情况下是可以奏效的。如果有极端情况，即使这样也不能关闭，那么只需要按照以下两个步骤就可以优雅地关闭 Kafka 的服务进程：

1. 获取 Kafka 的服务进程号 PIDS。可以使用 Java 中的 jps 命令或使用 Linux 系统中的 ps 命令来查看。
2. 使 用kill -s TERM $PIDS 或 kill -15 $PIDS 的方式来关闭进程，注意千万不要使用 kill -9 的方式。

​    为什么这样关闭的方式会是优雅的？Kafka 服务入口程序中有一个名为“kafka-shutdown- hock”的关闭钩子，待 Kafka 进程捕获终止信号的时候会执行这个关闭钩子中的内容，其中除了正常关闭一些必要的资源，还会执行一个控制关闭（ControlledShutdown）的动作。使用 ControlledShutdown 的方式关闭 Kafka 有两个优点：一是可以让消息完全同步到磁盘上，在服务     下次重新上线时不需要进行日志的恢复操作；二是 ControllerShutdown 在关闭服务之前，会对其上的 leader 副本进行迁移，这样就可以减少分区的不可用时间。

​     若要成功执行 ControlledShutdown 动作还需要有一个先决条件，就是参数 controlled.shutdown.enable 的值需要设置为true，不过这个参数的默认值就为 true，即默认开始此项功能。ControlledShutdown 动作如果执行不成功还会重试执行，这个重试的动作由参数 controlled.shutdown.max.retries 配置，默认为3次，每次重试的间隔由参数 controlled.shutdown.retry.backoff.ms 设置，默认为5000ms。

下面我们具体探讨 ControlledShutdown 的整个执行过程。



![6-16](%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png-1679498703752-17.png)

 



​     参考上图，假设此时有两个 broker，其中待关闭的 broker 的 id 为 x，Kafka 控制器所对应的 broker 的 id 为 y。待关闭的 broker 在执行 ControlledShutdown 动作时首先与 Kafka 控制器建立专用连接（对应上图中的步骤①），然后发送 ControlledShutdownRequest 请求，ControlledShutdownRequest 请求中只有一个 brokerId 字段，这个 brokerId 字段的值设置为自身的 brokerId 的值，即 x（对应上图中的步骤②）。

​    Kafka 控制器在收到 ControlledShutdownRequest 请求之后会将与待关闭 broker 有关联的所有分区进行专门的处理，这里的“有关联”是指分区中有副本位于这个待关闭的 broker 之上（这里会涉及 Kafka 控制器与待关闭 broker 之间的多次交互动作，涉及 leader 副本的迁移和副本的关闭动作，对应上图中的步骤③）。

ControlledShutdownRequest 的结构如下图所示。



![6-17](%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png-1679498703753-18.png)

 



​    如果这些分区的副本数大于1且 leader 副本位于待关闭 broker 上，那么需要实施 leader 副本的迁移及新的 ISR 的变更。具体的选举分配的方案由专用的选举器 ControlledShutdownLeaderSelector 提供。

​     如果这些分区的副本数只是大于1，leader 副本并不位于待关闭 broker 上，那么就由 Kafka 控制器来指导这些副本的关闭。如果这些分区的副本数只是为1，那么这个副本的关闭动作会在整个 ControlledShutdown 动作执行之后由副本管理器来具体实施。

​    对于分区的副本数大于1且 leader 副本位于待关闭 broker 上的这种情况，如果在 Kafka 控制器处理之后 leader 副本还没有成功迁移，那么会将这些没有成功迁移 leader 副本的分区记录下来，并且写入 ControlledShutdownResponse 的响应（对应往上第二张图中的步骤④，整个 ControlledShutdown 动作是一个同步阻塞的过程）。ControlledShutdownResponse 的结构如下图所示。



![6-18](%E5%85%AD%E3%80%81%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF.resource/format,png-1679498703753-19.png)

 



​      待关闭的 broker 在收到 ControlledShutdownResponse 响应之后，需要判断整个 ControlledShutdown 动作是否执行成功，以此来进行可能的重试或继续执行接下来的关闭资源的动作。执行成功的标准是 ControlledShutdownResponse 中 error_code 字段值为0，并且 partitions_remaining 数组字段为空。

> 注意要点：往上第三张图中也有可能 x=y，即待关闭的 broker同时是 Kafka 控制器，这也就意味着自己可以给自己发送 ControlledShutdownRequest 请求，以及等待自身的处理并接收 ControlledShutdownResponse 的响应，具体的执行细节和 x!=y 的场景相同。

​     在了解了整个 ControlledShutdown 动作的具体细节之后，我们不难看出这一切实质上都是由 ControlledShutdownRequest 请求引发的，我们完全可以自己开发一个程序来连接 Kafka 控制器，以此来模拟对某个 broker 实施 ControlledShutdown 的动作。为了实现方便，我们可以对 KafkaAdminClient 做一些扩展来达到目的。

首先参考 org.apache.kafka.clients.admin.AdminClient 接口中的惯有编码样式来添加两个方法：

```java
    public abstract ControlledShutdownResult controlledShutdown(



            Node node, final ControlledShutdownOptions options);



 



    public ControlledShutdownResult controlledShutdown(Node node){



        return controlledShutdown(node, new ControlledShutdownOptions());



    }
```

​     第一个方法中的 ControlledShutdownOptions 和 ControlledShutdownResult 都是 KafkaAdminClient 的惯有编码样式，ControlledShutdownOptions 中没有实质性的内容，具体参考如下：

```scala
@InterfaceStability.Evolving



public class ControlledShutdownOptions extends 



AbstractOptions<ControlledShutdownOptions> {



}
```

ControlledShutdownResult 的实现如下：

```cobol
@InterfaceStability.Evolving



public class ControlledShutdownResult {



    private final KafkaFuture<ControlledShutdownResponse> future;



    public ControlledShutdownResult(



    KafkaFuture<ControlledShutdownResponse> future) {



        this.future = future;



    }



    public KafkaFuture<ControlledShutdownResponse> values(){



        return future;



    }



}
```

​      ControlledShutdownResult 中没有像 KafkaAdminClient 中惯有的那样对 ControlledShutdownResponse 进行细致化的处理，而是直接将 ControlledShutdownResponse 暴露给用户，这样用户可以更加细腻地操控内部的细节。

​    第二个方法中的参数 Node 是我们需要执行 ControlledShutdown 动作的 broker 节点，Node 的构造方法至少需要三个参数：id、host 和 port，分别代表所对应的 broker 的 id 编号、IP 地址和端口号。一般情况下，对用户而言，并不一定清楚这个三个参数的具体值，有的要么只知道要关闭的 broker 的 IP 地址和端口号，要么只清楚具体的 id 编号，为了程序的通用性，我们还需要做进一步的处理。详细看一下 org.apache.kafka.clients.admin.KafkaAdminClient 中的具体做法：

```cobol
public ControlledShutdownResult controlledShutdown(



            Node node,



            final ControlledShutdownOptions options) {



        final KafkaFutureImpl<ControlledShutdownResponse> future 



                = new KafkaFutureImpl<>();



        final long now = time.milliseconds();



 



        runnable.call(new Call("controlledShutdown", 



                calcDeadlineMs(now, options.timeoutMs()),



                new ControllerNodeProvider()) {



            @Override



            AbstractRequest.Builder createRequest(int timeoutMs) {



                int nodeId = node.id();



                if (nodeId < 0) {



                    List<Node> nodes = metadata.fetch().nodes();



                    for (Node nodeItem : nodes) {



                        if (nodeItem.host().equals(node.host()) 



                                && nodeItem.port() == node.port()) {



                            nodeId = nodeItem.id();



                            break;



                        }



                    }



                }



                return new ControlledShutdownRequest.Builder(nodeId, 



                        ApiKeys.CONTROLLED_SHUTDOWN.latestVersion());



            }



 



            @Override



            void handleResponse(AbstractResponse abstractResponse) {



                ControlledShutdownResponse response = 



                        (ControlledShutdownResponse) abstractResponse;



                future.complete(response);



            }



 



            @Override



            void handleFailure(Throwable throwable) {



                future.completeExceptionally(throwable);



            }



        }, now);



 



        return new ControlledShutdownResult(future);



    }
```

​      我们可以看到在内部的 createRequest 方法中对 Node 的 id 做了一些处理，因为对 ControlledShutdownRequest 协议的包装只需要这个 id 的值。程序中首先判断 Node 的 id 是否大于0，如果不是则需要根据 host 和 port 去 KafkaAdminClient 缓存的[元数据](https://so.csdn.net/so/search?q=元数据&spm=1001.2101.3001.7020) metadata 中查找匹配的 id。注意到代码里还有一个标粗的 ControllerNodeProvider，它提供了 Kafka 控制器对应的节点信息，这样用户只需要提供 Kafka 集群中的任意节点的连接信息，不需要知晓具体的 Kafka 控制器是谁。

​    最后我们再用一段测试程序来模拟发送 ControlledShutdownRequest 请求及处理 ControlledShutdownResponse，详细参考如下：

```cobol
String brokerUrl = "hostname1:9092";



Properties props = new Properties();



props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, brokerUrl);



//1. 初始化KafkaAdminClient



AdminClient adminClient = AdminClient.create(props);



 



//2. 需要关闭的节点node，暂不清楚node的id，故设置为-1



Node node = new Node(-1, "hostname2", 9092);



//3. 使用KafkaAdminClient发送ControlledShutdownRequest请求及



//阻塞等待ControlledShutdownResponse响应



ControlledShutdownResponse response =



        adminClient.controlledShutdown(node).values().get();



if (response.error() == Errors.NONE



        && response.partitionsRemaining().isEmpty()) {



    System.out.println("controlled shutdown completed");



}else {



    System.out.println("controlled shutdown occured error with: "



            + response.error().message());



}
```

​    其中 brokerUrl 是连接的任意节点，node 是需要关闭的 broker 节点，当然这两个可以是同一个节点，即代码中的 hostname1 等于 hostname2。使用 KafkaAdminClient 的整个流程为：首先连接集群中的任意节点；接着通过这个连接向 Kafka 集群发起元数据请求（MetadataRequest）来获取集群的元数据 metadata；然后获取需要关闭的 broker 节点的 id，如果没有指定则去 metadata 中查找，根据这个 id 封装 ControlledShutdownRequest 请求；之后再去 metadata 中查找 Kafka 控制器的节点，向这个 Kafka 控制器节点发送请求；最后等待 Kafka 控制器的 ControlledShutdownResponse 响应并做相应的处理。

​    注意 ControlledShutdown 只是关闭 Kafka broker 的一个中间过程，所以不能寄希望于只使用 ControlledShutdownRequest 请求就可以关闭整个 Kafka broker 的服务进程。

## 分区leader的选举

​    分区 leader 副本的选举由控制器负责具体实施。当创建分区（创建主题或增加分区都有创建分区的动作）或分区上线（比如分区中原先的 leader 副本下线，此时分区需要选举一个新的 leader 上线来对外提供服务）的时候都需要执行 leader 的选举动作，对应的选举策略为     OfflinePartitionLeaderElectionStrategy。这种策略的基本思路是按照 AR [集合](https://so.csdn.net/so/search?q=集合&spm=1001.2101.3001.7020)中副本的顺序查找第一个存活的副本，并且这个副本在 ISR 集合中。

​    一个分区的 AR 集合在分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的 ISR 集合中副本的顺序可能会改变。

​    注意这里是根据 AR 的顺序而不是 ISR 的顺序进行选举的。举个例子，集群中有3个节点：broker0、broker1 和 broker2，在某一时刻具有3个分区且副本因子为3的主题 topic-leader 的具体信息如下：

```cobol
[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-leader



Topic:topic-leader	PartitionCount:3	ReplicationFactor:3	Configs: 



    Topic: topic-leader	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 2,0,1



    Topic: topic-leader	Partition: 1	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1



    Topic: topic-leader	Partition: 2	Leader: 0	Replicas: 0,1,2	Isr: 0,2,1
```

​    此时关闭 broker0，那么对于分区2而言，存活的 AR 就变为[1,2]，同时 ISR 变为[2,1]。此时查看主题 topic-leader 的具体信息（参考如下），分区2的 leader 就变为了1而不是2。

```cobol
[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-leader



Topic:topic-leader	PartitionCount:3	ReplicationFactor:3	Configs: 



    Topic: topic-leader	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 2,1



    Topic: topic-leader	Partition: 1	Leader: 2	Replicas: 2,0,1	Isr: 2,1



    Topic: topic-leader	Partition: 2	Leader: 1	Replicas: 0,1,2	Isr: 2,1
```

​     如果 ISR 集合中没有可用的副本，那么此时还要再检查一下所配置的 unclean.leader.election.enable 参数（默认值为 false）。如果这个参数配置为 true，那么表示允许从非 ISR 列表中的选举 leader，从 AR 列表中找到第一个存活的副本即为 leader。

   当分区进行重分配的时候也需要执行 leader 的选举动作，对应的选举策略为 ReassignPartitionLeaderElectionStrategy。这个选举策略的思路比较简单：从重分配的AR列表中找到第一个存活的副本，且这个副本在目前的 ISR 列表中。

​    当发生优先副本的选举时，直接将优先副本设置为 leader 即可，AR 集合中的第一个副本即为优先副本（PreferredReplicaPartitionLeaderElectionStrategy）。

​    还有一种情况会发生 leader 的选举，当某节点被优雅地关闭（也就是执行 ControlledShutdown）时，位于这个节点上的 leader 副本都会下线，所以与此对应的分区需要执行 leader 的选举。与此对应的选举策略（ControlledShutdownPartitionLeaderElectionStrategy）为：从 AR 列表中找到第一个存活的副本，且这个副本在目前的 ISR 列表中，与此同时还要确保这个副本不处于正在被关闭的节点上。