# 五、日志存储

## 一、文件目录布局

Kafka 中的消息是以主题为基本单位进行归类的，各个主题在逻辑上相互独立。每个主题又可以分为一个或多个分区。每条消息在发送的时候会根据分区规则被追加到指定的分区中，分区中的每条消息都会被分配一个唯一的序列号即偏移量（offset）。 如果分区规则设置得合理，那么所有的消息可以均匀地分布到不同的分区中，这样就可以实现水平扩展。不考虑多副本的情况，一个分区对应一个日志（Log）。为了防止 Log 过大，Kafka 又引入了日志分段（LogSegment）的概念，将 Log 切分为多个 LogSegment，相当于一个巨型文件被平均分配为多个相对较小的文件，这样也便于消息的维护和清理。

Log 在物理上只以文件夹的形式存储，而每个 LogSegment 对应于磁盘上的一个日志文件和两个索引文件，以及可能的其他文件（比如以“.txnindex”为后缀的事务索引文件），即 LogSegment 逻辑上存在，物理上不存在。主题、分区、副本、Log 以及 LogSegment 之间的关系如下。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230125211207757.png" alt="image-20230125211207757" style="zoom:50%;" />

Log 对应了一个命名形式为 `<topic>-<partition>`的文件夹。

```shell
## 拥有 4 个分区的 topic-log 主题在物理存储上为 4 个文件夹
[root@node1 kafka-logs]# ls -al | grep topic-log
drwxr-xr-x   2 root root 4096 May 16 18:33 topic-log-0
drwxr-xr-x   2 root root 4096 May 16 18:33 topic-log-1
drwxr-xr-x   2 root root 4096 May 16 18:33 topic-log-2
drwxr-xr-x   2 root root 4096 May 16 18:33 topic-log-3
```


同时向 Log 中追加消息时是顺序写入的，只有最后一个 LogSegment 才能执行写入操作，在此之前所有的 LogSegment 都不能写入数据。将最后一个 LogSegment 称为“activeSegment”，即表示当前活跃的日志分段。随着消息的不断写入，当 activeSegment 满足一定的条件时，就需要创建新的 activeSegment，之后追加的消息将写入新的 activeSegment。

 为了便于消息的检索，每个 LogSegment 中的日志文件（以 `.log` 为文件后缀）都有对应的两个索引文件：偏移量索引文件（以 `.index` 为文件后缀）和时间戳索引文件（以 `.timeindex` 为文件后缀）。每个 LogSegment 都有一个基准偏移量 baseOffset，用来表示当前 LogSegment 中第一条消息的 offset。偏移量是一个 64 位的长整型数，日志文件和两个索引文件都是根据基准偏移量（baseOffset）命名的，名称固定为20位数字，没有达到的位数则用0填充。比如第一个 LogSegment 的基准偏移量为0，对应的日志文件为00000000000000000000.log。

举例说明，向主题 topic-log 中发送一定量的消息，某一时刻 topic-log-0 目录中的布局如下所示。

```shell
-rw-r--r-- 1 root root       400 May 15 19:43 	00000000000000000000.index
-rw-r--r-- 1 root root      5111 May 15 19:43 	00000000000000000000.log
-rw-r--r-- 1 root root       600 May 15 19:43 	00000000000000000000.timeindex
## 该 LogSegment 对应的基准位移为 133，即该 LogSegment 中第一条消息的偏移量为 133，同样上一个 LogSegment 中有133 条消息（偏移量从 0 至 132 的消息）。
-rw-r--r-- 1 root root       296 May 16 18:33 	00000000000000000133.index
-rw-r--r-- 1 root root      4085 May 16 18:33 	00000000000000000133.log
-rw-r--r-- 1 root root       444 May 16 18:33 	00000000000000000133.timeindex
```

每个 LogSegment 中不只包含 `.log`、`.index`、`.timeindex` 这 3 种文件，还可能包含 `.deleted`、`.cleaned`、`.swap` 等临时文件，以及可能的 `.snapshot`、`.txnindex`、`leader-epoch-checkpoint` 等文件。

当 Kafka 服务第一次启动的时候，还有一些检查点文件，即默认的根目录下就会创建以下5个文件：

```shell
[root@node1 kafka-logs]# ls
cleaner-offset-checkpoint  log-start-offset-checkpoint  meta.properties  recovery-point-offset-checkpoint  replication-offset-checkpoint
```


消费者提交的位移是保存在 Kafka 内部的主题 `__consumer_offsets` 中的，初始情况下这个主题并不存在，当第一次有消费者消费消息时会自动创建这个主题。

在某一时刻，Kafka 中的文件目录布局图所示。每一个根目录都会包含最基本的 4 个检查点文件（xxx-checkpoint）和 meta.properties 文件。在创建主题的时候，如果当前 broker 中不止配置了一个根目录，那么会挑选分区数最少的那个根目录来完成本次创建任务。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230126105742182.png" alt="image-20230126105742182" style="zoom: 33%;" />

## 二、日志格式的演变

消息中间件的消息格式（或者称为“日志格式”）关系功能维度的扩展与性能维度（存储和传输）的优化。kafka 从 0.8.x - 0.10.0（不含） 使用 V0 版本，0.10.0.0（含）-0.11.0.0（不含） 使用 V1 版本，0.11.0.0 及其往后使用 V2 版本。

### v0版本
默认情况下，消息均为未压缩情形。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230126135336484.png" alt="image-20230126135336484" style="zoom:33%;" />

完整消息格式包括日志头部（LOG_OVERHEAD）和 RECORD 部分。消息集中包含一条或多条消息，消息集不仅是存储于磁盘及在网络上传输（Produce & Fetch）的基本形式，而且是 Kafka 中压缩的基本单元，详细结构参考上图中的右边部分。

- 日志头部：
    - offset：每条消息都有一个 offset（逻辑值，非实际物理偏移量）用来标志它在分区中的偏移量；
    - message size 表示消息的大小，两者合并大小为固定 12B。
- 消息具体的 RECORD 部分：
    - crc32（4B）：crc32 校验值。校验范围为 magic 至 value 之间。用于确保消息在传输过程中不会被篡改。
    - magic（1B）：消息格式版本号，此版本的 magic 值为 0。
    - attributes（1B）：消息的属性。总共占1个字节，低3位表示压缩类型：0 表示 NONE、1 表示 GZIP、2 表示 SNAPPY、3 表示 LZ4（LZ4 自 Kafka 0.9.x引入），其余位保留。
    - key length（4B）：表示消息的 key 的长度。如果为-1，则表示没有设置 key，即 key = null。
    - key：可选，如果没有 key 则无此字段。
    - value length（4B）：实际消息体的长度。如果为-1，则表示消息为空。
    - value：消息体。可以为空，比如墓碑（tombstone）消息。      

v0 版本中一个消息的最小长度（RECORD_OVERHEAD_V0）（不包括头部）为 crc32 + magic + attributes + key length + value length = 4B + 1B + 1B + 4B + 4B =14B。也就是说，v0 版本中一条消息的最小长度为 14B，如果小于这个值，那么这就是一条破损的消息而不被接收。

示例：创建一个分区数和副本因子都为 1 的主题，名称为“msg_format_v0”，然后往msg_format_v0中发送一条key ="key"、value = "value"的消息，之后查看对应的日志（这里采用 Kafka 0.8.2.1的版本）：

```shell
[root@node1 kafka_2.10-0.8.2.1]# bin/kafka-run-class.sh 
     kafka.tools.DumpLogSegments --files 
     /tmp/kafka-logs/msg_format_v0-0/00000000000000000000.log
## kafka 2.0 版本后可以使用 bin/kafka-dump-log.sh --files 替换  bin/kafka-run-class.sh     kafka.tools.DumpLogSegments --files  使用
     
Dumping /tmp/kafka-logs-08/msg_format_v0-0/00000000000000000000.log
Starting offset: 0
offset: 0 position: 0 isvalid: true payloadsize: 5 magic: 0 
compresscodec: NoCompressionCodec crc: 592888119 keysize: 3
## 查看该 log 大小为 34B，其值正好等于 LOG_OVERHEAD + RECORD_OVERHEAD_V0 + 3B的 key + 5B的 value = 12B + 14B + 3B + 5B = 34B。
```

#### 代码

对应 org.apache.kafka.common.record.Record 类，

```java
public final class Record {
    // 具体字段长度的定义
    public static final int CRC_OFFSET = 0;
    public static final int CRC_LENGTH = 4;
    public static final int MAGIC_OFFSET = 4;
    public static final int MAGIC_LENGTH = 1;
    public static final int ATTRIBUTES_OFFSET = 5;
    public static final int ATTRIBUTE_LENGTH = 1;
    public static final int KEY_SIZE_OFFSET = 6;
    public static final int KEY_SIZE_LENGTH = 4;
    public static final int KEY_OFFSET = 10;
    public static final int VALUE_SIZE_LENGTH = 4;
    public static final int HEADER_SIZE = 6;
    public static final int RECORD_OVERHEAD = 14;
    public static final byte CURRENT_MAGIC_VALUE = 0;
    public static final int COMPRESSION_CODEC_MASK = 7;
    public static final int NO_COMPRESSION = 0;
    // 使用 ByteBuffer（是一种紧凑的二进制字节结构）来保存消息，而非 Java 类
    private final ByteBuffer buffer;

    // kafka 消息通过 writeTo 方法将消息写入 ByteBuffer
    public static void write(Compressor compressor, long crc, byte attributes, byte[] key, byte[] value, int valueOffset, int valueSize) {
        // 写 crc
        compressor.putInt((int)(crc & 4294967295L));
		// 写 magic
        compressor.putByte((byte)0);
        // 写 attributes
        compressor.putByte(attributes);
        // 写 key 与 keyLength
        if (key == null) {
            compressor.putInt(-1);
        } else {
            compressor.putInt(key.length);
            compressor.put(key, 0, key.length);
        }
        // 写 value 和 valueLength
        if (value == null) {
            compressor.putInt(-1);
        } else {
            int size = valueSize >= 0 ? valueSize : value.length - valueOffset;
            compressor.putInt(size);
            compressor.put(value, valueOffset, size);
        }
    }
}
```

### v1版本

v1 比 v0 版本就多了一个 timestamp 字段，表示消息的时间戳。v1 版本的消息结构如下图所示。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230126154605121.png" alt="image-20230126154605121" style="zoom:33%;" />

v1 版本的 magic 字段的值为 1。v1 版本的 attributes 字段中的低 3 位和 v0 版本的一样，还是表示压缩类型，而第4个位（bit）也被利用了起来：0表示 timestamp 类型为 CreateTime，而1表示 timestamp 类型为 LogAppendTime，其他位保留。timestamp 类型由 broker 端参数 `log.message.timestamp.type` 来配置，默认值为 CreateTime，即采用生产者创建消息时的时间戳。如果在创建 ProducerRecord 时没有显式指定消息的时间戳，那么 KafkaProducer 也会在发送这条消息前自动添加上。

```java
## 下面是 KafkaProducer 中与此对应的一句关键代码：
long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();
```

v1 版本的消息的最小长度（RECORD_OVERHEAD_V1）要比v0版本的大8个字节，即22B。

同时 V1 版本的 Record 中的 write 方法，多了下面的 timestamp 的赋值操作：

```java
// write timestamp
compressor.putLong(timestamp);
```

### 消息集

V0、V1 版本的消息集合的设计没有任何区别，被称作“日志项”，对应源码中的 `org.apache.kafka.common.record.LogEntry` 类：

```java
public final class LogEntry {
    // 记录消息在 kafka 分区日志中的 offset
    private final long offset;
    // 消息本身
    private final Record record;

	// 去除构造函数、toString 等
    
    // 记录消息集合的长度
    public int size() {
        return this.record.size() + 12;
    }
}
```

其中 size() 方法中的 12 对应 org.apache.kafka.common.record.Records 类中的 LOG_OVERHEAD 参数:

```java
public interface Records extends Iterable<LogEntry> {
  // 消息集大小字段占 4 字节
  int SIZE_LENGTH = 4;
  // 消息集中的 offset 占 8 字节
  int OFFSET_LENGTH = 8;
  int LOG_OVERHEAD = SIZE_LENGTH + OFFSET_LENGTH;
}
```

### 消息压缩

常见的压缩算法是数据量越大压缩效果越好，但是一条消息通常不会太大，导致压缩效果并不是太好。因此 Kafka 是将多条消息一起进行压缩。

**在一般情况下，生产者发送的压缩数据在 broker 中也是保持压缩状态进行存储的，消费者从服务端获取的也是压缩的消息，消费者在处理消息之前才会解压消息，这样保持了端到端的压缩。**

Kafka 日志中使用哪种压缩方式是通过参数 `compression.type` 来配置的，默认值为 `producer`，表示保留生产者使用的压缩方式。其它可选值包括：`gzip`、`snappy`、`lz4`、`uncompressed`，分别对应 GZIP、SNAPPY、LZ4 这 3 种压缩算法和不压缩。

注意：压缩率是压缩后的大小与压缩前的对比。例如：把 100MB 的文件压缩后是 90 MB，压缩率为 90/100×100% = 90%，压缩率越小，压缩效果越好。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230126160912138.png" alt="image-20230126160912138" style="zoom:33%;" />

消息压缩时是将整个消息集进行压缩作为内层消息（inner message），内层消息整体作为外层（wrapper message）的 value，其结构如图所示。压缩后的外层消息（wrapper message）中的 key 为 null，所以上图左半部分没有画出 key 字段，value 字段中保存的是多条压缩消息（inner message，内层消息），**其中 Record 表示的是从 crc32 到 value 的消息格式**。当生产者创建压缩消息时候，对内部压缩消息设置的 offset 从 0 开始为每个内部消息分配 offset，详细可以参考下图右半部分。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230126163647039.png" alt="image-20230126163647039" style="zoom:33%;" />

其实每个从生产者发出的消息集中的消息 offset 都是从 0 开始的，当然这个 offset 不能直接存储在日志文件中，对 offset 的转换是在服务端进行的，客户端不需要做这个工作。外层消息保存了内层消息中最后一条消息的绝对位移（absolute offset），绝对位移是相对于整个分区而言的。

示例：对于未压缩的情形，图右内层消息中最后一条的 offset 理应是1030，但被压缩之后就变成了 5，而这个 1030 被赋予给了外层的 offset。当消费者消费这个消息集的时候，首先解压缩整个消息集，然后找到内层消息中最后一条消息的 inner offset，根据如下公式找到内层消息中最后一条消息前面的消息的 absolute offset（RO 表示 Relative Offset，IO 表示 Inner Offset，而 AO 表示 Absolute Offset）：

注意这里的 RO 是前面的消息相对最后一条消息的 IO 而言的，所以其值小于等于0，0表示最后一条消息自身。

```shell
RO = IO_of_a_message - IO_of_the_last_message 
AO = AO_Of_Last_Inner_Message + RO
```

> 注意：压缩消息（compress message），Kafka 中还有一个 compact message，常常被人们直译成压缩消息，需要注意两者的区别。compact message 是针对日志清理策略而言的（cleanup.policy = compact），是指日志压缩（Log Compaction）后的消息，这个在后面的章节中会有相关介绍。本节中的压缩消息单指 compress message，即采用 GZIP、LZ4 等压缩工具压缩的消息。

V1 比 V0 的消息多了一个 timestamp 字段。对于压缩的情形，外层消息的 timestamp 设置为：

- 如果 timestamp 类型是 CreateTime，那么设置的是内层消息中最大的时间戳。
- 如果 timestamp 类型是 LogAppendTime，那么设置的是 Kafka 服务器当前的时间戳。 内层消息的 timestamp 设置为：
- 如果外层消息的 timestamp 类型是 CreateTime，那么设置的是生产者创建消息时的时间戳。
- 如果外层消息的 timestamp 类型是 LogAppendTime，那么所有内层消息的时间戳都会被忽略。

对 attributes 字段而言，它的 timestamp 位只在外层消息中设置，内层消息中的 timestamp 类型一直都是 CreateTime。

### 变长字段
v2 版本的消息参考了 Protocol Buffer 而引入了变长整型（Varints）和 ZigZag 编码。

Varints 是使用一个或多个字节来序列化整数的一种方法。数值越小，其占用的字节数就越少。Varints 中的每个字节都有一个位于最高位的 msb 位（most significant bit），除最后一个字节外，其余 msb 位都设置为 1，最后一个字节的 msb 位为0。这个 msb 位表示其后的字节是否和当前字节一起来表示同一个整数。除 msb 位外，剩余的 7 位用于存储数据本身，这种表示类型又称为 Base 128。

通常而言，一个字节 8 位可以表示 256 个值，所以称为 Base 256，而这里只能用7位表示，2的7次方即128。Varints 中采用的是小端字节序，即最小的字节放在最前面。示例如下：

```shell
## 数字 1，只占一个字节，所以 msb 位为 0
0000 0001
## 数字 300，原本二进制：0000 0001 0010 1100，首先去掉每个字节的 msb 位：010 1100 000 0010，因为 varints 使用小端字节序的布局方式，所以两个字节位置翻转为：000 0010 010 1100 然后前后合并为：00000100101100 然后放弃前面 0 得到从最后开始 4 位一组，最前面不够的补 0 得到：0000 0001 0010 1100 即 300
1010 1100 0000 0010
```

Varints 可以用来表示 int32、int64、uint32、uint64、sint32、sint64、bool、enum 等类型。在实际使用过程中，如果当前字段可以表示为负数，那么对 int32/int64 和 sint32/sint64 而言，它们在进行编码时存在较大的区别。比如使用 int64 表示一个负数，那么哪怕是-1，其编码后的长度始终为10个字节（可以通过下面的代码来测试长度），就如同对待一个很大的无符号长整型数一样。**为了使编码更加高效，Varints 使用了 ZigZag 的编码方式。**

```java
public int sizeOfLong(int v) {
    int bytes = 1;
    while ((v & 0xffffffffffffff80L) != 0L) {
        bytes += 1;
        v >>>= 7;
    }
    return bytes;
}
```

**ZigZag 编码以一种锯齿形（zig-zags）的方式来回穿梭正负整数，将带符号整数映射为无符号整数**，这样可以使绝对值较小的负数仍然享有较小的 Varints 编码值，比如 -1 编码为 1，1 编码为 2，-2 编码为 3。

其编码的转换公式为：

```math
## 针对 sint32 公式为：
## 示例：-1 的二进制表现形式为补码：1111 1111 1111 1111 1111 1111 1111 1111，
## (n << 1)	= 1111 1111 1111 1111 1111 1111 1111 1110
## (n >> 31) 	= 1111 1111 1111 1111 1111 1111 1111 1111
## (n << 1) ^ (n >> 31) = 1，因此，-1 对应的 Varints 编码为 0000 0001，即 -1 原要 4 个字节表示，现在只要1个
(n << 1) ^ (n >> 31)

## 针对 sint64 公式为：
(n << 1) ^ (n >> 63)
```

因为 Varints 中的一个字节中只有 7 位是有效数值位，即只能表示 128 个数值，转变成绝对值之后其实质上只能表示 64 个数值。比如对消息体长度而言，其值肯定是大于等于 0 的正整数，那么一个字节长度的 Varints 最大只能表示63。64 的二进制数表示为：0100 0000 经过 ZigZag 处理后为：1000 0000 ^ 0000 0000 = 1000 0000
每个字节的低7位是有效数值位，所以1000 0000进一步转变为：000 0001 000 0000 而 Varints 使用小端字节序，所以需要翻转一下位置：000 0000 000 0001 设置非最后一个字节的 msb 位为1，最后一个字节的 msb 位为0，最终有：1000 0000 0000 0001
所以最终64表示为1000 0000 0000 0001，而63却表示为0111 1110。

回顾 Kafka v0 和 v1 版本的消息格式，如果消息本身没有 key，那么 key length 字段为-1，int 类型的需要4个字节来保存，而如果采用 Varints 来编码则只需要1个字节。根据 Varints 的规则可以推导出0～63之间的数字占1个字节，64～8191之间的数字占2个字节，8192～1048575之间的数字占3个字节。而 Kafka broker 端配置 message.max.bytes 的默认大小为1000012（Varints 编码占3个字节），如果消息格式中与长度有关的字段采用 Varints 的编码，那么绝大多数情况下都会节省空间，而 v2 版本的消息格式也正是这样做的。

注意：Varints 并非一直会节省空间，一个 int32 最长会占用 5 个字节（大于默认的4个字节），一个 int64 最长会占用 10 个字节（大于默认的8个字节）。有关 int32/int64 的更多实现细节可以参考 org.apache.kafka.common.utils.ByteUtils。

#### V0、V1 版本消息格式的缺陷

- 空间使用率低：无论 key 或 value 是否有记录，都需要一个固定大小 4 字节去保存它们的长度信息，当消息足够多时，会浪费非常多的存储空间；
- 消息长度没有保存：每次计算单条消息的总字节书都需要通过实时计算得出，效率低下；
- 只保存最新消息位移：上面内容也提到过，如果消息使用压缩，那么消息集合中的 offset 字段只会保存最后一条消息在分区日志中的 offset；
- 冗余的消息级 CRC 校验：即使是批次发送消息，每条消息也需要单独保存 CRC，在 V2 版本中已经将 CRC 放到消息集合。

### v2版本

v2 版本参考了 Protocol Buffer 而引入了变长整型（Varints）和 ZigZag 编码。使用可变长度解决了空间使用率低的问题，增加了消息总长度字段，使用增量的形式保存时间戳和位移，并且把一些字段统一抽取到消息集合中。

该版本中消息集称为 Record Batch（而非之前的 Message Set），其内部也包含了一条或多条消息，消息的格式参见下图的中部和右部。在消息压缩的情形下，Record Batch Header 部分（参见下图左部，从 first offset 到 records count 字段）是不被压缩的，而被压缩的是 records 字段中的所有内容。生产者客户端中的 ProducerBatch 对应这里的 RecordBatch，而 ProducerRecord 对应这里的 Record。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230126204149120.png" alt="image-20230126204149120" style="zoom:33%;" />

**针对单个消息 Record** 

该版本内部字段大量采用了 Varints，这样 Kafka 可以根据具体的值来确定需要几个字节来保存。v2 版本的消息格式去掉了 crc 字段，另外增加了 length（消息总长度）、timestamp delta（时间戳增量）、offset delta（位移增量）和 headers 信息，并且 attributes 字段被弃用了，其它字段同之前版本。

- length：消息总长度。
- attributes：弃用，但还是在消息格式中占据1B的大小，以备未来的格式扩展。
- timestamp delta：时间戳增量。通常一个 timestamp 需要占用8个字节，如果像这里一样保存与 RecordBatch 的起始时间戳的差值，则可以进一步节省占用的字节数。
- offset delta：位移增量。保存与 RecordBatch 起始位移的差值，可以节省占用的字节数。
- headers：这个字段用来支持应用级别的扩展，而不需要像 v0 和 v1 版本一样不得不将一些应用级别的属性值嵌入消息体。Header 的格式如图中最右部分所示，包含 key 和 value，一个 Record 里面可以包含0至多个 Header。

对于 v1 版本的消息，如果用户指定的 timestamp 类型是 LogAppendTime 而不是 CreateTime，那么消息从生产者进入 broker 后，timestamp 字段会被更新，此时消息的 crc 值将被重新计算，而此值在生产者中已经被计算过一次。再者，broker 端在进行消息格式转换时（比如 v1 版转成 v0 版的消息格式）也会重新计算 crc 的值。在这些类似的情况下，消息从生产者到消费者之间流动时，crc 的值是变动的，需要计算两次 crc 的值，所以这个字段的设计在 v0 和 v1 版本中显得比较“鸡肋”。在 v2 版本中将 crc 的字段从 Record 中转移到了 RecordBatch 中。

**针对消息集**：

v2 版本对消息集（RecordBatch）做了彻底的修改，参考上图最左部分，除了刚刚提及的 crc 字段，还多了如下字段。

- first offset：表示当前 RecordBatch 的起始位移。
- length：计算从 partition leader epoch 字段开始到末尾的长度。
- partition leader epoch：分区 leader 纪元，可以看作分区 leader 的版本号或更新次数，详细内容请参考16节。
- magic：消息格式的版本号，对 v2 版本而言，magic 等于2。
- attributes：消息属性，注意这里占用了两个字节。低3位表示压缩格式，可以参考 v0 和 v1；第4位表示时间戳类型；第5位表示此 
- RecordBatch 是否处于事务中，0表示非事务，1表示事务。第6位表示是否是控制消息（ControlBatch），0表示非控制消息，而1表示是控制消息，控制消息用来支持事务功能，详细内容请参考14节。
- last offset delta：RecordBatch 中最后一个 Record 的 offset 与 first offset 的差值。主要被 broker 用来确保 RecordBatch 中 Record 组装的正确性。
- first timestamp：RecordBatch 中第一条 Record 的时间戳。
- max timestamp：RecordBatch 中最大的时间戳，一般情况下是指最后一个 Record 的时间戳，和 last offset delta 的作用一样，用来确保消息组装的正确性。
- producer id：PID，用来支持幂等和事务，producer epoch、first sequence 和 producer id 一样，用来支持幂等和事务;

- records count：RecordBatch 中 Record 的个数。

该版本消息对应的日志信息使用 16 进制数保存，可以参考 P177 解析获取对应字段含义；

同时该版本中使用上述版本的测试方式，可以验证各个字段的长度和图中一致，同时如果发送同样的消息，V2 版本单条消息比之前版本占用空间大，但是如果多发送几条消息会发现空间比其他版本低，因为 V2 版本将多个消息（Record）打包存放到单个 RecordBatch 中，又通过 Varints 编码极大地节省了空间。

v2 版本的消息不仅提供了更多的功能，比如事务、幂等性等，某些情况下还减少了消息的空间占用，总体性能提升很大。

## 三、日志索引

每个日志分段文件对应了两个索引文件用于提高查找消息的效率：

- 偏移量索引文件：用来建立消息偏移量（offset）到物理地址之间的映射关系，方便快速定位消息所在的物理文件位置；
- 时间戳索引文件：根据指定的时间戳（timestamp）来查找对应的偏移量信息。

**Kafka 中的索引文件以稀疏索引（sparse index）的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引项**。每当写入一定量（由 broker 端参数 `log.index.interval.bytes` 指定，默认值为 4096，即 4KB）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小 `log.index.interval.bytes` 的值，对应地可以增加或缩小索引项的密度。稀疏索引通过 MappedByteBuffer 将索引文件映射到内存中，以加快索引的查询速度。

- 偏移量索引文件中的偏移量是单调递增的，查询指定偏移量时，使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量的最大偏移量。

- 时间戳索引文件中的时间戳也保持严格的单调递增，查询指定时间戳时，也根据二分查找法来查找不大于该时间戳的最大偏移量，至于要找到对应的物理文件位置还需要根据偏移量索引文件来进行再次定位。

稀疏索引的方式是在磁盘空间、内存空间、查找时间等多方面之间的一个折中。

日志分段文件达到一定的条件时需要进行切分，那么其对应的索引文件也需要进行切分。日志分段文件切分包含以下几个条件，满足其一即可：

- 当前日志分段文件的大小超过了 broker 端参数 `log.segment.bytes` 配置的值（默认值为 1073741824，即1GB）。
- 当前日志分段中消息的最大时间戳与当前系统的时间戳的差值大于 `log.roll.ms` 或 `log.roll.hours` 参数值。如果两个参数同时配置，前者优先级高。默认情况下，只配置 `log.roll.hours` 参数，其值为 168，即 7 天。
- 偏移量索引文件或时间戳索引文件的大小达到 broker 端参数 `log.index.size.max.bytes` 值（默认值 10485760，即10MB）。
- 追加的消息的偏移量与当前日志分段的偏移量之间的差值大于 `Integer.MAX_VALUE`，即要追加的消息的偏移量不能转变为相对偏移量（offset - baseOffset > Integer.MAX_VALUE）。

对非当前活跃的日志分段而言，其对应的索引文件内容已经固定而不需要再写入索引项，所以会被设定为只读。而对当前活跃的日志分段（activeSegment）而言，索引文件还会追加更多的索引项，所以被设定为可读写。

在索引文件切分的时候，Kafka 会关闭当前正在写入的索引文件并置为只读模式，同时以可读写的模式创建新的索引文件，索引文件的大小由 broker 端参数 `log.index.size.max.bytes` 配置。Kafka 在创建索引文件的时候会为其预分配该参数配置的大小的空间，注意这一点与日志分段文件不同，只有当索引文件进行切分的时候，Kafka 才会把该索引文件裁剪到实际的数据大小。也就是说，与当前活跃的日志分段对应的索引文件的大小固定为 log.index.size.max.bytes，而其余日志分段对应的索引文件的大小为实际的占用空间。

### （一）偏移量索引

偏移量索引项的格式如下图所示。每个索引项占用 8 个字节，分为两个部分。

- relativeOffset：相对偏移量，表示消息相对于 baseOffset 的偏移量，占 4 字节，**当前索引文件的文件名即为 baseOffset 的值。**两者关系：relativeOffset = offset - baseOffset

    消息的偏移量（offset，也称为绝对偏移量）占用 8 字节。索引项中使用相对偏移量而非绝对偏移量可以减小索引文件占用的空间。举个例子，一个日志分段的 baseOffset 为 32，那么其文件名就是 00000000000000000032.log，offset 为 35 的消息在索引文件中的 relativeOffset 的值为 35-32=3。

- position：物理地址，即消息在日志分段文件中对应的物理位置，占 4 字节。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127180208520.png" alt="image-20230127180208520" style="zoom:33%;" />

回顾：日志分段文件切分的第4个条件：追加的消息的偏移量与当前日志分段的偏移量之间的差值大于 Integer.MAX_VALUE。因为如果彼此的差值超过了这个最大值，那么 relativeOffset 就不能用 4 个字节表示了，进而不能享受这个索引项的设计所带来的便利了。

`.index` 文件内容为 16 进制，每行就是一个索引项，例如：

```shell
0000 0006 0000 009c
0000 000e 0000 01cb
## 可以解析如下：
relativeOffset=6, position=156
relativeOffset=14, position=459
```

通常使用 kafka-dump-log.sh 脚本来解析 .index 文件（还包括 .timeindex、 .snapshot、.txnindex 等文件），示例如下：

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-dump-log.sh --files /tmp/kafka-logs/ topic-log-0/00000000000000000000.index
Dumping /tmp/kafka-logs/topic-log-0/00000000000000000000.index
offset: 6 position: 156
offset: 14 position: 459
```


示例：00000000000000000000.index 和 00000000000000000000.log 的对照图来做进一步的陈述，如下图所示。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127181847615.png" alt="image-20230127181847615" style="zoom:50%;" />

示例：如果要查找偏移量为 23 的消息，首先通过二分法在偏移量索引文件中找到不大于 23 的最大索引项，即[22, 656]，然后从日志分段文件中的物理位置 656 开始顺序查找偏移量为 23 的消息。

示例二：如果要查找偏移量为 268 的消息，首先肯定是定位到 baseOffset 为251的日志分段，然后计算相对偏移量 relativeOffset = 268 - 251 = 17，之后再在对应的索引文件中找到不大于17的索引项，最后根据索引项中的 position 定位到具体的日志分段文件位置开始查找目标消息。那么又是如何查找 baseOffset 为251的日志分段的呢？这里并不是顺序查找，而是用了跳跃表的结构。Kafka 的每个日志对象中使用了 ConcurrentSkipListMap 来保存各个日志分段，每个日志分段的 baseOffset 作为 key，这样可以根据指定偏移量来快速定位到消息所在的日志分段。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127182322191.png" alt="image-20230127182322191" style="zoom:33%;" />

注意：Kafka 强制要求索引文件大小必须是索引项大小的整数倍，对偏移量索引文件而言，必须为 8 的整数倍。如果 broker 端参数 `log.index.size.max.bytes` 配置为 67，那么 Kafka 在内部会将其转换为 64，即不大于 67，并且满足为 8 的整数倍的条件。

### （二）时间戳索引

时间戳索引项的格式如下图所示，每个索引项占用12个字节，分为两个部分。

- timestamp：当前日志分段最大的时间戳。
- relativeOffset：时间戳所对应的消息的相对偏移量。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127182558304.png" alt="image-20230127182558304" style="zoom:33%;" />


时间戳索引文件中包含若干时间戳索引项，每个追加的时间戳索引项中的 timestamp 必须大于之前追加的索引项的 timestamp，否则不予追加。如果 broker 端参数 `log.message.timestamp.type` 设置为 LogAppendTime，那么消息的时间戳必定能够保持单调递增；相反，如果是 CreateTime 类型则无法保证。生产者可以使用类似 ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value) 的方法来指定时间戳的值。即使生产者客户端采用自动插入的时间戳也无法保证时间戳能够单调递增，如果两个不同时钟的生产者同时往一个分区中插入消息，那么也会造成当前分区的时间戳乱序。

**时间戳索引文件大小必须是索引项大小（12B）的整数倍**，如果不满足条件也会进行裁剪。同样假设 broker 端参数 `log.index.size.max.bytes` 配置为 67，那么对应于时间戳索引文件，Kafka 在内部会将其转换为 60。

每当写入一定量的消息时，就会在偏移量索引文件和时间戳索引文件中分别增加一个偏移量索引项和时间戳索引项。**两个文件增加索引项的操作是同时进行的，但并不意味着偏移量索引中的 relativeOffset 和时间戳索引项中的 relativeOffset 是同一个值**。与上面偏移量索引一节示例中所对应的时间戳索引文件 00000000000000000000.timeindex 的部分内容如下（也是 16 进制）：

0000 0163 639e 5a35 0000 0006 
0000 0163 639e 65fa 0000 000f

该时间戳索引文件结构如下图左上部分：

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127211732959.png" alt="image-20230127211732959" style="zoom:33%;" />

如果要查找指定时间戳 targetTimeStamp = 1526384718288 开始的消息，首先是找到不小于指定时间戳的日志分段。这里就无法使用跳跃表来快速定位到相应的日志分段了，需要分以下几个步骤来完成：

- 步骤1：将 targetTimeStamp 和每个日志分段中的最大时间戳 largestTimeStamp 逐一对比，直到找到不小于 targetTimeStamp 的 largestTimeStamp 所对应的日志分段。日志分段中的 largestTimeStamp 的计算是先查询该日志分段所对应的时间戳索引文件，找到最后一条索引项，若最后一条索引项的时间戳字段值大于0，则取其值，否则取该日志分段的最近修改时间。

- 步骤2：找到相应的日志分段之后，在时间戳索引文件中使用二分查找算法查找到不大于 targetTimeStamp 的最大索引项，即[1526384718283, 28]，如此便找到了一个相对偏移量28。

- 步骤3：在偏移量索引文件中使用二分算法查找到不大于28的最大索引项，即[26, 838]。

- 步骤4：从步骤1中找到日志分段文件中的838的物理位置开始查找不小于 targetTimeStamp 的消息。

## 四、日志清理

Kafka 中每一个分区副本都对应一个 Log，而 Log 又可以分为多个日志分段。Kafka 提供了两种日志清理策略：

- 日志删除（Log Retention）：按照一定的保留策略直接删除不符合条件的**日志分段**。

    通过将 broker 端参数 `log.cleanup.policy` 配置为 `delete` （默认值）来实现；

- 日志压缩（Log Compaction）：针对每个消息的 key 进行整合，对于有相同 key 的不同 value 值，只保留最后一个版本。

    通过将 broker 端参数 `log.cleanup.policy` 配置为 `compact`，同时将 `log.cleaner.enable` 配置为 `true`（默认值）

    ==问题：如果 key 为 null 怎么办？==

注：如果将 log.cleanup.policy 参数设置为 `delete,compact`，则同时支持日志删除和日志压缩两种策略。

**日志清理的粒度可以控制到主题级别，比如与 `log.cleanup.policy` 对应的主题级别的参数为 `cleanup.policy`**，为了简化说明，本节只采用 broker 端参数做陈述，topic 级别的参数可以查看《图解Kafka之实战指南》的相关章节。

### （一）日志删除【后面都没看】

Kafka 的日志管理器中会有一个专门的日志删除任务来周期性地检测和删除不符合保留条件的日志分段文件，这个周期可以通过 broker 端参数 `log.retention.check.interval.ms` 来配置，默认值为 300000，即 5 分钟。**当前日志分段的保留策略有 3 种：基于时间的保留策略、基于日志大小的保留策略和基于日志起始偏移量的保留策略。**

#### 1.基于时间

日志删除任务会检查当前日志文件中是否有保留时间超过设定的阈值（retentionMs）来寻找可删除的日志分段文件集合（deletableSegments），如下图所示。retentionMs 可以通过 broker 端参数 log.retention.hours、log.retention.minutes 和 log.retention.ms 来配置，其中 log.retention.ms 的优先级最高，log.retention.minutes 次之，log.retention.hours 最低。默认情况下只配置了 log.retention.hours 参数，其值为168，故默认情况下日志分段文件的保留时间为7天。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127213738512.png" alt="image-20230127213738512" style="zoom:33%;" />

查找过期的日志分段文件，并不是简单地根据日志分段的最近修改时间 lastModifiedTime 来计算的，而是根据日志分段中最大的时间戳 largestTimeStamp 来计算的。因为日志分段的 lastModifiedTime 可以被有意或无意地修改，比如执行了 touch 操作，或者分区副本进行了重新分配，lastModifiedTime 并不能真实地反映出日志分段在磁盘的保留时间。要获取日志分段中的最大时间戳 largestTimeStamp 的值，首先要查询该日志分段所对应的时间戳索引文件，查找时间戳索引文件中最后一条索引项，若最后一条索引项的时间戳字段值大于0，则取其值，否则才设置为最近修改时间 lastModifiedTime。

若待删除的日志分段的总数等于该日志文件中所有的日志分段的数量，那么说明所有的日志分段都已过期，但该日志文件中还要有一个日志分段用于接收消息的写入，即必须要保证有一个活跃的日志分段 activeSegment，在此种情况下，会先切分出一个新的日志分段作为 activeSegment，然后执行删除操作。

删除日志分段时，首先会从 Log 对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作。然后将日志分段所对应的所有文件添加上“.deleted”的后缀（当然也包括对应的索引文件）。最后交由一个以“delete-file”命名的延迟任务来删除这些以“.deleted”为后缀的文件，这个任务的延迟执行时间可以通过 file.delete.delay.ms 参数来调配，此参数的默认值为60000，即1分钟。

#### 2.基于日志大小

日志删除任务会检查当前日志的大小是否超过设定的阈值（retentionSize）来寻找可删除的日志分段的文件集合（deletableSegments），如下图所示。retentionSize 可以通过 broker 端参数 log.retention.bytes 来配置，默认值为-1，表示无穷大。注意 log.retention.bytes 配置的是 Log 中所有日志文件的总大小，而不是单个日志分段（确切地说应该为 .log 日志文件）的大小。单个日志分段的大小由 broker 端参数 log.segment.bytes 来限制，默认值为1073741824，即 1GB。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127213807956.png" alt="image-20230127213807956" style="zoom:33%;" />

基于日志大小的保留策略与基于时间的保留策略类似，首先计算日志文件的总大小 size 和 retentionSize 的差值 diff，即计算需要删除的日志总大小，然后从日志文件中的第一个日志分段开始进行查找可删除的日志分段的文件集合 deletableSegments。查找出 deletableSegments 之后就执行删除操作，这个删除操作和基于时间的保留策略的删除操作相同，这里不再赘述。

#### 3.基于日志起始偏移量

一般情况下，日志文件的起始偏移量 logStartOffset 等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset 的值可以通过 DeleteRecordsRequest 请求（比如使用 KafkaAdminClient 的 deleteRecords() 方法、使用 kafka-delete-records.sh 脚本）、日志的清理和截断等操作进行修改。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127213835689.png" alt="image-20230127213835689" style="zoom:33%;" />

基于日志起始偏移量的保留策略的判断依据是某日志分段的下一个日志分段的起始偏移量 baseOffset 是否小于等于 logStartOffset，若是，则可以删除此日志分段。如上图所示，假设 logStartOffset 等于25，日志分段1的起始偏移量为0，日志分段2的起始偏移量为11，日志分段3的起始偏移量为23，通过如下动作收集可删除的日志分段的文件集合 deletableSegments：

从头开始遍历每个日志分段，日志分段1的下一个日志分段的起始偏移量为11，小于 logStartOffset 的大小，将日志分段1加入 deletableSegments。
日志分段2的下一个日志偏移量的起始偏移量为23，也小于 logStartOffset 的大小，将日志分段2加入 deletableSegments。
日志分段3的下一个日志偏移量在 logStartOffset 的右侧，故从日志分段3开始的所有日志分段都不会加入 deletableSegments。
收集完可删除的日志分段的文件集合之后的删除操作同基于日志大小的保留策略和基于时间的保留策略相同，这里不再赘述。

## （二）日志压缩
Kafka 中的 Log Compaction 是指在默认的日志删除（Log Retention）规则之外提供的一种清理过时数据的方式。如下图所示，Log Compaction 对于有相同 key 的不同 value 值，只保留最后一个版本。如果应用只关心 key 对应的最新 value 值，则可以开启 Kafka 的日志清理功能，Kafka 会定期将相同 key 的消息进行合并，只保留最新的 value 值。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127214006442.png" alt="image-20230127214006442" style="zoom:33%;" />

有很多中文资料会把 Log Compaction 翻译为“日志压缩”，笔者认为不够妥当，压缩应该是指 Compression，在 Kafka 中消息可以采用 gzip、Snappy、LZ4 等压缩方式进行压缩，如果把 Log Compaction 翻译为日志压缩，容易让人和消息压缩（Message Compression）产生关联，其实是两个不同的概念。英文“Compaction”可以直译为“压紧、压实”，如果这里将 Log Compaction 直译为“日志压紧”或“日志压实”又未免太过生硬。考虑到“日志压缩”的说法已经广为用户接受，笔者这里勉强接受此种说法，不过文中尽量直接使用英文 Log Compaction 来表示日志压缩。读者在遇到类似“压缩”的字眼之时需格外注意这个压缩具体是指日志压缩（Log Compaction）还是指消息压缩（Message Compression）。

Log Compaction 执行前后，日志分段中的每条消息的偏移量和写入时的偏移量保持一致。Log Compaction 会生成新的日志分段文件，日志分段中每条消息的物理位置会重新按照新文件来组织。Log Compaction 执行过后的偏移量不再是连续的，不过这并不影响日志的查询。

Kafka 中的 Log Compaction 可以类比于 Redis 中的 RDB 的持久化模式。试想一下，如果一个系统使用 Kafka 来保存状态，那么每次有状态变更都会将其写入 Kafka。在某一时刻此系统异常崩溃，进而在恢复时通过读取 Kafka 中的消息来恢复其应有的状态，那么此系统关心的是它原本的最新状态而不是历史时刻中的每一个状态。如果 Kafka 的日志保存策略是日志删除（Log Deletion），那么系统势必要一股脑地读取 Kafka 中的所有数据来进行恢复，如果日志保存策略是 Log Compaction，那么可以减少数据的加载量进而加快系统的恢复速度。Log Compaction 在某些应用场景下可以简化技术栈，提高系统整体的质量。

我们知道可以通过配置 log.dir 或 log.dirs 参数来设置 Kafka 日志的存放目录，而每一个日志目录下都有一个名为“cleaner-offset-checkpoint”的文件，这个文件就是清理检查点文件，用来记录每个主题的每个分区中已清理的偏移量。

通过清理检查点文件可以将 Log 分成两个部分，如下图所示。通过检查点 cleaner checkpoint 来划分出一个已经清理过的 clean 部分和一个还未清理过的 dirty 部分。在日志清理的同时，客户端也可以读取日志中的消息。dirty 部分的消息偏移量是逐一递增的，而 clean 部分的消息偏移量是断续的，如果客户端总能赶上 dirty 部分，那么它就能读取日志的所有消息，反之就不可能读到全部的消息。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127214035939.png" alt="image-20230127214035939" style="zoom:33%;" /> 

上图中的 firstDirtyOffset（与 cleaner checkpoint 相等）表示 dirty 部分的起始偏移量，而 firstUncleanableOffset 为 dirty 部分的截止偏移量，整个 dirty 部分的偏移量范围为[firstDirtyOffset, firstUncleanableOffset），注意这里是左闭右开区间。为了避免当前活跃的日志分段 activeSegment 成为热点文件，activeSegment 不会参与 Log Compaction 的执行。同时 Kafka 支持通过参数 log.cleaner.min.compaction.lag.ms （默认值为0）来配置消息在被清理前的最小保留时间，默认情况下 firstUncleanableOffset 等于 activeSegment 的 baseOffset。

注意 Log Compaction 是针对 key 的，所以在使用时应注意每个消息的 key 值不为 null。每个 broker 会启动 log.cleaner.thread （默认值为1）个日志清理线程负责执行清理任务，这些线程会选择“污浊率”最高的日志文件进行清理。用 cleanBytes 表示 clean 部分的日志占用大小，dirtyBytes 表示 dirty 部分的日志占用大小，那么这个日志的污浊率（dirtyRatio）为：

`dirtyRatio = dirtyBytes / (cleanBytes + dirtyBytes)`
为了防止日志不必要的频繁清理操作，Kafka 还使用了参数 log.cleaner.min.cleanable.ratio （默认值为0.5）来限定可进行清理操作的最小污浊率。Kafka 中用于保存消费者消费位移的主题 __consumer_offsets 使用的就是 Log Compaction 策略。

这里我们已经知道怎样选择合适的日志文件做清理操作，然而怎么对日志文件中消息的 key 进行筛选操作呢？

Kafka 中的每个日志清理线程会使用一个名为“SkimpyOffsetMap”的对象来构建 key 与 offset 的映射关系的哈希表。日志清理需要遍历两次日志文件，第一次遍历把每个 key 的哈希值和最后出现的 offset 都保存在 SkimpyOffsetMap 中，映射模型如下图所示。第二次遍历会检查每个消息是否符合保留条件，如果符合就保留下来，否则就会被清理。假设一条消息的 offset 为 O1，这条消息的 key 在 SkimpyOffsetMap 中对应的 offset 为 O2，如果 O1 大于等于 O2 即满足保留条件。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127214114374.png" alt="image-20230127214114374" style="zoom:33%;" /> 

默认情况下，SkimpyOffsetMap 使用 MD5 来计算 key 的哈希值，占用空间大小为16B，根据这个哈希值来从 SkimpyOffsetMap 中找到对应的槽位，如果发生冲突则用线性探测法处理。为了防止哈希冲突过于频繁，也可以通过 broker 端参数 log.cleaner.io.buffer.load.factor （默认值为0.9）来调整负载因子。

偏移量占用空间大小为8B，故一个映射项占用大小为24B。每个日志清理线程的 SkimpyOffsetMap 的内存占用大小为 log.cleaner.dedupe.buffer.size / log.cleaner.thread，默认值为 = 128MB/1 = 128MB。所以默认情况下 SkimpyOffsetMap 可以保存128MB × 0.9 /24B ≈ 5033164个key的记录。假设每条消息的大小为1KB，那么这个 SkimpyOffsetMap 可以用来映射 4.8GB 的日志文件，如果有重复的 key，那么这个数值还会增大，整体上来说，SkimpyOffsetMap 极大地节省了内存空间且非常高效。

题外话：“SkimpyOffsetMap”的取名也很有意思，“Skimpy”可以直译为“不足的”，可以看出它最初的设计者也认为这种实现不够严谨。如果遇到两个不同的 key 但哈希值相同的情况，那么其中一个 key 所对应的消息就会丢失。虽然说 MD5 这类摘要算法的冲突概率非常小，但根据墨菲定律，任何一个事件，只要具有大于0的概率，就不能假设它不会发生，所以在使用 Log Compaction 策略时要注意这一点。

Log Compaction 会保留 key 相应的最新 value 值，那么当需要删除一个 key 时怎么办？Kafka 提供了一个墓碑消息（tombstone）的概念，如果一条消息的 key 不为 null，但是其 value 为 null，那么此消息就是墓碑消息。日志清理线程发现墓碑消息时会先进行常规的清理，并保留墓碑消息一段时间。

墓碑消息的保留条件是当前墓碑消息所在的日志分段的最近修改时间 lastModifiedTime 大于 deleteHorizonMs，如往上第二张图所示。这个 deleteHorizonMs 的计算方式为 clean 部分中最后一个日志分段的最近修改时间减去保留阈值 deleteRetionMs（通过 broker 端参数 log.cleaner.delete.retention.ms 配置，默认值为86400000，即24小时）的大小，即：

deleteHorizonMs = 
    clean部分中最后一个LogSegment的lastModifiedTime - deleteRetionMs
所以墓碑消息的保留条件为（可以对照往上第二张图中的 deleteRetionMs 所标记的位置去理解）：

所在LogSegment的lastModifiedTime > deleteHorizonMs 
=> 所在LogSegment的lastModifiedTime > clean部分中最后一个LogSegment的 
     lastModifiedTime - deleteRetionMs
=> 所在LogSegment的lastModifiedTime + deleteRetionMs > clean部分中最后一个
     LogSegment的lastModifiedTime

Log Compaction 执行过后的日志分段的大小会比原先的日志分段的要小，为了防止出现太多的小文件，Kafka 在实际清理过程中并不对单个的日志分段进行单独清理，而是将日志文件中 offset 从0至 firstUncleanableOffset 的所有日志分段进行分组，每个日志分段只属于一组，分组策略为：按照日志分段的顺序遍历，每组中日志分段的占用空间大小之和不超过 segmentSize（可以通过 broker 端参数 log.segment.bytes 设置，默认值为 1GB），且对应的索引文件占用大小之和不超过 maxIndexSize（可以通过 broker 端参数 log.index.interval.bytes 设置，默认值为 10MB）。同一个组的多个日志分段清理过后，只会生成一个新的日志分段。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127214156503.png" alt="image-20230127214156503" style="zoom:33%;" />

如上图所示，假设所有的参数配置都为默认值，在 Log Compaction 之前 checkpoint 的初始值为0。执行第一次 Log Compaction 之后，每个非活跃的日志分段的大小都有所缩减，checkpoint 的值也有所变化。执行第二次 Log Compaction 时会组队成[0.4GB, 0.4GB]、[0.3GB, 0.7GB]、[0.3GB]、[1GB]这4个分组，并且从第二次 Log Compaction 开始还会涉及墓碑消息的清除。同理，第三次 Log Compaction 过后的情形可参考上图的尾部。

Log Compaction 过程中会将每个日志分组中需要保留的消息复制到一个以“.clean”为后缀的临时文件中，此临时文件以当前日志分组中第一个日志分段的文件名命名，例如 00000000000000000000.log.clean。Log Compaction 过后将“.clean”的文件修改为“.swap”后缀的文件，例如：00000000000000000000. log.swap。然后删除原本的日志文件，最后才把文件的“.swap”后缀去掉。整个过程中的索引文件的变换也是如此，至此一个完整 Log Compaction 操作才算完成。

以上是整个日志压缩（Log Compaction）过程的详解，读者需要注意将日志压缩和日志删除区分开，日志删除是指清除整个日志分段，而日志压缩是针对相同 key 的消息的合并清理。

## 五、磁盘存储
Kafka 依赖于文件系统（更底层地来说就是磁盘）来存储和缓存消息。在我们的印象中，对于各个存储介质的速度认知大体同下图所示的相同，层级越高代表速度越快。很显然，磁盘处于一个比较尴尬的位置，这不禁让我们怀疑 Kafka 采用这种持久化形式能否提供有竞争力的性能。在传统的消息中间件 RabbitMQ 中，就使用内存作为默认的存储介质，而磁盘作为备选介质，以此实现高吞吐和低延迟的特性。然而，事实上磁盘可以比我们预想的要快，也可能比我们预想的要慢，这完全取决于我们如何使用它。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127214241295.png" alt="image-20230127214241295" style="zoom:33%;" />

有关测试结果表明，一个由6块 7200r/min 的 RAID-5 阵列组成的磁盘簇的线性（顺序）写入速度可以达到 600MB/s，而随机写入速度只有 100KB/s，两者性能相差6000倍。操作系统可以针对线性读写做深层次的优化，比如预读（read-ahead，提前将一个比较大的磁盘块读入内存）和后写（write-behind，将很多小的逻辑写操作合并起来组成一个大的物理写操作）技术。顺序写盘的速度不仅比随机写盘的速度快，而且也比随机写内存的速度快，如下图所示。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127214316599.png" alt="image-20230127214316599" style="zoom:33%;" />

Kafka 在设计时采用了文件追加的方式来写入消息，即只能在日志文件的尾部追加新的消息，并且也不允许修改已写入的消息，这种方式属于典型的顺序写盘的操作，所以就算Kafka使用磁盘作为存储介质，它所能承载的吞吐量也不容小觑。但这并不是让 Kafka 在性能上具备足够竞争力的唯一因素，我们不妨继续分析。

### （一）页缓存

页缓存是操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘 I/O 的操作。具体来说，就是把磁盘中的数据缓存到内存中，把对磁盘的访问变为对内存的访问。为了弥补性能上的差异，现代操作系统越来越“激进地”将内存作为磁盘缓存，甚至会非常乐意将所有可用的内存用作磁盘缓存，这样当内存回收时也几乎没有性能损失，所有对于磁盘的读写也将经由统一的缓存。

当一个进程准备读取磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页（page）是否在页缓存（pagecache）中，如果存在（命中）则直接返回数据，从而避免了对物理磁盘的 I/O 操作；如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。

同样，如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性。

Linux 操作系统中的 vm.dirty_background_ratio 参数用来指定当脏页数量达到系统内存的百分之多少之后就会触发 pdflush/flush/kdmflush 等后台回写进程的运行来处理脏页，一般设置为小于10的值即可，但不建议设置为0。与这个参数对应的还有一个 vm.dirty_ratio 参数，它用来指定当脏页数量达到系统内存的百分之多少之后就不得不开始对脏页进行处理，在此过程中，新的 I/O 请求会被阻挡直至所有脏页被冲刷到磁盘中。对脏页有兴趣的读者还可以自行查阅 vm.dirty_expire_centisecs、vm.dirty_writeback.centisecs 等参数的使用说明。

对一个进程而言，它会在进程内部缓存处理所需的数据，然而这些数据有可能还缓存在操作系统的页缓存中，因此同一份数据有可能被缓存了两次。并且，除非使用 Direct I/O 的方式，否则页缓存很难被禁止。此外，用过 Java 的人一般都知道两点事实：对象的内存开销非常大，通常会是真实数据大小的几倍甚至更多，空间使用率低下；Java 的垃圾回收会随着堆内数据的增多而变得越来越慢。基于这些因素，使用文件系统并依赖于页缓存的做法明显要优于维护一个进程内缓存或其他结构，至少我们可以省去了一份进程内部的缓存消耗，同时还可以通过结构紧凑的字节码来替代使用对象的方式以节省更多的空间。如此，我们可以在32GB的机器上使用28GB至30GB的内存而不用担心 GC 所带来的性能问题。

此外，即使 Kafka 服务重启，页缓存还是会保持有效，然而进程内的缓存却需要重建。这样也极大地简化了代码逻辑，因为维护页缓存和文件之间的一致性交由操作系统来负责，这样会比进程内维护更加安全有效。

Kafka 中大量使用了页缓存，这是 Kafka 实现高吞吐的重要因素之一。虽然消息都是先被写入页缓存，然后由操作系统负责具体的刷盘任务的，但在 Kafka 中同样提供了同步刷盘及间断性强制刷盘（fsync）的功能，这些功能可以通过 log.flush.interval.messages、log.flush.interval.ms 等参数来控制。

同步刷盘可以提高消息的可靠性，防止由于机器掉电等异常造成处于页缓存而没有及时写入磁盘的消息丢失。不过笔者并不建议这么做，刷盘任务就应交由操作系统去调配，消息的可靠性应该由多副本机制来保障，而不是由同步刷盘这种严重影响性能的行为来保障。

Linux 系统会使用磁盘的一部分作为 swap 分区，这样可以进行进程的调度：把当前非活跃的进程调入 swap 分区，以此把内存空出来让给活跃的进程。对大量使用系统页缓存的 Kafka 而言，应当尽量避免这种内存的交换，否则会对它各方面的性能产生很大的负面影响。

我们可以通过修改 vm.swappiness 参数（Linux 系统参数）来进行调节。vm.swappiness 参数的上限为100，它表示积极地使用 swap 分区，并把内存上的数据及时地搬运到 swap 分区中；vm.swappiness 参数的下限为0，表示在任何情况下都不要发生交换（vm.swappiness = 0 的含义在不同版本的 Linux 内核中不太相同，这里采用的是变更后的最新解释），这样一来，当内存耗尽时会根据一定的规则突然中止某些进程。笔者建议将这个参数的值设置为1，这样保留了 swap 的机制而又最大限度地限制了它对 Kafka 性能的影响。

### （二）磁盘I/O流程

读者可能对于前面提及的页缓存、Direct I/O、文件系统等概念的认知比较模糊，下面通过一张磁盘 I/O 的流程图来加深理解，如下图所示。

参考下图，从编程角度而言，一般磁盘 I/O 的场景有以下四种：

- 用户调用标准 C 库进行 I/O 操作，数据流为：应用程序 buffer→C 库标准 IObuffer→文件系统页缓存→通过具体文件系统到磁盘。
- 用户调用文件 I/O，数据流为：应用程序 buffer→文件系统页缓存→通过具体文件系统到磁盘。
- 用户打开文件时使用 O_DIRECT，绕过页缓存直接读写磁盘。
- 用户使用类似 dd 工具，并使用 direct 参数，绕过系统 cache与文件系统直接写磁盘。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127214802711.png" alt="image-20230127214802711" style="zoom:50%;" />


发起 I/O 请求的步骤可以表述为如下的内容（以最长链路为例）：

写操作：用户调用 fwrite 把数据写入 C 库标准 IObuffer 后就返回，即写操作通常是异步操作；数据写入 C 库标准 IObuffer 后，不会立即刷新到磁盘，会将多次小数据量相邻写操作先缓存起来合并，最终调用 write 函数一次性写入（或者将大块数据分解多次 write 调用）页缓存；数据到达页缓存后也不会立即刷新到磁盘，内核有 pdflush 线程在不停地检测脏页，判断是否要写回到磁盘，如果是则发起磁盘 I/O 请求。

读操作：用户调用 fread 到 C 库标准 IObuffer 中读取数据，如果成功则返回，否则继续；到页缓存中读取数据，如果成功则返回，否则继续；发起 I/O 请求，读取数据后缓存 buffer 和 C 库标准 IObuffer 并返回。可以看出，读操作是同步请求。

I/O请求处理：通用块层根据 I/O 请求构造一个或多个 bio 结构并提交给调度层；调度器将 bio 结构进行排序和合并组织成队列且确保读写操作尽可能理想：将一个或多个进程的读操作合并到一起读，将一个或多个进程的写操作合并到一起写，尽可能变随机为顺序（因为随机读写比顺序读写要慢），读必须优先满足，而写也不能等太久。

针对不同的应用场景，I/O 调度策略也会影响 I/O 的读写性能，目前 Linux 系统中的 I/O 调度策略有4种，分别为 NOOP、CFQ、DEADLINE 和 ANTICIPATORY，默认为 CFQ。

1. NOOP

    NOOP 算法的全写为 No Operation。该算法实现了最简单的 FIFO队列，所有 I/O 请求大致按照先来后到的顺序进行操作。之所以说“大致”，原因是 NOOP 在 FIFO 的基础上还做了相邻 I/O 请求的合并，并不是完全按照先进先出的规则满足 I/O 请求。

假设有如下的 I/O 请求序列：

100，500，101，10，56，1000 
NOOP 将会按照如下顺序满足 I/O 请求：

100(101)，500，10，56，1000
2. CFQ

      CFQ 算法的全写为 Completely Fair Queuing。该算法的特点是按照 I/O 请求的地址进行排序，而不是按照先来后到的顺序进行响应。

假设有如下的 I/O 请求序列：

100，500，101，10，56，1000 
CFQ 将会按照如下顺序满足：

100，101，500，1000，10，56 
CFQ 是默认的磁盘调度算法，对于通用服务器来说是最好的选择。它试图均匀地分布对 I/O 带宽的访问。CFQ 为每个进程单独创建一个队列来管理该进程所产生的请求，也就是说，每个进程一个队列，各队列之间的调度使用时间片进行调度，以此来保证每个进程都能被很好地分配到 I/O 带宽。I/O 调度器每次执行一个进程的4次请求。在传统的 SAS 盘上，磁盘寻道花去了绝大多数的 I/O 响应时间。CFQ 的出发点是对 I/O 地址进行排序，以尽量少的磁盘旋转次数来满足尽可能多的 I/O 请求。在 CFQ 算法下，SAS 盘的吞吐量大大提高了。相比于 NOOP 的缺点是，先来的 I/O 请求并不一定能被满足，可能会出现“饿死”的情况。

3. DEADLINE

      DEADLINE 在 CFQ 的基础上，解决了 I/O 请求“饿死”的极端情况。除了 CFQ 本身具有的 I/O 排序队列，DEADLINE 额外分别为读 I/O 和写 I/O 提供了 FIFO 队列。读 FIFO 队列的最大等待时间为500ms，写 FIFO 队列的最大等待时间为5s。FIFO 队列内的 I/O 请求优先级要比 CFQ 队列中的高，而读 FIFO 队列的优先级又比写 FIFO 队列的优先级高。优先级可以表示如下：

FIFO(Read) > FIFO(Write) > CFQ
4. ANTICIPATORY

   CFQ 和 DEADLINE 考虑的焦点在于满足零散 I/O 请求上。对于连续的 I/O 请求，比如顺序读，并没有做优化。为了满足随机 I/O 和顺序 I/O 混合的场景，Linux 还支持 ANTICIPATORY 调度算法。ANTICIPATORY 在 DEADLINE 的基础上，为每个读 I/O 都设置了6ms的等待时间窗口。如果在6ms内 OS 收到了相邻位置的读 I/O 请求，就可以立即满足。ANTICIPATORY 算法通过增加等待时间来获得更高的性能，假设一个块设备只有一个物理查找磁头（例如一个单独的 SATA 硬盘），将多个随机的小写入流合并成一个大写入流（相当于将随机读写变顺序读写），通过这个原理来使用读取/写入的延时换取最大的读取/写入吞吐量。适用于大多数环境，特别是读取/写入较多的环境。


不同的磁盘调度算法（以及相应的 I/O 优化手段）对 Kafka 这类依赖磁盘运转的应用的影响很大，建议根据不同的业务需求来测试并选择合适的磁盘调度算法。

从文件系统层面分析，Kafka 操作的都是普通文件，并没有依赖于特定的文件系统，但是依然推荐使用 EXT4 或 XFS。尤其是对 XFS 而言，它通常有更好的性能，这种性能的提升主要影响的是 Kafka 的写入性能。

### （三）零拷贝
除了消息顺序追加、页缓存等技术，Kafka 还使用零拷贝（Zero-Copy）技术来进一步提升性能。所谓的零拷贝是指将数据直接从磁盘文件复制到网卡设备中，而不需要经由应用程序之手。零拷贝大大提高了应用程序的性能，减少了内核和用户模式之间的上下文切换。对 Linux 操作系统而言，零拷贝技术依赖于底层的 sendfile() 方法实现。对应于 Java 语言，FileChannal.transferTo() 方法的底层实现就是 sendfile() 方法。

单纯从概念上理解“零拷贝”比较抽象，这里简单地介绍一下它。考虑这样一种常用的情形：你需要将静态内容（类似图片、文件）展示给用户。这个情形就意味着需要先将静态内容从磁盘中复制出来放到一个内存 buf 中，然后将这个 buf 通过套接字（Socket）传输给用户，进而用户获得静态内容。这看起来再正常不过了，但实际上这是很低效的流程，我们把上面的这种情形抽象成下面的过程：

```java
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```


首先调用 read() 将静态内容（这里假设为文件 A ）读取到 tmp_buf，然后调用 write() 将 tmp_buf 写入 Socket，如下图所示。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127214837597.png" alt="image-20230127214837597" style="zoom:33%;" />

 

在这个过程中，文件 A 经历了4次复制的过程：

- 调用 read() 时，文件 A 中的内容被复制到了内核模式下的 Read Buffer 中。
- CPU 控制将内核模式数据复制到用户模式下。
- 调用 write() 时，将用户模式下的内容复制到内核模式下的 Socket Buffer 中。
- 将内核模式下的 Socket Buffer 的数据复制到网卡设备中传送。

从上面的过程可以看出，数据平白无故地从内核模式到用户模式“走了一圈”，浪费了2次复制过程：第一次是从内核模式复制到用户模式；第二次是从用户模式再复制回内核模式，即上面4次过程中的第2步和第3步。而且在上面的过程中，内核和用户模式的上下文的切换也是4次。

如果采用了零拷贝技术，那么应用程序可以直接请求内核把磁盘中的数据传输给 Socket，如下图所示。

<img src="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.resource/image-20230127214907936.png" alt="image-20230127214907936" style="zoom:33%;" />

零拷贝技术通过 DMA（Direct Memory Access）技术将文件内容复制到内核模式下的 Read Buffer 中。不过没有数据被复制到 Socket Buffer，相反只有包含数据的位置和长度的信息的文件描述符被加到 Socket Buffer 中。DMA 引擎直接将数据从内核模式中传递到网卡设备（协议引擎）。这里数据只经历了2次复制就从磁盘中传送出去了，并且上下文切换也变成了2次。零拷贝是针对内核模式而言的，数据在内核模式下实现了零拷贝。

从第1节至本节主要讲述的是 Kafka 中与存储相关的知识点，既包含 Kafka 自身的日志格式、日志   索引、日志清理等方面的内容，也包含底层物理存储相关的知识点。通过对这些内容的学习，相信读者对 Kafka 的一些核心机理有了比较深刻的认知。下面会讲述在存储层之上的 Kafka 的核心实现原理，这样可以让读者对 Kafka 的整理实现脉络有比较清晰的认知。