# 主题与分区

主题作为消息的归类，可以再细分为一个或多个分区，分区也可以看作对消息的二次归类。

分区的划分不仅为 Kafka 提供了可伸缩性、水平扩展的功能，还通过多副本机制来为 Kafka 提供数据冗余以提高数据可靠性。

**主题和分区都是逻辑上的概念，分区可以有一至多个副本，每个副本对应一个日志文件**，每个日志文件对应一至多个日志分段（LogSegment），每个日志分段还可以细分为索引文件、日志存储文件和快照文件等。

## 一、主题的管理

主题的管理包括创建主题、查看主题信息、修改主题和删除主题等操作。主要有三种方式可实现主题管理操作：

- 通过 Kafka 提供的 [kafka-topics.sh](https://link.zhihu.com/?target=http%3A//kafka-topics.sh) 脚本（实质是调用 kafka.admin.TopicCommand 类，[使用示例](https://stackoverflow.com/questions/54087748/kafka-create-topic-by-using-topiccommand)）；
- 通过 KafkaAdminClient 的方式；
- 通过直接操纵日志文件和 ZooKeeper 节点。

### （一）创建主题

如果 broker 端配置参数 `auto.create.topics.enable` 设置为 true（默认 true，推荐设置为 false），则当生产者向一个尚未创建的主题发送消息时或者当一个消费者开始从未知主题中读取消息时或者当任意一个客户端向未知主题发送元数据请求时，会自动创建一个分区数为 `num.partitions`（默认值为 1）、副本因子为 `default.replication.factor`（默认值为 1）的主题。

创建主题示例：

```java
// 创建一个分区数为 4、副本因子为 2 的主题，kafka 集群环境为包括三个 borker 节点
D:\Kafka\kafka_2.13-3.2.0\bin\windows\kafka-topics.bat  --bootstrap-server localhost:9092,localhost:9093 --create --topic gjxaiou --partitions 4 --replication-factor 2
```

执行完 kafka-topic.sh 脚本之后，kafka 会在 log.dir 和 log.dirs 参数配置的目录下创建相应的主题分区，因为三个节点配置了三个 log 文件夹，内容如下：

```java
// Logs9092 文件夹下包括如下文件夹：
gjxaiou-1
gjxaiou-2
gjxaiou-3
// Logs9093 文件夹下：
gjxaiou-0   
gjxaiou-1
// Logs9094 文件夹下： 
gjxaiou-0
gjxaiou-2
gjxaiou-3    
```

三个 broker 节点创建了 8 个文件夹（分区数 4 * 副本因子 2），每个副本（也称为日志，日志和副本一一对应）形成了如 `<topic>-<partition>` 形式的文件夹。

主题和分区是提供给上层用户的抽象，副本层（Log 层）是实际的物理存在，同一个分区的多个副本必须分布在不同的 broker 中，从而提供有效的冗余。

> 4 个分区，2 个副本因子，broker 为 3 则按照 2/3/3 的分区副本个数分配给各个 broker；
>
> 如果是 3 个分区、3 个副本因子，broker 为 3，则按照 3/3/3 的分区副本个数分配给各个 broker，这样每个 broker 都拥有所有分区的一个副本。

![image-20220717181828221](%E5%9B%9B%E3%80%81%E4%B8%BB%E9%A2%98%E4%B8%8E%E5%88%86%E5%8C%BA.resource/image-20220717181828221.png)

同时可以使用 `replica-assignment` 参数代替 `--partitions` 和 `--replication-factor` 实现手动指定分区副本的分配方案，格式为：

```shell
--replica-assignment <String:  broker_id_for_part1_replica1: broker_id_for_part1_replica2, broker_id_for_part2_replica1: broker_id_for_part2_replica2,...>
```

示例：

```shell
kafka-topics.bat  --bootstrap-server localhost:9092,localhost:9093 --create --topic gjxaiou-same --replica-assignment 2:0,0:1,1:2,2:1
```

创建的效果：

```shell
kafka-topics.bat  --bootstrap-server localhost:9092,localhost:9093 --describe --topic gjxaiou,gjxaiou-same
Topic: gjxaiou-same     TopicId: jTMgA8MvTAqTZzZLb_CUSw PartitionCount: 4       ReplicationFactor: 2    Configs: segment.bytes=1073741824
        Topic: gjxaiou-same     Partition: 0    Leader: 2       Replicas: 2,0   Isr: 2,0
        Topic: gjxaiou-same     Partition: 1    Leader: 0       Replicas: 0,1   Isr: 0,1
        Topic: gjxaiou-same     Partition: 2    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: gjxaiou-same     Partition: 3    Leader: 2       Replicas: 2,1   Isr: 2,1
Topic: gjxaiou  TopicId: KI-oJoNETA-Xk0LDvY1TcA PartitionCount: 4       ReplicationFactor: 2    Configs: segment.bytes=1073741824
        Topic: gjxaiou  Partition: 0    Leader: 2       Replicas: 2,1   Isr: 1,2
        Topic: gjxaiou  Partition: 1    Leader: 1       Replicas: 1,0   Isr: 1,0
        Topic: gjxaiou  Partition: 2    Leader: 0       Replicas: 0,2   Isr: 0,2
        Topic: gjxaiou  Partition: 3    Leader: 2       Replicas: 2,0   Isr: 0,2
```

注意：

- 同一个分区内的副本不能有重复，如指定 0:0,1:1，报错 AdminCommandFailedException 

- 如果分区之间指定的副本数不同，如指定 0:0,0,1:0，报错 AdminOperationException
- 跳过分区也不行，如指定 0:1,,0:1,1:0 报错 NumberFormatException

创建主题时可以通过 config 设置所创建主题的相关参数，从而覆盖原有的默认设置，使用方式如下：

`--config <String:name1=value1> --config <String:name2=value2>`

**创建主题的命名不能重复，否则报 TopicExistsException，如果创建主题命令最后加上 `--if-not-exists` 参数，则命名冲突时不做任何处理（不创建/覆盖也不报错），同时因为 kafka 内部埋点分析时会根据主题名称来命名 metrics 名称，并将 `.` 转换为 `_`，所以如果两个 topic 中只有 `.` 和 `_` 的区别，导致最后的 metrics 名称重复报错。同时不推荐 topic 名称开头为 `__`，因为双下划线开头的 topic 一般认为是 kafka 内部主题。**

**主题名称必须由大小写字母、数字、点号`.`、连接线 `-`，下画线 `_` 组成，不能只有点号或者双点，长度不能超过 249。**

通过 broker 端参数 `broker.rack` 配置机架信息（机架名称），如果指定机架信息，则在分区副本分配时会尽可能让分区副本分配到不同机器上，但是集群中所有 broker 要不全部指定，要不全部不指定，否则报错，如果想不包括，则创建 topic 时候使用参数 `--disable-rack-aware`。

### （二）分区副本的分配

**生产者的分区分配是指为每条消息指定其所要发往的分区，消费者中的分区分配是指为消费者指定其可以消费消息的分区**，上节中的分区分配是指为集群指定创建主题时的分区副本分配方案，即在哪个 broker 中创建哪些分区的副本。

使用 kafka-topics.sh 脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。

当创建一个主题时，无论通过 kafka-topics.sh 脚本，还是通过其他方式（比如 4.2 节中介绍的 KafkaAdminClient）创建主题时，实质上是在 ZooKeeper 中的 `/brokers/topics` 节点下创建与该主题对应的子节点并写入分区副本分配方案，并且在 `/config/topics/` 节点下创建与该主题对应的子节点并写入主题相关的配置信息（这个步骤可以省略不执行）。而Kafka创建主题的实质性动作是交由控制器异步去完成的。

> 详细的源码分析，详见 P107-109

### （三）查看主题

kafka-topics.sh 脚本有 5 种指令类型：create、list、describe、alter 和 delete。其中 list 和 describe 指令可以用来方便地查看主题信息。

```shell
## 查询所有主题信息
kafka-topics.bat  --bootstrap-server localhost:9092,localhost:9093 --list
## 查询单个或多个主题信息
kafka-topics.bat  --bootstrap-server localhost:9092,localhost:9093 --describe --topic gjxaiou,gjxaiou-same
```

同时 describe 指定可以指定如下参数来查询指定 Topic 信息：

- `–-topic-with-overrides`：找出所有包含覆盖配置的主题，即只列出包含了与集群不一样配置的主题的简略信息；
- `--under-replicated-partitions`：找出所有包含失效副本的分区（可能是正在同步或者同步异常），表示集群中某个 broker 已经失效或同步效率降低，如某个集群节点下线了；
- `--unavailable-partitions`：用于查看主题中没有 leader 副本的分区，这些分区已经处于离线状态，即对于生产者和消费者都是不可用的；

### （四）修改主题

当一个主题被创建之后，依然允许我们对其做一定的修改，比如修改分区个数、修改配置等，这个修改的功能就是由 kafka-topics.sh 脚本中的 alter 指令提供的。

**目前 Kafka 只支持增加分区数而不支持减少分区数**。

```shell
## 修改分区数
kafka-topics.bat  --bootstrap-server localhost:9092,localhost:9093 --alter --topic gjxaiou --partitions 3
```

注意：当主题中的消息的 key 不为 null 时，修改分区操作会影响根据 key 计算分区的行为；

如当分区数为 1 则不管消息的 key 为何值，消息都发给该分区；但是分区数增加之后，会根据消息的 key 来计算分区号，导致消息会发送到其它分区，并且会影响既定消息的顺序，所以针对基于 Key 计算分区的主题，建议不要调整分区数。

- 修改不存在的主题会报错，当同样可以通过增加 `–-if-exists` 参数来忽略异常；

- alter 指令可以配合 config 参数实现增加或者修改一些配置以实现覆盖原有配置；

  ```shell
  kafka-topics.bat  --bootstrap-server localhost:9092,localhost:9093 --alter --topic gjxaiou --config max.message.bytes=2000
  ```

- alter 指令还可以配合 `--delete-config` 加上配置名称来删除之前覆盖的配置；

### （五）配置管理

[kafka-configs.sh](https://link.zhihu.com/?target=http%3A//kafka-configs.sh) 脚本是专门用来对配置进行操作的，这里的操作是指在运行状态下修改原有的配置，达到动态变更的目的。

该脚本包含变更配置的 alter 和查看配置的 describe 两种指令类型，且该脚本支持操作主题、broker、用户和客户端配置；

该脚本中使用 `entity-type` 参数指定操作配置的类型（有四个值：topics/brokers/clients/users，使用 `entity-name` 参数指定操作配置的名称。

```shell
## 查看 topic-config 这个 topic 的信息
kafka-configs.bat --bootstrap-server localhost:9092,localhost:9093  --describe --entity-type topics --entity-name topic-config
```

其中 entity-type 和 entity-name 对应关系如下：

| entity-type 的释义                | entity-name 的释义                                           |
| --------------------------------- | ------------------------------------------------------------ |
| 主题类型的配置，取值为 topics     | 指定主题的名称                                               |
| broker 类型的配置，取值为 brokers | 指定 brokerId 值，即 broker 中 broker.id 参数配置的值        |
| 客户端类型的配置，取值为 clients  | 指定 clientId 值，即 KafkaProducer 或 KafkaConsumer 的 client.id 参数配置的值 |
| 用户类型的配置，取值为 users      | 指定用户名                                                   |

其中使用 alter 指令，需要配合 `add-config`（实现配置的增、改（即覆盖原有配置）） 或 `delete-config`（实现配置的删除，删除就是删除被覆盖的配置以恢复默认值） 参数使用。

```shell
## 为 topic-config 这个 topic 增加两个配置：cleanup.policy 和 max.message.bytes
kafka-configs.bat --bootstrap-server localhost:9092,localhost:9093 --alter --entity-type topics --entity-name topic-config --add-config cleanup.policy=compact,max.message.bytes=10000

# 删除 topic-config 这个 topic 的以下两个配置：clean.policy 和 max.message.bytes
kafka-configs.bat --bootstrap-server localhost:9092,localhost:9093 --alter --entity-type topics --entity-name topic-config --delete-config cleanup.policy,max.message.bytes
```

**内部原理**：使用脚本中的 alter 指令来变更配置时，本质会在 ZK 中创建一个命名形式为 `/config/<entity-type>/<entity-name>` 的节点，并将变更的配置写入该节点。例如上面的配置会在 `/config/topics/topic-config` 节点中包括以下数据内容

```shell
[zk]  get /config/topics/topic-config
{
	"version":1,
	"config":
		{
			"cleanup.policy":"compact",
		 	"max.message.bytes":"10000"
		}
}
## 即节点内容的数据格式如下
{"version":1,"config":{<property-name>:<property-value>}}
```

增加配置就是往节点内容中添加属性的键值对，修改配置就是修改相应属性的属性值，删除就是删除相应的属性键值对。

查看（describe）配置时，就是从 `/config/<entity-type>/<entity-name>` 节点中获取相应的数据内容。如果使用 kafka-configs.sh 脚本查看配置信息时没有指定 entity-name 参数的值，则会查看 entity-type 所对应的所有配置信息。示例如下：

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-configs.sh --zookeeper localhost:2181/ kafka --describe --entity-type topics
Configs for topic 'topic-config' are 
      cleanup.policy=compact,max.message.bytes=20000
Configs for topic 'topic-create' are 
Configs for topic '__consumer_offsets' are 
      segment.bytes=104857600,cleanup.policy=compact,compression.type=producer
```



### （六）主题端参数

与主题相关的所有配置参数在 broker 层面都有对应参数，比如主题端参数 cleanup. policy 对应 broker 层面的 log.cleanup.policy。如果没有修改过主题的任何配置参数，那么就会使用 broker 端的对应参数作为其默认值。可以在创建主题时覆盖相应参数的默认值，也可以在创建完主题之后变更相应参数的默认值。比如在创建主题的时候没有指定 cleanup.policy 参数的值，那么就使用 log.cleanup.policy 参数所配置的值作为 cleanup.policy 的值。下表列出了主题端参数与 broker 端参数的对照关系。

| 主题端参数                              | 释 义                                                        | 对应的 broker 端参数                     |
| --------------------------------------- | ------------------------------------------------------------ | ---------------------------------------- |
| cleanup.policy                          | 日志压缩策略。默认值为 delete，还可以配置为 compact          | log.cleanup.policy                       |
| compression.type                        | 消息的压缩类型。默认值为 producer，表示保留生产者中所使用的原始压缩类型。还可以配置为  uncompressed、snappy、lz4、gzip | compression.type                         |
| delete.retention.ms                     | 被标识为删除的数据能够保留多久。默认值为86400000，即1天      | log.cleaner.delete.retention.ms          |
| file.delete.delay.ms                    | 清理文件之前可以等待多长时间，默认值为60000，即1分钟         | log.segment.delete.delay.ms              |
| flush.messages                          | 需要收集多少消息才会将它们强制刷新到磁盘，默认值为 Long.MAX_VALUE，即让操作系统来决定。建议不要修改此参数的默认值 | log.flush.interval.messages              |
| flush.ms                                | 需要等待多久才会将消息强制刷新到磁盘，默认值为 Long.MAX_VALUE，即让操作系统来决定。建议不要修改此参数的默认值 | log.flush.interval.ms                    |
| follower.replication.throttled.replicas | 用来配置被限制速率的主题所对应的 follower 副本列表           | follower.replication.throttled.replicas  |
| index.interval.bytes                    | 用来控制添加索引项的频率。每超过这个参数所设置的消息字节数时就可以添加一个新的索引项，默认值为4096 | log.index.interval.bytes                 |
| leader.replication.throttled.replicas   | 用来配置被限制速率的主题所对应的 leader 副本列表             | leader.replication.throttled.replicas    |
| max.message.bytes                       | 消息的最大字节数，默认值为1000012                            | message.max.bytes                        |
| message.format.version                  | 消息格式的版本，默认值为 2.0-IV1                             | log.message.format.version               |
| message.timestamp.difference. max.ms    | 消息中自带的时间戳与 broker 收到消息时的时间戳之间最大的差值，默认值为 Long.MAX_VALUE。此参数只有在 meesage.  timestamp.type 参数设置为 CreateTime 时才有效 | log.message.timestamp. difference.max.ms |
| message.timestamp.type                  | 消息的时间戳类型。默认值为 CreateTime，还可以设置为 LogAppendTime | log.message.timestamp. type              |
| min.cleanable.dirty.ratio               | 日志清理时的最小污浊率，默认值为0.5                          | log.cleaner.min.cleanable. ratio         |
| min.compaction.lag.ms                   | 日志再被清理前的最小保留时间，默认值为0                      | log.cleaner.min.compaction. lag.ms       |
| min.insync.replicas                     | 分区ISR集合中至少要有多少个副本，默认值为1                   | min.insync.replicas                      |
| preallocate                             | 在创建日志分段的时候是否要预分配空间，默认值为 false         | log.preallocate                          |
| retention.bytes                         | 分区中所能保留的消息总量，默认值为-1，即没有限制             | log.retention.bytes                      |
| retention.ms                            | 使用 delete 的日志清理策略时消息能够保留多长时间，默认值为604800000，即7天。如果设置为-1，则表示没有限制 | log.retention.ms                         |
| segment.bytes                           | 日志分段的最大值，默认值为1073741824，即1GB                  | log.segment.bytes                        |
| segment.index.bytes                     | 日志分段索引的最大值，默认值为10485760，即10MB               | log.index.size.max.bytes                 |
| segment.jitter.ms                       | 滚动日志分段时，在 segment.ms 的基础之上增加的随机数，默认为0 | log.roll.jitter.ms                       |
| segment.ms                              | 最长多久滚动一次日志分段，默认值为604800000，即7天           | log.roll.ms                              |
| unclean.leader.election.enable          | 是否可以从非 ISR 集合中选举 leader 副本，默认值为 false，如果设置为 true，则可能造成数据丢失 | unclean.leader.election. enable          |

### （七）删除主题

删除不用的主题可以释放如磁盘、文件句柄等资源，同时删除主题是不可逆操作，删除之后与其相关的消息数据会被全部删除。

```shell
## 删除 topic-delete 这个 topic
kafka-topics.bat --bootstrap-server localhost:9092,localhost:9093 --delete --topic topic-delete
```

前提：将 broker 端配置参数 `delete.topic.enable` 置为 ture（默认值），如果设置为 false 则删除主题操作会被忽略。

- 如果删除的是 kafka 内部主题，则会直接报错。
- 如果删除不存在的主题也会报错，可以在最后加上 `if-exists` 参数来忽略异常；

kafka-topics 脚本删除主题的本质是在 ZK 中的 `/admin/delete_topics` 路径下创建一个与待删除主题同名的节点，用于标记改主题为待删除的状态，真正删除主题的动作和创建主题一样，均是由 kafka 控制器负责完成。

方式二：直接通过 ZK 客户端删除主题

```shell
## 使用 ZK 客户端 zkCli.sh 来删除主题 topic-delete
[zk]  create /admin/delete_topics/topic-delete ""
```

方式三：手动删除主题

因为主题中的元数据保存在 ZK 中的 `/brokers/topics` 和 `/config/topics` 路径下，主题中的消息数据存储在 `log.dir` 或者 `log.dirs` 配置的路径下，需要将以上所有位置内容删除。

```shell
## 删除 ZK 中的 /config/topics/topic-delete 节点
[zk] delete /config/topics/topic-delete
## 删除 ZK 中的 /brokers/topics/topic-delete 节点及其子节点
[zk] rmr /brokers/topics/topic-delete

## 集群中各个 broker 节点中执行 rm -rf /tmp/kafka-logs/topic-delete* 命令来删除与主题 topic-delete 相关文件
[kafka-node1]  rm -rf /tmp/kafka-logs/topic-delete*
[kafka-node2]  rm -rf /tmp/kafka-logs/topic-delete*
```



补充：kafka-topics.sh 脚本中的主要参数

| 参数名称                      | 释义                                                       |
| ----------------------------- | ---------------------------------------------------------- |
| alter                         | 用于修改主题                                               |
| config <键值对>               | 创建或修改主题时，用于设置主题级别的参数                   |
| create                        | 创建主题                                                   |
| delete                        | 删除主题                                                   |
| delete-config <配置名称>      | 删除主题级别被覆盖的配置                                   |
| describe                      | 查看主题的详细信息                                         |
| disable-rack-aware            | 创建主题时不考虑机架信息                                   |
| help                          | 打印帮助信息                                               |
| if-exists                     | 修改或删除主题时使用，只有当主题存在时才会执行动作         |
| if-not-exists                 | 创建主题时使用，只有主题不存在时才会执行动作               |
| list                          | 列出所有可用的主题                                         |
| partitons <分区数>            | 创建主题或增加分区时指定的分区数                           |
| replica-assignment <分配方案> | 手工指定分区副本分配方案                                   |
| replication-factor <副本数>   | 创建主题时指定副本因子                                     |
| topic <主题名称>              | 指定主题名称                                               |
| topics-with-overrides         | 使用describe查看主题信息时，只展示包含覆盖配置的主题       |
| unavailable-partitions        | 使用describe查看主题信息时，只展示包含没有leader副本的分区 |
| under-replicated-partitions   | 使用describe查看主题信息时，只展示包含失效副本的分区       |
| zookeeper                     | 指定连接的zk的地址，已经过时，可以改成 --bootstrap-server  |

## 二、初识 KafkaAdminClient

### （一）基本使用

除了脚本，可以使用 TopicCommand 来创建、修改、查看、删除主题，其与脚本使用一样，只不过命令是通过字符串拼接，然后直接调用 `kafka.admin.TopicCommand.main(String[] option)` 来执行命令，执行就是将 option 数组中所有字符串中间加空格之后拼接起来，其与程序交互性很差。

kafka 同时提供了工具类 `org.apache.kafka.clients.admin.KafkaAdminClient` 来管理 broker、配置和 ACL（Access ControlList），还可以用来管理主题。

KafkaAdminClient 继承了 `org.apache.kafka.clients.admin.AdminClient` 抽象类，并提供了多种方法，例如：

- 创建主题：CreateTopicsResult createTopics（Collection＜NewTopic＞newTopics）。
- 删除主题：DeleteTopicsResult deleteTopics（Collection＜String＞topics）。
- 列出所有可用的主题：ListTopicsResult listTopics（）。
- 查看主题的信息：DescribeTopicsResult describeTopics（Collection＜String＞topicNames）。
- 查询配置信息：DescribeConfigsResult describeConfigs（Collection＜ConfigResource＞resources）。
- 修改配置信息：AlterConfigsResult alterConfigs（Map＜ConfigResource，Config＞configs）。
- 增加分区：CreatePartitionsResult createPartitions（Map＜String，NewPartitions＞newPartitions）。

示例：创建一个分区数为 4、副本因子为 1 的主题 topic-admin

```java
String brokerList =  "localhost:9092";
String topic = "topic-admin";
 
Properties props = new Properties();	①
props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);
props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);

// 创建 KafkaAdminClient 实例，其中引用 1 位置的配置来连接 kafka 集群
AdminClient client = AdminClient.create(props);		②
 
// newTopic 用于构建创建的主题的具体信息，包括主题名称、分区数、副本因子等 
// 方式一：直接指定分区和副本因此来创建主题    
NewTopic newTopic = new NewTopic(topic, 4, (short) 1);	③
// 方式二：指定分区副本的具体分配方案来创建主题
Map<Integer, List<Integer>> replicasAssignments = new HashMap<>();
replicasAssignments.put(0, Arrays.asList(0));
replicasAssignments.put(1, Arrays.asList(0));
replicasAssignments.put(2, Arrays.asList(0));
replicasAssignments.put(3, Arrays.asList(0));
NewTopic newTopic = new NewTopic(topic, replicasAssignments);   

// 在创建主题时可以指定需要覆盖的配置
Map<String, String> configMap = new HashMap<>(4);
configMap.put("cleanup.policy", "compact");
newTopic.configs(configMap);
    
CreateTopicsResult result = client.
        createTopics(Collections.singleton(newTopic));	④
try {
    result.all().get();			⑤
} catch (InterruptedException | ExecutionException e) {
    e.printStackTrace();
}
// 使用完成之后，必须释放资源
client.close();	

```

注1：其中 NewTopic 类主要属性为：

```java
private final String name;	//主题名称
private final int numPartitions; 	//分区数
private final short replicationFactor; 	//副本因子
private final Map<Integer, List<Integer>> replicasAssignments; 	//分配方案
private Map<String, String> configs = null; 	//配置
```

其中上述步骤的第四步 createTopics() 为核心，KafkaAdminClient 内部使用 Kafka 的一套自定义二进制协议来实现诸如创建主题的管理功能。它主要的实现步骤如下：

- 客户端根据方法的调用创建相应的协议请求，比如创建主题的 createTopics 方法，其内部就是发送 CreateTopicsRequest 请求。
- 客户端将请求发送至服务端。
- 服务端处理相应的请求并返回响应，比如这个与 CreateTopicsRequest 请求对应的就是 CreateTopicsResponse。
- 客户端接收相应的响应并进行解析处理。和协议相关的请求和相应的类基本都在 org.apache.kafka.common.requests 包下，AbstractRequest 和 AbstractResponse 是这些请求和响应类的两个基本父类。

注2：创建主题的返回值 CreateTopicsResult 类主要结构如下：

```java
public class CreateTopicsResult {
    // key 表示主题名称，KafkaFuture<Void> 表示创建后的返回值类型
    // 该类方法主要都是操作该成员变量，其中 KafkaFuture 后续会替换为 JDK8 中的 CompletableFuture
    private final Map<String, KafkaFuture<Void>> futures;
 
    CreateTopicsResult(Map<String, KafkaFuture<Void>> futures) {
        this.futures = futures;
    }
 
    public Map<String, KafkaFuture<Void>> values() {
        return futures;
    }
 
    public KafkaFuture<Void> all() {
        return KafkaFuture.allOf(futures.values()
                .toArray(new KafkaFuture[0]));
    }
}
```

其他 describeConfigs() 方法使用示例如下：

该方法输出结果不会只列出被覆盖的配置信息，而是会列出主题中所有的配置信息。

```java
public static void describeTopicConfig() throws ExecutionException,
        InterruptedException {
    String brokerList =  "localhost:9092";
    String topic = "topic-admin";
 
    Properties props = new Properties();
    props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);
    props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);
    AdminClient client = AdminClient.create(props);
 
    ConfigResource resource =
            new ConfigResource(ConfigResource.Type.TOPIC, topic);①
       
    DescribeConfigsResult result =
            client.describeConfigs(Collections.singleton(resource));②
    Config config = result.all().get().get(resource);③
    System.out.println(config);④
    client.close();
}
```

alterConfigs() 方法使用如下，将 describeConfigs() 示例中的 1-4 替换为如下内容：

```java
ConfigResource resource = new ConfigResource(ConfigResource.Type.TOPIC, topic);
ConfigEntry entry = new ConfigEntry("cleanup.policy", "compact");
Config config = new Config(Collections.singleton(entry));
Map<ConfigResource, Config> configs = new HashMap<>();
configs.put(resource, config);
AlterConfigsResult result = client.alterConfigs(configs);
result.all().get();
```

createPartitions() 方法来增加一个主题的分区。下面的示例将主题 topic-admin 的分区从4增加到5，只需将上述代码中的第①至第④行替换为下面的内容即可：

```java
NewPartitions newPartitions = NewPartitions.increaseTo(5);
Map<String, NewPartitions> newPartitionsMap = new HashMap<>();
newPartitionsMap.put(topic, newPartitions);
CreatePartitionsResult result = client.createPartitions(newPartitionsMap);
result.all().get();
```

### （二）主题合法性验证

通常 Kafka 生产环境中的 `auto.create.topics.enable` 参数会被设置为 false，即自动创建主题这条路会被堵住。kafka-topics.sh 脚本创建的方式一般由运维人员操作，普通用户无权过问。那么 KafkaAdminClient 就为普通用户提供了一个“口子”，或者将其集成到公司内部的资源申请、审核系统中会更加方便。

普通用户在创建主题的时候，有可能由于误操作或其他原因而创建了不符合运维规范的主题，比如命名不规范，副本因子数太低等，这些都会影响后期的系统运维。如果创建主题的操作封装在资源申请、审核系统中，那么在前端就可以根据规则过滤不符合规范的申请操作。如果用户用 KafkaAdminClient 或类似的工具创建了一个错误的主题，我们有什么办法可以做相应的规范处理呢？

Kafka broker 端有一个这样的参数：`create.topic.policy.class.name`，默认值为 null，它提供了一个入口用来验证主题创建的合法性。使用方式很简单，只需要自定义实现 `org.apache.kafka.server.policy.CreateTopicPolicy` 接口，比如下面示例中的 PolicyDemo。然后在 broker 端的配置文件 `config/server.properties` 中配置参数 `create.topic.policy.class.name` 的值为 `org.apache.kafka.server.policy.PolicyDemo`，最后启动服务。

PolicyDemo 的代码参考代码清单20-5，主要实现接口中的 configure()、close() 及 validate() 方法，configure() 方法会在 Kafka 服务启动的时候执行，validate() 方法用来鉴定主题参数的合法性，其在创建主题时执行，close() 方法在关闭 Kafka 服务时执行。

```java
public class PolicyDemo implements CreateTopicPolicy {
    public void configure(Map<String, ?> configs) {
    }

    public void close() throws Exception {
    }

    public void validate(RequestMetadata requestMetadata)
        throws PolicyViolationException {
        if (requestMetadata.numPartitions() != null || 
            requestMetadata.replicationFactor() != null) {
            if (requestMetadata.numPartitions() < 5) {
                throw new PolicyViolationException("Topic should have at " +
                                                   "least 5 partitions, received: "+ 
                                                   requestMetadata.numPartitions());
            }
            if (requestMetadata.replicationFactor() <= 1) {
                throw new PolicyViolationException("Topic should have at " +
                                                   "least 2 replication factor, recevied: "+ 
                                                   requestMetadata.replicationFactor());
            }
        }
    }
}
```
此时如果采用代码清单20-3中的方式创建一个分区数为4、副本因子为1的主题，那么客户端就出报出如下的错误：

java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.PolicyViolationException: Topic should have at least 5 partitions, received: 4
相应的 Kafka 服务端的日志如下：

CreateTopicPolicy.RequestMetadata(topic=topic-test2, numPartitions=4, replicationFactor=1, replicasAssignments=null, configs={})
[2018-04-18 19:52:02,747] INFO [Admin Manager on Broker 0]: Error processing create topic request for topic topic-test2 with arguments (numPartitions=4, replicationFactor=1, replicasAssignments={}, configs={}) (kafka.server.AdminManager)
org.apache.kafka.common.errors.PolicyViolationException: Topic should have at least 5 partitions, receiv

## 三、分区的管理

### （一）优先副本的选举

分区使用多副本机制来提升可靠性，但只有 leader 副本对外提供读写服务，而 follower 副本只负责在内部进行消息的同步。如果一个分区的 leader 副本不可用，那么就意味着整个分区变得不可用，此时就需要 Kafka 从剩余的 follower 副本中挑选一个新的 leader 副本来继续对外提供服务。虽然不够严谨，但从某种程度上说，**broker 节点中 leader 副本个数的多少决定了这个节点负载的高低**。

在创建主题的时候，该主题的分区及副本会尽可能均匀地分布到 Kafka 集群的各个 broker 节点上，对应的 leader 副本的分配也比较均匀。比如我们使用 kafka-topics.sh 脚本创建一个分区数为3、副本因子为3的主题 topic-partitions，创建之后的分布信息如下：

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-partitions
Topic:topic-partitions	PartitionCount:3	ReplicationFactor:3	Configs:
	Topic: topic-partitions	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
	Topic: topic-partitions	Partition: 1	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1
	Topic: topic-partitions	Partition: 2	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
```


可以看到 leader 副本均匀分布在 brokerId 为0、1、2的 broker 节点之中。针对同一个分区而言，同一个 broker 节点中不可能出现它的多个副本，即 Kafka 集群的一个 broker 中最多只能有它的一个副本，我们可以将 leader 副本所在的 broker 节点叫作分区的 leader 节点，而 follower 副本所在的 broker 节点叫作分区的 follower 节点。

但是当因 Kafka 集群的 broker 节点宕机或崩溃，导致分区的 leader 节点发生故障时，其中一个 follower 节点就会成为新的 leader 节点，这样就会导致集群的负载不均衡，从而影响整体的健壮性和稳定性。**当原来的 leader 节点恢复之后重新加入集群时，它只能成为一个新的 follower 节点而不再对外提供服务**。比如我们将 brokerId 为2的节点重启，那么主题 topic-partitions 新的分布信息如下：

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-partitions
Topic:topic-partitions	PartitionCount:3	ReplicationFactor:3	Configs: 
    Topic: topic-partitions	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,0,2
    Topic: topic-partitions	Partition: 1	Leader: 0	Replicas: 2,0,1	Isr: 0,1,2
    Topic: topic-partitions	Partition: 2	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
```


可以看到原本分区1的 leader 节点为2，现在变成了0，如此一来原本均衡的负载变成了失衡：节点0的负载最高，而节点2的负载最低。

**为了有效地治理负载失衡的情况，Kafka 引入了优先副本（preferred replica）概念。优先副本是指在 AR 集合列表中的第一个副本**。比如上面主题 topic-partitions 中分区0的AR集合列表（Replicas）为[1,2,0]，那么分区0的优先副本即为1。理想情况下，优先副本就是该分区的leader 副本，所以也可以称之为 preferred leader。Kafka 要确保所有主题的优先副本在 Kafka 集群中均匀分布，这样就保证了所有分区的 leader 均衡分布。如果 leader 分布过于集中，就会造成集群负载不均衡。

**优先副本的选举是指通过一定的方式促使优先副本选举为 leader 副本，以此来促进集群的负载均衡，该行为也称为“分区平衡”。**

需要注意的是，分区平衡并不意味着 Kafka 集群的负载均衡，因为还要考虑集群中的分区分配是否均衡。更进一步，每个分区的 leader 副本的负载也是各不相同的，有些 leader 副本的负载很高，比如需要承载 TPS 为30000的负荷，而有些 leader 副本只需承载个位数的负荷。也就是说，就算集群中的分区分配均衡、leader 分配均衡，也并不能确保整个集群的负载就是均衡的，还需要其他一些硬性的指标来做进一步的衡量，这个会在后面的章节中涉及，本节只探讨优先副本的选举。

在 Kafka 中可以提供分区自动平衡的功能，与此对应的 broker 端参数是 auto.leader. rebalance.enable，此参数的默认值为 true，即默认情况下此功能是开启的。如果开启分区自动平衡的功能，则 Kafka 的控制器会启动一个定时任务，这个定时任务会轮询所有的 broker 节点，计算每个 broker 节点的分区不平衡率（broker 中的不平衡率=非优先副本的 leader 个数/分区总数）是否超过 leader.imbalance.per.broker.percentage 参数配置的比值，默认值为10%，如果超过设定的比值则会自动执行优先副本的选举动作以求分区平衡。执行周期由参数 leader.imbalance.check.interval.seconds 控制，默认值为300秒，即5分钟。

不过在生产环境中不建议将 auto.leader.rebalance.enable 设置为默认的 true，因为这可能引起负面的性能问题，也有可能引起客户端一定时间的阻塞。因为执行的时间无法自主掌控，如果在关键时期（比如电商大促波峰期）执行关键任务的关卡上执行优先副本的自动选举操作，势必会有业务阻塞、频繁超时之类的风险。前面也分析过，分区及副本的均衡也不能完全确保集群整体的均衡，并且集群中一定程度上的不均衡也是可以忍受的，为防止出现关键时期“掉链子”的行为，笔者建议还是将掌控权把控在自己的手中，可以针对此类相关的埋点指标设置相应的告警，在合适的时机执行合适的操作，而这个“合适的操作”就是指手动执行分区平衡。

Kafka 中 kafka-perferred-replica-election.sh 脚本提供了对分区 leader 副本进行重新平衡的功能。优先副本的选举过程是一个安全的过程，Kafka 客户端可以自动感知分区 leader 副本的变更。下面的示例演示了 kafka-perferred-replica-election.sh 脚本的具体用法：

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181/kafka

Created preferred replica election path with topic-demo-3,__consumer_offsets-22, topic-config-1,__consumer_offsets-30,__bigdata_monitor-12,__consumer_offsets-8,__consumer_offsets-21,topic-create-0,__consumer_offsets-4,topic-demo-1,topic-partitions-1,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,(…省略若干)

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-partitions
Topic:topic-partitions	PartitionCount:3	ReplicationFactor:3	Configs: 
    Topic: topic-partitions	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,0,2
    Topic: topic-partitions	Partition: 1	Leader: 2	Replicas: 2,0,1	Isr: 0,1,2
    Topic: topic-partitions	Partition: 2	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
```


可以看到在脚本执行之后，主题 topic-partitions 中的所有 leader 副本的分布已经和刚创建时的一样了，所有的优先副本都成为 leader 副本。

上面示例中的这种使用方式会将集群上所有的分区都执行一遍优先副本的选举操作，分区数越多打印出来的信息也就越多。leader 副本的转移也是一项高成本的工作，如果要执行的分区数很多，那么必然会对客户端造成一定的影响。如果集群中包含大量的分区，那么上面的这种使用方式有可能会失效。在优先副本的选举过程中，具体的元数据信息会被存入 ZooKeeper 的/admin/preferred_replica_election 节点，如果这些数据超过了 ZooKeeper 节点所允许的大小，那么选举就会失败。默认情况下 ZooKeeper 所允许的节点数据大小为1MB。

kafka-perferred-replica-election.sh 脚本中还提供了 path-to-json-file 参数来小批量地对部分分区执行优先副本的选举操作。通过 path-to-json-file 参数来指定一个 JSON 文件，这个 JSON 文件里保存需要执行优先副本选举的分区清单。

举个例子，我们再将集群中 brokerId 为2的节点重启，不过我们现在只想对主题 topic- partitions 执行优先副本的选举操作，那么先创建一个JSON文件，文件名假定为 election.json，文件的内容如下：

```json
{
        "partitions":[
                {
                        "partition":0,
                        "topic":"topic-partitions"
                },
                {
                        "partition":1,
                        "topic":"topic-partitions"
                },
                {
                        "partition":2,
                        "topic":"topic-partitions"
                }
        ]
}
```

然后通过 kafka-perferred-replica-election.sh 脚本配合 path-to-json-file 参数来对主题 topic-partitions 执行优先副本的选举操作，具体示例如下：

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181/kafka --path-to-json-file election.json
Created preferred replica election path with topic-partitions-0,topic-partitions-1, topic-partitions-2
Successfully started preferred replica election for partitions Set(topic- partitions-0, topic-partitions-1, topic-partitions-2)

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-partitions
Topic:topic-partitions	PartitionCount:3	ReplicationFactor:3	Configs:
    Topic: topic-partitions	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,0,2 
    Topic: topic-partitions	Partition: 1	Leader: 2	Replicas: 2,0,1	Isr: 0,1,2
    Topic: topic-partitions	Partition: 2	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
```


读者可以自行查看一下集群中的其他主题是否像之前没有使用 path-to-json-file 参数的一样也被执行了选举操作。

**实际生产环境中，一般使用 path-to-json-file 参数来分批、手动地执行优先副本的选举操作。尤其是在应对大规模的 Kafka 集群时，理应杜绝采用非 path-to-json-file 参数的选举操作方式。同时，优先副本的选举操作也要注意避开业务高峰期，以免带来性能方面的负面影响。**

### （二）分区重分配

当集群中的一个节点突然宕机下线时，如果节点上的分区是单副本的，那么这些分区就变得不可用了，在节点恢复前，相应的数据也就处于丢失状态；如果节点上的分区是多副本的，那么位于这个节点上的 leader 副本的角色会转交到集群的其他 follower 副本中。总而言之，这个节点上的分区副本都已经处于功能失效的状态，Kafka 并不会将这些失效的分区副本自动地迁移到集群中剩余的可用 broker 节点上，如果放任不管，则不仅会影响整个集群的均衡负载，还会影响整体服务的可用性和可靠性。

- 当要对集群中的一个节点进行有计划的下线操作时，为了保证分区及副本的合理分配，我们也希望通过某种方式能够将该节点上的分区副本迁移到其他的可用节点上。

- 当集群中新增 broker 节点时，只有新创建的主题分区才有可能被分配到这个节点上，而之前的主题分区并不会自动分配到新加入的节点中，因为在它们被创建时还没有这个新节点，这样新节点的负载和原先节点的负载之间严重不均衡。

#### 1.操作步骤

为了解决上述问题，需要让分区副本再次进行合理的分配（分区重分配）。**Kafka 提供了 kafka-reassign-partitions.sh 脚本来执行分区重分配的工作，它可以在集群扩容、broker 节点失效的场景下对分区进行迁移**。 该脚本的使用分为3个步骤：

- 首先创建需要一个包含主题清单的 JSON 文件；
- 其次根据主题清单和 broker 节点清单生成一份重分配方案；
- 最后根据这份方案执行具体的重分配动作。

> 注：其中第一步和第二步是让脚本自动生成候选方案，用户可以自定义重分配方案，即按照第二步生成的 JSON 格式自定义方案，就不需要执行步骤一、二了。

示例：集群中 3 个节点（broker 0/1/2），创建一个主题 topic-reassign，主题中包含 4 个分区和 2 个副本：

创建完成之后使用 `--describe --topic topic-reassign` 查看该主题的分区副本在各个 broker 节点中的分布：

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-reassign
Topic:topic-reassign	PartitionCount:4	ReplicationFactor:2	Configs: 
    Topic: topic-reassign	Partition: 0	Leader: 0	Replicas: 0,2	Isr: 0,2
    Topic: topic-reassign	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 1,0
    Topic: topic-reassign	Partition: 2	Leader: 2	Replicas: 2,1	Isr: 2,1
    Topic: topic-reassign	Partition: 3	Leader: 0	Replicas: 0,1	Isr: 0,1
## leader 后面是 leader 副本所在的 broker，replicas 后是所有副本所在的 broker    
```

如果需要下线 broker 1，则首先需要将其上的所有分区副本迁移。

- 步骤一：创建一个 JSON 文件（文件名暂定为 reassign.json），文件格式如下，文件内容为要进行分区重分配的主题清单。

    ```json
    {
        "topics":[
            {
                "topic":"topic-reassign"
            }
            // 多个主题在这里添加即可
        ],
        "version":1
    }
    ```

- 步骤二：根据该 JSON 文件和指定所要分配的 broker 节点列表来生成一份候选的重分配方案，具体内容参考如下：

    其中 `-generate` 为脚本中参数，表示生成一个重分配的候选方案，`topics-to-move-json-file` 用于指定分区重分配对应的主题清单文件路径。`broker-list` 用于指定所要分配的 broker 节点列表。

    ```shell
    [root@node1 kafka_2.11-2.0.0]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181/kafka --generate --topics-to-move-json-file reassign.json --broker-list 0,2
    
    ## 当前的分区副本分配情况，可以保留用于回滚
    Current partition replica assignment
    {"version":1,"partitions":[{"topic":"topic-reassign","partition":2,"replicas":[2,1],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":1,"replicas":[1,0],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":3,"replicas":[0,1],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":0,"replicas":[0,2],"log_dirs":["any","any"]}]}
    
    ## 生成的重分配候选方案
    Proposed partition reassignment configuration
    {"version":1,"partitions":[{"topic":"topic-reassign","partition":2,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":1,"replicas":[0,2],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":3,"replicas":[0,2],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":0,"replicas":[2,0],"log_dirs":["any","any"]}]}
    ```

​	将生成的方案保存到 JSON 文件中（暂定命名：project.json）。

- 执行具体的重分配动作

    其中 `execute` 指令参数用于指定执行重分配操作，`reassignment-json-file` 用于指定分区重分配方案的文件路径。

    ```shell
    [root@node1 kafka_2.11-2.0.0]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181/kafka --execute --reassignment-json-file project.json 
    
    ## 这里的 current 和上一个步骤返回的 current 一样，都可以保存用于回滚
    Current partition replica assignment
    
    {"version":1,"partitions":[{"topic":"topic-reassign","partition":2,"replicas":[2,1],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":1,"replicas":[1,0],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":3,"replicas":[0,1],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":0,"replicas":[0,2],"log_dirs":["any","any"]}]}
    
    Save this to use as the --reassignment-json-file option during rollback
    Successfully started reassignment of partitions.
    ```

​	执行过程中，可以验证查看分区重分配的进度。只需将上面的 execute 替换为 verify 即可，具体示例如下：

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181/kafka --verify --reassignment-json-file project.json 
Status of partition reassignment: 
Reassignment of partition topic-reassign-2 completed successfully
Reassignment of partition topic-reassign-1 completed successfully
Reassignment of partition topic-reassign-3 completed successfully
Reassignment of partition topic-reassign-0 completed successfully
```

执行完成后，可以再次查看该主题的分区副本分配情况：

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-reassign
Topic:topic-reassign	PartitionCount:4	ReplicationFactor:2	Configs: 
    Topic: topic-reassign	Partition: 0	Leader: 0	Replicas: 2,0	Isr: 0,2
    Topic: topic-reassign	Partition: 1	Leader: 0	Replicas: 0,2	Isr: 0,2
    Topic: topic-reassign	Partition: 2	Leader: 2	Replicas: 2,0	Isr: 2,0
    Topic: topic-reassign	Partition: 3	Leader: 0	Replicas: 0,2	Isr: 0,2
## 该主题的所有分区副本都只在 broker 0 和 2 上了。
```

#### 2.基本原理

分区重分配的基本原理是先通过控制器为每个分区添加新副本（增加副本因子），新的副本将从分区的 leader 副本那里复制所有的数据。根据分区的大小不同，复制过程可能需要花一些时间，因为数据是通过网络复制到新副本上的。在复制完成之后，控制器将旧副本从副本清单里移除（恢复为原先的副本因子数）。注意在重分配的过程中要确保有足够的空间。

#### 3.后续 

重分配之后，该主题有 3 个 leader 副本在 broker 0 上，而只有 1 个 leader 副本在 broker 2 上，导致负载不均衡。可以借助上一节中的 kafka-perferred-replica-election.sh 脚本来执行一次优先副本的选举动作即可。

#### 4.注意点

分区重分配对集群的性能有很大的影响，需要占用额外的资源，比如网络和磁盘。在实际操作中，我们将降低重分配的粒度，分成多个小批次来执行，以此来将负面的影响降到最低。 

同时如果要将某个 broker 下线，那么在执行分区重分配动作之前最好先关闭或重启 broker。这样这个 broker 就不再是任何分区的 leader 节点了，它的分区就可以被分配给集群中的其他 broker。这样可以减少 broker 间的流量复制，以此提升重分配的性能，以及减少对集群的影响。

### （三）复制限流【未整理】

  在上一节中我们了解了分区重分配本质在于数据复制，先增加新的副本，然后进行数据同步，最后删除旧的副本来达到最终的目的。数据复制会占用额外的资源，如果重分配的量太大必然会严重影响整体的性能，尤其是处于业务高峰期的时候。减小重分配的粒度，以小批次的方式来操作是一种可行的解决思路。如果集群中某个主题或某个分区的流量在某段时间内特别大，那么只靠减小粒度是不足以应对的，这时就需要有一个限流的机制，可以对副本间的复制流量加以限制来保证重分配期间整体服务不会受太大的影响。

     副本间的复制限流有两种实现方式：kafka-config.sh 脚本和 kafka-reassign-partitions.sh 脚本。
    
     首先，我们讲述如何通过 kafka-config.sh 脚本来实现限流，如果对这个脚本的使用有些遗忘，则可以再回顾一下19节的内容。不过19节里只演示了主题相关的配置变更，并没有涉及其他的类型，本节的内容会与broker类型的配置相关，不妨借助这个机会再来了解一下 broker 类型的配置用法。
    
     kafka-config.sh 脚本主要以动态配置的方式来达到限流的目的，在 broker 级别有两个与复制限流相关的配置参数：follower.replication.throttled.rate 和 leader.replication. throttled.rate，前者用于设置 follower 副本复制的速度，后者用于设置 leader 副本传输的速度，它们的单位都是 B/s。通常情况下，两者的配置值是相同的。下面的示例中将 broker 1中的 leader 副本和 follower 副本的复制速度限制在1024B/s之内，即1KB/s：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-configs.sh --zookeeper localhost: 2181/kafka --entity-type brokers --entity-name 1 --alter --add-config follower.replication. throttled.rate=1024,leader.replication.throttled.rate=1024
Completed Updating config for entity: brokers '1'.
我们再来查看一下 broker 1 中刚刚添加的配置，参考如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-configs.sh --zookeeper localhost: 2181/kafka --entity-type brokers --entity-name 1 --describe
Configs for brokers '1' are leader.replication.throttled.rate=1024,follower. replication.throttled.rate=1024
      在19节中我们了解到变更配置时会在 ZooKeeper 中创建一个命名形式为/config/ <entity-type>/<entity-name>的节点，对于这里的示例而言，其节点就是/config/brokers/1，节点中相应的信息如下：

[zk: localhost:2181/kafka(CONNECTED) 6] get /config/brokers/1
{"version":1,"config":{"leader.replication.throttled.rate":"1024","follower.replication.throttled.rate":"1024"}}
删除刚刚添加的配置也很简单，与19节中主题类型的方式一样，参考如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-configs.sh --zookeeper localhost: 2181/kafka --entity-type brokers --entity-name 1 --alter --delete-config follower. replication.throttled.rate,leader.replication.throttled.rate
Completed Updating config for entity: brokers '1'.
      在主题级别也有两个相关的参数来限制复制的速度：leader.replication.throttled. replicas 和 follower.replication.throttled.replicas，它们分别用来配置被限制速度的主题所对应的 leader 副本列表和 follower 副本列表。为了演示具体的用法，我们先创建一个分区数为3、副本数为2的主题 topic-throttle，并查看它的详细信息：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --create --topic topic-throttle --replication-factor 2 --partitions 3
Created topic "topic-throttle".

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-throttle
Topic:topic-throttle	PartitionCount:3	ReplicationFactor:2	Configs: 
    Topic: topic-throttle	Partition: 0	Leader: 0	Replicas: 0,1	Isr: 0,1
    Topic: topic-throttle	Partition: 1	Leader: 1	Replicas: 1,2	Isr: 1,2
    Topic: topic-throttle	Partition: 2	Leader: 2	Replicas: 2,0	Isr: 2,0
      在上面示例中，主题 topic-throttle 的三个分区所对应的 leader 节点分别为0、1、2，即分区与代理的映射关系为0:0、1:1、2:2，而对应的 follower 节点分别为1、2、0，相关的分区与代理的映射关系为0:1、1:2、2:0，那么此主题的限流副本列表及具体的操作细节如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-configs.sh --zookeeper localhost: 2181/kafka --entity-type topics --entity-name topic-throttle --alter --add-config leader.replication.throttled.replicas=[0:0,1:1,2:2],follower.replication.throttled.replicas=[0:1,1:2,2:0]
Completed Updating config for entity: topic 'topic-throttle'.
      对应的 ZooKeeper 中的/config/topics/topic-throttle 节点信息如下：

{"version":1,"config":{"leader.replication.throttled.replicas":"0:0,1:1,2:2","follower.replication.throttled.replicas":"0:1,1:2,2:0"}}
        在了解了与限流相关的4个配置参数之后，我们演示一下带有限流的分区重分配的用法。首先按照上一节的步骤创建一个包含可行性方案的 project.json 文件，内容如下：

{"version":1,"partitions":[{"topic":"topic-throttle","partition":1,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":0,"replicas":[0,2],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":2,"replicas":[0,2],"log_dirs":["any","any"]}]}
     接下来设置被限流的副本列表，这里就很有讲究了，首先看一下重分配前和分配后的分区副本布局对比，详细如下：

partition   重分配前的AR     分配后的预期AR
0             0,1              0,2
1             1,2              2,0
2             2,0              0,2
      如果分区重分配会引起某个分区AR集合的变更，那么这个分区中与 leader 有关的限制会应用于重分配前的所有副本，因为任何一个副本都可能是 leader，而与 follower 有关的限制会应用于所有移动的目的地。从概念上理解会比较抽象，这里不妨举个例子，对上面的布局对比而言，分区0重分配的AR为[0,1]，重分配后的AR为[0,2]，那么这里的目的地就是新增的2。也就是说，对分区0而言，leader.replication.throttled.replicas 配置为[0:0, 0:1]，follower.replication.throttled.replicas 配置为[0:2]。同理，对于分区1而言，leader.replication.throttled.replicas 配置为[1:1,1:2]，follower.replication. throttled.replicas 配置为[1:0]。分区3的AR集合没有发生任何变化，这里可以忽略。

     获取限流副本列表之后，我们就可以执行具体的操作了，详细如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-configs.sh --zookeeper localhost: 2181/kafka --entity-type topics --entity-name topic-throttle --alter --add-config leader.replication.throttled.replicas=[1:1,1:2,0:0,0:1],follower.replication.throttled.replicas=[1:0,0:2]
Completed Updating config for entity: topic 'topic-throttle'.
     接下来再设置 broker 2 的复制速度为10B/s，这样在下面的操作中可以很方便地观察限流与不限流的不同：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-configs.sh --zookeeper localhost: 2181/kafka --entity-type brokers --entity-name 2 --alter --add-config follower. replication.throttled.rate=10,leader.replication.throttled.rate=10
Completed Updating config for entity: brokers '2'.
     在执行具体的重分配操作之前，我们需要开启一个生产者并向主题 topic-throttle 中发送一批消息，这样可以方便地观察正在进行数据复制的过程。

之后我们再执行正常的分区重分配的操作，示例如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181/kafka --execute --reassignment-json-file project.json
Current partition replica assignment

{"version":1,"partitions":[{"topic":"topic-throttle","partition":2,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":1,"replicas":[1,2],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":0,"replicas":[0,1],"log_dirs":["any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Successfully started reassignment of partitions.
执行之后，可以查看执行的进度，示例如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181/kafka --verify --reassignment-json-file project.json
Status of partition reassignment: 
Reassignment of partition topic-throttle-1 completed successfully
Reassignment of partition topic-throttle-0 is still in progress
Reassignment of partition topic-throttle-2 completed successfully
      可以看到分区 topic-throttle-0 还在同步过程中，因为我们之前设置了 broker 2 的复制速度为10B/s，这样使同步变得缓慢，分区 topic-throttle-0 需要同步数据到位于 broker 2 的新增副本中。随着时间的推移，分区 topic-throttle-0 最终会变成“completed successful”的状态。

      为了不影响 Kafka 本身的性能，往往对临时设置的一些限制性的配置在使用完后要及时删除，而 kafka-reassign-partitions.sh 脚本配合指令参数 verify 就可以实现这个功能，在所有的分区都重分配完成之后执行查看进度的命令时会有如下的信息：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181/kafka --verify --reassignment-json-file project.json
Status of partition reassignment: 
Reassignment of partition topic-throttle-1 completed successfully
Reassignment of partition topic-throttle-0 completed successfully
Reassignment of partition topic-throttle-2 completed successfully
Throttle was removed.
      注意到最后一行信息“Throttle was removed.”，它提示了所有之前针对限流做的配置都已经被清除了，读者可以自行查看一下相应的 ZooKeeper 节点中是否还有相关的配置。

        kafka-reassign-partitions.sh 脚本本身也提供了限流的功能，只需一个 throttle 参数即可，具体用法如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181/kafka --execute --reassignment-json-file project.json  --throttle 10
Current partition replica assignment

{"version":1,"partitions":[{"topic":"topic-throttle","partition":2,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":1,"replicas":[1,2],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":0,"replicas":[0,1],"log_dirs":["any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.
The inter-broker throttle limit was set to 10 B/s
Successfully started reassignment of partitions.
      上面的信息中包含了明确的告警信息：需要周期性地执行查看进度的命令直到重分配完成，这样可以确保限流设置被移除。也就是说，使用这种方式的限流同样需要显式地执行某些操作以使在重分配完成之后可以删除限流的设置。上面的信息中还告知了目前限流的速度上限为10B/s。

     如果想在重分配期间修改限制来增加吞吐量，以便完成得更快，则可以重新运行 kafka- reassign-partitions.sh 脚本的 execute 命令，使用相同的 reassignment-json-file，示例如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181/kafka --execute --reassignment-json-file project.json  --throttle 1024
There is an existing assignment running.
     这样限流的速度上限为1024B/s，可以查看对应的 ZooKeeper 节点内容：

[zk: localhost:2181/kafka(CONNECTED) 30] get /config/topics/topic-throttle
{"version":1,"config":{"follower.replication.throttled.replicas":"1:0,0:2","leader.replication.throttled.replicas":"1:1,1:2,0:0,0:1"}}
      可以看到 ZooKeeper 节点内容中的限流副本列表和前面使用 kafka-config.sh 脚本时的一样。其实 kafka-reassign-partitions.sh 脚本提供的限流功能背后的实现原理就是配置与限流相关的那4个参数而已，没有什么太大的差别。不过使用 kafka-config.sh 脚本的方式来实现复制限流的功能比较烦琐，并且在手动配置限流副本列表时也比较容易出错，这里推荐大家使用 kafka-reassign-partitions.sh 脚本配合 throttle 参数的方式，方便快捷且不容易出错。


------------------------------------------------
版权声明：本文为CSDN博主「码匠小双」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_34475529/article/details/121805412

### （四）修改副本因子【未整理】

 创建主题之后我们还可以修改分区的个数，同样可以修改副本因子（副本数）。修改副本因子的使用场景也很多，比如在创建主题时填写了错误的副本因子数而需要修改，再比如运行一段时间之后想要通过增加副本因子数来提高容错性和可靠性。

      前面主要讲述了分区重分配的相关细节，本节中修改副本因子的功能也是通过重分配所使用的 kafka-reassign-partition.sh 脚本实现的。我们仔细观察一下上一节中的示例使用的 project.json 文件：

{
    "version": 1,
    "partitions": [
        {
            "topic": "topic-throttle",
            "partition": 1,
            "replicas": [
                2,
                0
            ],
            "log_dirs": [
                "any",
                "any"
            ]
        },
        {
            "topic": "topic-throttle",
            "partition": 0,
            "replicas": [
                0,
                2
            ],
            "log_dirs": [
                "any",
                "any"
            ]
        },
        {
            "topic": "topic-throttle",
            "partition": 2,
            "replicas": [
                0,
                2
            ],
            "log_dirs": [
                "any",
                "any"
            ]
        }
    ]
}
      可以观察到 JSON 内容里的 replicas 都是2个副本，我们可以自行添加一个副本，比如对分区1而言，可以改成下面的内容：

{
    "topic": "topic-throttle",
    "partition": 1,
    "replicas": [
        2,
        1,
        0
    ],
    "log_dirs": [
        "any",
        "any",
        "any"
    ]
}
      我们可以将其他分区的 replicas 内容也改成[0,1,2]，这样每个分区的副本因子就都从2增加到了3。注意增加副本因子时也要在 log_dirs中添加一个“any”，这个log_dirs 代表 Kafka 中的日志目录，对应于 broker 端的 log.dir 或 log.dirs 参数的配置值，如果不需要关注此方面的细节，那么可以简单地设置为“any”。我们将修改后的 JSON 内容保存为新的 add.json 文件。在执行 kafka-reassign-partition.sh 脚本前，主题 topic-throttle 的详细信息（副本因子为2）如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-throttle
Topic:topic-throttle    PartitionCount:3    ReplicationFactor:2 Configs:
    Topic: topic-throttle   Partition: 0    Leader: 0   Replicas: 0,1   Isr: 0,1
    Topic: topic-throttle   Partition: 1    Leader: 1   Replicas: 1,2   Isr: 2,1
    Topic: topic-throttle   Partition: 2    Leader: 2   Replicas: 2,0   Isr: 2,0
        执行 kafka-reassign-partition.sh 脚本（execute），详细信息如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-reassign-partitions.sh --zookeeper localhost:2181/kafka --execute --reassignment-json-file add.json
Current partition replica assignment

{"version":1,"partitions":[{"topic":"topic-throttle","partition":2,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":1,"replicas":[1,2],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":0,"replicas":[0,1],"log_dirs":["any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Successfully started reassignment of partitions.
执行之后再次查看主题 topic-throttle 的详细信息，详细信息如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-throttle
Topic:topic-throttle    PartitionCount:3    ReplicationFactor:3 Configs:
    Topic: topic-throttle   Partition: 0    Leader: 0   Replicas: 0,1,2 Isr: 0,1,2
    Topic: topic-throttle   Partition: 1    Leader: 1   Replicas: 0,1,2 Isr: 2,1,0
    Topic: topic-throttle   Partition: 2    Leader: 2   Replicas: 0,1,2 Isr: 2,0,1
可以看到相应的副本因子数已经增加到3了。

       与修改分区数不同的是，副本数还可以减少，这个其实很好理解，最直接的方式是关闭一些 broker，不过这种手法不太正规。这里我们同样可以通过 kafka-reassign-partition.sh 脚本来减少分区的副本因子。再次修改 project.json 文件中的内容，内容参考如下：

{"version":1,"partitions":[{"topic":"topic-throttle","partition":2,"replicas":[0],"log_dirs":["any"]},{"topic":"topic-throttle","partition":1,"replicas":[1],"log_dirs":["any"]},{"topic":"topic-throttle","partition":0,"replicas":[2],"log_dirs":["any"]}]}
再次执行 kafka-reassign-partition.sh 脚本（execute）之后，主题 topic-throttle 的详细信息如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --describe --topic topic-throttle 
Topic:topic-throttle	PartitionCount:3	ReplicationFactor:1	Configs:
     Topic: topic-throttle	Partition: 0	Leader: 2	Replicas: 2	Isr: 2
     Topic: topic-throttle	Partition: 1	Leader: 1	Replicas: 1	Isr: 1
     Topic: topic-throttle	Partition: 2	Leader: 0	Replicas: 0	Isr: 0
可以看到主题 topic-throttle 的副本因子又被修改为1了。

        细心的读者可能注意到我们执行 kafka-reassign-partition.sh 脚本（execute）所使用的候选方案都是手动修改的，在增加副本因子的时候由于整个示例集群中只有3个 broker 节点，从2增加到3只需填满副本即可。再者，示例中减少副本因子的时候改成了1，这样可以简单地把各个 broker 节点轮询一遍，如此也就不太会有负载不均衡的影响。不过在真实应用中，可能面对的是一个包含了几十个 broker 节点的集群，将副本数从2修改为5，或者从4修改为3的时候，如何进行合理的分配是一个关键的问题。
    
       我们可以参考17节中的分区副本的分配来进行相应的计算，不过如果不是通过程序来得出结果而是通过人工去计算的，也确实比较烦琐。下面演示了如何通过程序来计算出分配方案（实质上是17节中对应的方法），如代码清单24-1所示。

代码清单24-1 分配方案计算（Scala）
object ComputeReplicaDistribution {
  val partitions = 3
  val replicaFactor = 2

  def main(args: Array[String]): Unit = {
    val brokerMetadatas = List(new BrokerMetadata(0, Option("rack1")),
      new BrokerMetadata(1, Option("rack1")),
      new BrokerMetadata(2, Option("rack1")))
    val replicaAssignment = AdminUtils.assignReplicasToBrokers(brokerMetadatas,
      partitions, replicaFactor)
    println(replicaAssignment)
  }
}
代码中计算的是集群节点为[0,1,2]、分区数为3、副本因子为2、无机架信息的分配方案，程序输出如下：

Map(2 -> ArrayBuffer(0, 2), 1 -> ArrayBuffer(2, 1), 0 -> ArrayBuffer(1, 0))
分区2对应于[0,2]，分区1对应于[2,1]，分区0对应于[1,0]，所以在一个3节点的集群中将副本因子修改为2的对应候选方案为：

{"version":1,"partitions":[{"topic":"topic-throttle","partition":2,"replicas":[0,2],"log_dirs":["any","any

------------------------------------------------
版权声明：本文为CSDN博主「码匠小双」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_34475529/article/details/121805438

## 四、如何选择合适的分区数

该问题没有固定答案，只能从某些角度来做具体的分析，最终还是要根据实际的业务场景、软件条件、硬件条件、负载情况等来做具体的考量。但是有与本问题相关的一些重要决策因素作为参考依据。

在 Kafka 中，性能与分区数有着必然的关系，在设定分区数时一般也需要考虑性能的因素。对不同的硬件而言，其对应的性能也会不太一样。在实际生产环境中，我们需要了解一套硬件所对应的性能指标之后才能分配其合适的应用和负荷，所以性能测试工具必不可少。

### （一）性能测试工具

Kafka 本身提供了用于生产者性能测试的 kafka-producer-perf-test.sh 和用于消费者性能测试的 kafka-consumer-perf-test.sh。

#### 1.生产者性能测试

前提：向一个只有 1 个分区和 1 个副本的主题 topic-1 中发送 100 万条消息，并且每条消息大小为 1024B，生产者对应的 acks 参数为1。

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-producer-perf-test.sh --topic topic-1 --num-records 1000000 --record-size 1024 --throughput -1 --print-metrics --producer-props bootstrap. servers=localhost:9092 acks=1
273616 records sent, 54723.2 records/sec (53.44 MB/sec), 468.6 ms avg latency, 544.0 max latency.
337410 records sent, 67482.0 records/sec (65.90 MB/sec), 454.4 ms avg latency, 521.0 max latency.
341910 records sent, 68382.0 records/sec (66.78 MB/sec), 449.4 ms avg latency, 478.0 max latency.
1000000 records sent, 63690.210815 records/sec (62.20 MB/sec), 456.17 ms avg latency, 544.00 ms max latency, 458 ms 50th, 517 ms 95th, 525 ms 99th, 543 ms 99.9th.

Metric Name                                                         Value
app-info:commit-id:{client-id=producer-1}					: 3402a8361b734732
app-info:version:{client-id=producer-1}					: 2.0.0
kafka-metrics-count:count:{client-id=producer-1}			: 94.000
producer-metrics:batch-size-avg:{client-id=producer-1}	: 15555.923
producer-metrics:batch-size-max:{client-id=producer-1}	: 15556.000
producer-metrics:batch-split-rate:{client-id=producer-1}	: 0.000
producer-metrics:batch-split-total:{client-id=producer-1}	: 0.000
producer-metrics:buffer-available-bytes:{client-id=producer-1}  	  : 33554432.000
producer-metrics:buffer-exhausted-rate:{client-id=producer-1}	  : 0.000
producer-metrics:buffer-exhausted-total:{client-id=producer-1}  : 0.000
producer-metrics:buffer-total-bytes:{client-id=producer-1}	  : 33554432.000
producer-metrics:bufferpool-wait-ratio:{client-id=producer-1}	  : 0.278
producer-metrics:bufferpool-wait-time-total:{client-id=producer-1}	  : 12481086207.000
(…省略若干)
```

注：参数说明：

- topic 用来指定生产者发送消息的目标主题；
- num-records 用来指定发送消息的总条数；
- record-size 用来设置每条消息的字节数；
- producer-props 参数用来指定生产者的配置，可同时指定多组配置，各组配置之间以空格分隔，与 producer-props 参数对应的还有一个 producer.config 参数，它用来指定生产者的配置文件；
- throughput 用来进行限流控制，当设定的值小于0时不限流，当设定的值大于0时，当发送的吞吐量大于该值时就会被阻塞一段时间。
- print-metrics，指定了这个参数时会在测试完成之后打印很多指标信息，对很多测试任务而言具有一定的参考价值。

- 其它参数还有如 payload-delimiter、transactional-id 等。

**脚本输出结果分析**：

1000000 records sent, 63690.210815 records/sec (62.20 MB/sec), 456.17 ms avg latency, 544.00 ms max latency, 458 ms 50th, 517 ms 95th, 525 ms 99th, 543 ms 99.9th.

- records sent 表示测试时发送的消息总数；
- records/sec 表示以每秒发送的消息数来统计吞吐量，括号中的 MB/sec 表示以每秒发送的消息大小来统计吞吐量，注意这两者的维度；
- avg latency 表示消息处理的平均耗时；
- max latency 表示消息处理的最大耗时；50th、95th、99th 和 99.9th 分别表示 50%、95%、99% 和 99.9% 的消息处理耗时。 

#### 3.消费者性能测试

前提：简单地消费主题 topic-1 中的 100 万条消息。

注：其它还有如 from-latest、group、print-metrics、threads 等参数；

```shell
[root@node1 kafka_2.11-2.0.0]# bin/kafka-consumer-perf-test.sh --topic topic-1 --messages 1000000 --broker-list localhost:9092
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec
2018-09-22 12:27:49:827, 2018-09-22 12:27:57:068, 976.5625, 134.8657, 1000000, 138102.4720, 105, 7136, 136.8501, 140134.5291
```

**输出结果分析**：

起始运行时间（start.time）、结束运行时间（end.time）、消费的消息总量（data.consumed.in.MB，单位为MB）、按字节大小计算的消费吞吐量（MB.sec，单位为MB/s）、消费的消息总数（data.consumed.in.nMsg）、按消息个数计算的吞吐量（nMsg.sec）、再平衡的时间（rebalance.time.ms，单位为ms）、拉取消息的持续时间（fetch.time.ms，单位为ms）、每秒拉取消息的字节大小（fetch.MB.sec，单位为MB/s）、每秒拉取消息的个数（fetch.nMsg.sec）。其中 fetch.time.ms = end.time – start.time – rebalance.time.ms。

### （二）分区数越多吞吐量并非就一定越高

**分区是 Kafka 中最小的并行操作单元，对生产者而言，每一个分区的数据写入是完全可以并行化的；对消费者而言，Kafka 只允许单个分区中的消息被一个消费者线程消费，一个消费组的消费并行度完全依赖于所消费的分区数**。如果一个主题中的分区数越多，理论上所能达到的吞吐量就越大。

下面通过反证法分别针对生产端和消费端进行验证：

使用上面介绍的性能测试工具来实际测试一下。首先分别创建分区数为 1、20、50、100、200、500、1000 的主题，对应的主题名称分别为topic-1、topic-20、topic-50、topic-100、topic-200、topic-500、topic-1000，所有主题的副本因子都设置为 1。

消息中间件的性能一般是指吞吐量（广义来说还包括延迟）。抛开硬件资源的影响，消息写入的吞吐量还会受到消息大小、消息压缩方式、消息发送方式（同步/异步）、消息确认类型（acks）、副本因子等参数的影响，消息消费的吞吐量还会受到应用逻辑处理速度的影响。本案例中暂不考虑这些因素的影响，所有的测试除了主题的分区数不同，其余的因素都保持相同。

```shell
## 使用 kafka-producer-perf-test.sh 脚本分别向这些主题中发送100万条消息体大小为1KB的消息，对应的测试命令如下：
bin/kafka-producer-perf-test.sh --topic topic-xxx --num-records 1000000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers=localhost: 9092 acks=1
```


结果：随着分区数的增长，相应的吞吐量也跟着上涨。一旦分区数超过了某个阈值之后，整体的吞吐量是不升反降的。也就是说，并不是分区数越多吞吐量也越大。这里的分区数临界阈值针对不同的测试环境也会表现出不同的结果，实际应用中可以通过类似的测试案例（比如复制生产流量以便进行测试回放）来找到一个合理的临界值区间。

同样可以使用 kafka-consumer-perf-test.sh 脚本对消息消费者的吞吐进行验证，如分别消费这些主题中的100万条消息，对应的测试命令如下：

```shell
## 消费这些主题中的 100 万条消息
bin/kafka-consumer-perf-test.sh --topic topic-xxx --messages 1000000 --broker-list localhost:9092
```

结论：随着分区数的增加，相应的吞吐量也会有所增长。一旦分区数超过了某个阈值之后，整体的吞吐量也是不升反降的，同样说明了分区数越多并不会使吞吐量一直增长。

### （三）分区数的上限【未整理】

**一味地增加分区数并不能使吞吐量一直得到提升，并且分区数也并不能一直增加，如果超过默认的配置值，还会引起 Kafka 进程的崩溃**。

```shell
# 创建包含10000个分区的主题 topic-bomb：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --create --topic topic-bomb --replication-factor 1 --partitions 10000
Created topic "topic-bomb".
```


执行完成后可以检查 Kafka 的进程是否还存在（比如通过 jps 命令或 ps -aux|grep kafka 命令）。一般情况下，会发现原本运行完好的 Kafka 服务已经崩溃。通过分析 kafka 的服务日志文件（$KAFKA_HOME/logs/server.log），发现并不是因为 JVM 堆内存不足导致的。

```shell
[2018-09-13 00:36:40,019] ERROR Error while creating log for topic-bomb-xxx in dir /tmp/kafka-logs (kafka.server.LogDirFailureChannel)
## 这是一种常见的 Linux 系统错误，通常意味着文件描述符不足，通常发生在创建线程、创建 socket、打开文件等场景下
java.io.IOException: Too many open files 
     at java.io.UnixFileSystem.createFileExclusively(Native Method)
     at java.io.File.createNewFile(File.java:1012)
     at kafka.log.AbstractIndex.<init>(AbstractIndex.scala:54)
     at kafka.log.OffsetIndex.<init>(OffsetIndex.scala:53)
     at kafka.log.LogSegment$.open(LogSegment.scala:634)
     at kafka.log.Log.loadSegments(Log.scala:503)
     at kafka.log.Log.<init>(Log.scala:237)
```


在 Linux 系统的默认设置下，这个文件描述符的个数不是很多，通过 ulimit 命令可以查看：

[root@node1 kafka_2.11-2.0.0]# ulimit -n
1024
[root@node1 kafka_2.11-2.0.0]# ulimit -Sn
1024
[root@node1 kafka_2.11-2.0.0]# ulimit -Hn
4096
ulimit 是在系统允许的情况下，提供对特定 shell 可利用的资源的控制。-H 和 -S 选项指定资源的硬限制和软限制。硬限制设定之后不能再添加，而软限制则可以增加到硬限制规定的值。如果 -H 和 -S 选项都没有指定，则软限制和硬限制同时设定。限制值可以是指定资源的数值或 hard、soft、unlimited 这些特殊值，其中 hard 代表当前硬限制，soft 代表当前软件限制，unlimited 代表不限制。如果不指定限制值，则打印指定资源的软限制值，除非指定了 -H 选项。硬限制可以在任何时候、任何进程中设置，但硬限制只能由超级用户设置。软限制是内核实际执行的限制，任何进程都可以将软限制设置为任意小于等于硬限制的值。

我们可以通过测试来验证本案例中的 Kafka 的崩溃是否是由于文件描述符的限制而引起的。下面我们在一个包含3个节点的 Kafka 集群中挑选一个节点进行具体的分析。首先通过 jps 命令查看 Kafka 进程 pid 的值：

[root@node1 kafka_2.11-2.0.0]# jps -l
31796 kafka.Kafka
查看当前 Kafka 进程所占用的文件描述符的个数（注意这个值并不是Kafka第一次启动时就需要占用的文件描述符的个数，示例中的 Kafka 环境下已经存在了若干主题）：

[root@node1 kafka_2.11-2.0.0]# ls /proc/31796/fd | wc -l
194
我们再新建一个只有一个分区的主题，并查看 Kafka 进程所占用的文件描述符的个数：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --create --topic topic-bomb-1 --replication-factor 1 --partitions 1
Created topic "topic-bomb-1".

[root@node1 kafka_2.11-2.0.0]# ls /proc/31796/fd | wc -l
195
可以看到增加了一个分区，对应的也只增加了一个文件描述符。之前我们通过 ulimit命令可以看到软限制是1024，我们创建一个具有829（1024-195=829）个分区的主题：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --create --topic topic-bomb-2 --replication-factor 1 --partitions 829
Created topic "topic-bomb-2".

[root@node1 kafka_2.11-2.0.0]# ls /proc/31796/fd | wc -l
1024
可以看到 Kafka 进程此时占用了1024个文件描述符，并且运行完好。这时我们还可以联想到硬限制4096这个关键数字，我们再创建一个包含3071（4096-1024=3072，这里特地少创建1个分区）个分区的主题，示例如下：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --create --topic topic-bomb-3 --replication-factor 1 --partitions 3071
Created topic "topic-bomb-3".

[root@node1 kafka_2.11-2.0.0]# ls /proc/31796/fd | wc -l
4095
      Kafka 进程依旧完好，文件描述符占用为4095，逼近最高值4096。最后我们再次创建一个只有一个分区的主题：

[root@node1 kafka_2.11-2.0.0]# bin/kafka-topics.sh --zookeeper localhost:2181/ kafka --create --topic topic-bomb-4 --replication-factor 1 --partitions 1
Created topic "topic-bomb-4".

[root@node1 kafka_2.11-2.0.0]# ls /proc/31796/fd | wc -l
ls: cannot access /proc/31796/fd: No such file or directory
0
      此时 Kafka 已经崩溃，查看进程号时已没有相关信息。查看 Kafka 中的日志，还会发现报出前面提及的异常“java.io.IOException: Too many open files”，表明已达到上限。

      如何避免这种异常情况？对于一个高并发、高性能的应用来说，1024或4096的文件描述符限制未免太少，可以适当调大这个参数。比如使用 ulimit -n 65535命令将上限提高到65535，这样足以应对大多数的应用情况，再高也完全没有必要了。

[root@node1 kafka_2.11-2.0.0]# ulimit -n 65535
#可以再次查看相应的软硬限制数
[root@node1 kafka_2.11-2.0.0]# ulimit -Hn
65535
[root@node1 kafka_2.11-2.0.0]# ulimit -Sn
65535
也可以在/etc/security/limits.conf 文件中设置，参考如下：

#nofile - max number of open file descriptors
root soft nofile 65535
root hard nofile 65535
      limits.conf 文件修改之后需要重启才能生效。limits.conf 文件与 ulimit 命令的区别在于前者是针对所有用户的，而且在任何 shell 中都是生效的，即与 shell 无关，而后者只是针对特定用户的当前 shell 的设定。在修改最大文件打开数时，最好使用 limits.conf 文件来修改，通过这个文件，可以定义用户、资源类型、软硬限制等。也可以通过在/etc/profile 文件中添加 ulimit 的设置语句来使全局生效。

      设置之后可以再次尝试创建10000个分区的主题，检查一下 Kafka 是否还会再次崩溃。

### （四）选择合适分区数的考量因素

整体原则：视情况而定。

从吞吐量方面考虑，增加合适的分区数可以在一定程度上提升整体吞吐量，但超过对应的阈值之后吞吐量不升反降。如果应用对吞吐量有一定程度上的要求，则建议在投入生产环境之前对同款硬件资源做一个完备的吞吐量相关的测试，以找到合适的分区数阈值区间。

在创建主题之后，虽然我们还能够增加分区的个数，但基于 key 计算的主题需要严谨对待。当生产者向 Kafka 中写入基于 key 的消息时，Kafka 通过消息的 key 来计算出消息将要写入哪个具体的分区，这样具有相同 key 的数据可以写入同一个分区。Kafka 的这一功能对于一部分应用是极为重要的，比如日志压缩（Log Compaction），详细可以参考《图解Kafka之核心原理》；再比如对于同一个 key 的所有消息，消费者需要按消息的顺序进行有序的消费，如果分区的数量发生变化，那么有序性就得不到保证。

在创建主题时，最好能确定好分区数，这样也可以省去后期增加分区所带来的多余操作。**尤其对于与 key 高关联的应用，在创建主题时可以适当地多创建一些分区，以满足未来的需求。通常情况下，可以根据未来2年内的目标吞吐量来设定分区数**。当然如果应用与 key 弱关联，并且具备便捷的增加分区数的操作接口，那么也可以不用考虑那么长远的目标。

**如果应用场景要求主题中的消息都能保证顺序性，这种情况下在创建主题时可以设定分区数为 1，通过分区有序性的这一特性来达到主题有序性的目的**。

分区数会占用文件描述符，而一个进程所能支配的文件描述符是有限的（即文件句柄的开销）。虽然可以通过修改配置来增加可用文件描述符的个数，但凡事总有一个上限，在选择合适的分区数之前，最好再考量一下当前 Kafka 进程中已经使用的文件描述符的个数。

分区数的多少还会影响系统的可用性。在前面章节中，我们了解到 Kafka 通过多副本机制来实现集群的高可用和高可靠，每个分区都会有一至多个副本，每个副本分别存在于不同的 broker 节点上，并且只有 leader 副本对外提供服务。在 Kafka 集群的内部，所有的副本都采用自动化的方式进行管理，并确保所有副本中的数据都能保持一定程度上的同步。当 broker 发生故障时，leader 副本所属宿主的 broker 节点上的所有分区将暂时处于不可用的状态，此时 Kafka 会自动在其他的 follower 副本中选举出新的 leader 用于接收外部客户端的请求，整个过程由 Kafka 控制器负责完成。分区在进行 leader 角色切换的过程中会变得不可用，不过对于单个分区来说这个过程非常短暂，对用户而言可以忽略不计。如果集群中的某个 broker 节点宕机，那么就会有大量的分区需要同时进行 leader 角色切换，这个切换的过程会耗费一笔可观的时间，并且在这个时间窗口内这些分区也会变得不可用。

分区数越多也会让 Kafka 的正常启动和关闭的耗时变得越长，与此同时，主题的分区数越多不仅会增加日志清理的耗时，而且在被删除时也会耗费更多的时间。对旧版的生产者和消费者客户端而言，分区数越多，也会增加它们的开销，不过这一点在新版的生产者和消费者客户端中有效地得到了抑制。

如何选择合适的分区数？从某种意思来说，考验的是决策者的实战经验，更透彻地说，是对 Kafka 本身、业务应用、硬件资源、环境配置等多方面的考量而做出的选择。在设定完分区数，或者更确切地说是创建主题之后，还要对其追踪、监控、调优以求更好地利用它。读者看到本节的内容之前或许没有对分区数有太大的困扰，而看完本节的内容之后反而困惑了起来，其实大可不必太过惊慌，一般情况下，根据预估的吞吐量及是否与 key 相关的规则来设定分区数即可，后期可以通过增加分区数、增加 broker 或分区重分配等手段来进行改进。如果一定要给一个准则，则建议将分区数设定为集群中 broker 的倍数，即假定集群中有3个 broker 节点，可以设定分区数为3、6、9等，至于倍数的选定可以参考预估的吞吐量。不过，如果集群中的 broker 节点数有很多，比如大几十或上百、上千，那么这种准则也不太适用，在选定分区数时进一步可以引入机架等参考因素。

